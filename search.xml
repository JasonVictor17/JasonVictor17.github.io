<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Creating a Sentiment Analysis Web App</title>
    <url>/2020/02/23/SageMakerProject/</url>
    <content><![CDATA[<p>Creating a Sentiment Analysis Web App by using PyTorch and SageMaker</p>
<a id="more"></a>

<h1 id="Creating-a-Sentiment-Analysis-Web-App"><a href="#Creating-a-Sentiment-Analysis-Web-App" class="headerlink" title="Creating a Sentiment Analysis Web App"></a>Creating a Sentiment Analysis Web App</h1><h2 id="Using-PyTorch-and-SageMaker"><a href="#Using-PyTorch-and-SageMaker" class="headerlink" title="Using PyTorch and SageMaker"></a>Using PyTorch and SageMaker</h2><p><em>Deep Learning Nanodegree Program | Deployment</em><br>s</p>
<p>Now that we have a basic understanding of how SageMaker works we will try to use it to construct a complete project from end to end. Our goal will be to have a simple web page which a user can use to enter a movie review. The web page will then send the review off to our deployed model which will predict the sentiment of the entered review.</p>
<h2 id="Instructions"><a href="#Instructions" class="headerlink" title="Instructions"></a>Instructions</h2><p>Some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this notebook. You will not need to modify the included code beyond what is requested. Sections that begin with â€˜<strong>TODO</strong>â€˜ in the header indicate that you need to complete or implement some portion within them. Instructions will be provided for each section and the specifics of the implementation are marked in the code block with a <code># TODO: ...</code> comment. Please be sure to read the instructions carefully!</p>
<p>In addition to implementing code, there will be questions for you to answer which relate to the task and your implementation. Each section where you will answer a question is preceded by a â€˜<strong>Question:</strong>â€˜ header. Carefully read each question and provide your answer below the â€˜<strong>Answer:</strong>â€˜ header by editing the Markdown cell.</p>
<blockquote>
<p><strong>Note</strong>: Code and Markdown cells can be executed using the <strong>Shift+Enter</strong> keyboard shortcut. In addition, a cell can be edited by typically clicking it (double-click for Markdown cells) or by pressing <strong>Enter</strong> while it is highlighted.</p>
</blockquote>
<h2 id="General-Outline"><a href="#General-Outline" class="headerlink" title="General Outline"></a>General Outline</h2><p>Recall the general outline for SageMaker projects using a notebook instance.</p>
<ol>
<li>Download or otherwise retrieve the data.</li>
<li>Process / Prepare the data.</li>
<li>Upload the processed data to S3.</li>
<li>Train a chosen model.</li>
<li>Test the trained model (typically using a batch transform job).</li>
<li>Deploy the trained model.</li>
<li>Use the deployed model.</li>
</ol>
<p>For this project, you will be following the steps in the general outline with some modifications. </p>
<p>First, you will not be testing the model in its own step. You will still be testing the model, however, you will do it by deploying your model and then using the deployed model by sending the test data to it. One of the reasons for doing this is so that you can make sure that your deployed model is working correctly before moving forward.</p>
<p>In addition, you will deploy and use your trained model a second time. In the second iteration you will customize the way that your trained model is deployed by including some of your own code. In addition, your newly deployed model will be used in the sentiment analysis web app.</p>
<h2 id="Step-1-Downloading-the-data"><a href="#Step-1-Downloading-the-data" class="headerlink" title="Step 1: Downloading the data"></a>Step 1: Downloading the data</h2><p>As in the XGBoost in SageMaker notebook, we will be using the <a href="http://ai.stanford.edu/~amaas/data/sentiment/" target="_blank" rel="noopener">IMDb dataset</a></p>
<blockquote>
<p>Maas, Andrew L., et al. <a href="http://ai.stanford.edu/~amaas/data/sentiment/" target="_blank" rel="noopener">Learning Word Vectors for Sentiment Analysis</a>. In <em>Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</em>. Association for Computational Linguistics, 2011.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%mkdir ../data</span><br><span class="line">!wget -O ../data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz</span><br><span class="line">!tar -zxf ../data/aclImdb_v1.tar.gz -C ../data</span><br></pre></td></tr></table></figure>

<pre><code>mkdir: cannot create directory â€˜../dataâ€™: File exists
--2020-02-23 10:05:42--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz
Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10
Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 84125825 (80M) [application/x-gzip]
Saving to: â€˜../data/aclImdb_v1.tar.gzâ€™

../data/aclImdb_v1. 100%[===================&gt;]  80.23M  21.9MB/s    in 4.5s    

2020-02-23 10:05:47 (17.7 MB/s) - â€˜../data/aclImdb_v1.tar.gzâ€™ saved [84125825/84125825]</code></pre><h2 id="Step-2-Preparing-and-Processing-the-data"><a href="#Step-2-Preparing-and-Processing-the-data" class="headerlink" title="Step 2: Preparing and Processing the data"></a>Step 2: Preparing and Processing the data</h2><p>Also, as in the XGBoost notebook, we will be doing some initial data processing. The first few steps are the same as in the XGBoost example. To begin with, we will read in each of the reviews and combine them into a single input structure. Then, we will split the dataset into a training set and a testing set.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_imdb_data</span><span class="params">(data_dir=<span class="string">'../data/aclImdb'</span>)</span>:</span></span><br><span class="line">    data = &#123;&#125;</span><br><span class="line">    labels = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> data_type <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'test'</span>]:</span><br><span class="line">        data[data_type] = &#123;&#125;</span><br><span class="line">        labels[data_type] = &#123;&#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> sentiment <span class="keyword">in</span> [<span class="string">'pos'</span>, <span class="string">'neg'</span>]:</span><br><span class="line">            data[data_type][sentiment] = []</span><br><span class="line">            labels[data_type][sentiment] = []</span><br><span class="line">            </span><br><span class="line">            path = os.path.join(data_dir, data_type, sentiment, <span class="string">'*.txt'</span>)</span><br><span class="line">            files = glob.glob(path)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> f <span class="keyword">in</span> files:</span><br><span class="line">                <span class="keyword">with</span> open(f) <span class="keyword">as</span> review:</span><br><span class="line">                    data[data_type][sentiment].append(review.read())</span><br><span class="line">                    <span class="comment"># Here we represent a positive review by '1' and a negative review by '0'</span></span><br><span class="line">                    labels[data_type][sentiment].append(<span class="number">1</span> <span class="keyword">if</span> sentiment == <span class="string">'pos'</span> <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line">                    </span><br><span class="line">            <span class="keyword">assert</span> len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \</span><br><span class="line">                    <span class="string">"&#123;&#125;/&#123;&#125; data size does not match labels size"</span>.format(data_type, sentiment)</span><br><span class="line">                </span><br><span class="line">    <span class="keyword">return</span> data, labels</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data, labels = read_imdb_data()</span><br><span class="line">print(<span class="string">"IMDB reviews: train = &#123;&#125; pos / &#123;&#125; neg, test = &#123;&#125; pos / &#123;&#125; neg"</span>.format(</span><br><span class="line">            len(data[<span class="string">'train'</span>][<span class="string">'pos'</span>]), len(data[<span class="string">'train'</span>][<span class="string">'neg'</span>]),</span><br><span class="line">            len(data[<span class="string">'test'</span>][<span class="string">'pos'</span>]), len(data[<span class="string">'test'</span>][<span class="string">'neg'</span>])))</span><br></pre></td></tr></table></figure>

<pre><code>IMDB reviews: train = 12500 pos / 12500 neg, test = 12500 pos / 12500 neg</code></pre><p>Now that weâ€™ve read the raw training and testing data from the downloaded dataset, we will combine the positive and negative reviews and shuffle the resulting records.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.utils <span class="keyword">import</span> shuffle</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepare_imdb_data</span><span class="params">(data, labels)</span>:</span></span><br><span class="line">    <span class="string">"""Prepare training and test sets from IMDb movie reviews."""</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#Combine positive and negative reviews and labels</span></span><br><span class="line">    data_train = data[<span class="string">'train'</span>][<span class="string">'pos'</span>] + data[<span class="string">'train'</span>][<span class="string">'neg'</span>]</span><br><span class="line">    data_test = data[<span class="string">'test'</span>][<span class="string">'pos'</span>] + data[<span class="string">'test'</span>][<span class="string">'neg'</span>]</span><br><span class="line">    labels_train = labels[<span class="string">'train'</span>][<span class="string">'pos'</span>] + labels[<span class="string">'train'</span>][<span class="string">'neg'</span>]</span><br><span class="line">    labels_test = labels[<span class="string">'test'</span>][<span class="string">'pos'</span>] + labels[<span class="string">'test'</span>][<span class="string">'neg'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#Shuffle reviews and corresponding labels within training and test sets</span></span><br><span class="line">    data_train, labels_train = shuffle(data_train, labels_train)</span><br><span class="line">    data_test, labels_test = shuffle(data_test, labels_test)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Return a unified training data, test data, training labels, test labets</span></span><br><span class="line">    <span class="keyword">return</span> data_train, data_test, labels_train, labels_test</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_X, test_X, train_y, test_y = prepare_imdb_data(data, labels)</span><br><span class="line">print(<span class="string">"IMDb reviews (combined): train = &#123;&#125;, test = &#123;&#125;"</span>.format(len(train_X), len(test_X)))</span><br></pre></td></tr></table></figure>

<pre><code>IMDb reviews (combined): train = 25000, test = 25000</code></pre><p>Now that we have our training and testing sets unified and prepared, we should do a quick check and see an example of the data our model will be trained on. This is generally a good idea as it allows you to see how each of the further processing steps affects the reviews and it also ensures that the data has been loaded correctly.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(train_X[<span class="number">100</span>])</span><br><span class="line">print(train_y[<span class="number">100</span>])</span><br></pre></td></tr></table></figure>

<pre><code>Every new fall line-up show deserves, at least, my &quot;3 strikes and you&apos;re out&quot; policy. I give a comedy 3 chances to make me laugh, that is, 3 complete episodes. After Episode 1, I actually said to the TV,&quot;Cancelled tomorrow&quot;. It was that bad. I have now watched the first 4 episodes of &quot;Cavemen&quot; and have yet to manage even a smirk. Not a titter, a guffaw, a chortle, as a matter of fact, no facial movement at all. I will continue to punish myself by watching every future episode because I am convinced that I am clearly missing something in this show. I&apos;m simply not &quot;getting&quot; it, but I believe that a comedy on a major TV network in prime-time, just HAS to be funny; but there are no laughs from me YET. There&apos;s just no way that ABC would put on the least funniest comedy of all time at 8:00 p.m. I KNOW there has got to be an inside joke that just isn&apos;t jiving with my brain. I&apos;ve read each of the previous comments, I &quot;get&quot; the social aspect of it, but, WHERE ARE THE JOKES ???? I shall continue suffering for at least 30 minutes a week, until I have a light-bulb moment and smack myself in the head shouting &quot;Eureka&quot;.
0</code></pre><p>The first step in processing the reviews is to make sure that any html tags that appear should be removed. In addition we wish to tokenize our input, that way words such as <em>entertained</em> and <em>entertaining</em> are considered the same with regard to sentiment analysis.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"><span class="keyword">from</span> nltk.stem.porter <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">review_to_words</span><span class="params">(review)</span>:</span></span><br><span class="line">    nltk.download(<span class="string">"stopwords"</span>, quiet=<span class="literal">True</span>)</span><br><span class="line">    stemmer = PorterStemmer()</span><br><span class="line">    </span><br><span class="line">    text = BeautifulSoup(review, <span class="string">"html.parser"</span>).get_text() <span class="comment"># Remove HTML tags</span></span><br><span class="line">    text = re.sub(<span class="string">r"[^a-zA-Z0-9]"</span>, <span class="string">" "</span>, text.lower()) <span class="comment"># Convert to lower case</span></span><br><span class="line">    words = text.split() <span class="comment"># Split string into words</span></span><br><span class="line">    words = [w <span class="keyword">for</span> w <span class="keyword">in</span> words <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> stopwords.words(<span class="string">"english"</span>)] <span class="comment"># Remove stopwords</span></span><br><span class="line">    words = [PorterStemmer().stem(w) <span class="keyword">for</span> w <span class="keyword">in</span> words] <span class="comment"># stem</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> words</span><br></pre></td></tr></table></figure>

<p>The <code>review_to_words</code> method defined above uses <code>BeautifulSoup</code> to remove any html tags that appear and uses the <code>nltk</code> package to tokenize the reviews. As a check to ensure we know how everything is working, try applying <code>review_to_words</code> to one of the reviews in the training set.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Apply review_to_words to a review (train_X[100] or any other review)</span></span><br><span class="line">print(review_to_words(train_X[<span class="number">100</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>[&apos;everi&apos;, &apos;new&apos;, &apos;fall&apos;, &apos;line&apos;, &apos;show&apos;, &apos;deserv&apos;, &apos;least&apos;, &apos;3&apos;, &apos;strike&apos;, &apos;polici&apos;, &apos;give&apos;, &apos;comedi&apos;, &apos;3&apos;, &apos;chanc&apos;, &apos;make&apos;, &apos;laugh&apos;, &apos;3&apos;, &apos;complet&apos;, &apos;episod&apos;, &apos;episod&apos;, &apos;1&apos;, &apos;actual&apos;, &apos;said&apos;, &apos;tv&apos;, &apos;cancel&apos;, &apos;tomorrow&apos;, &apos;bad&apos;, &apos;watch&apos;, &apos;first&apos;, &apos;4&apos;, &apos;episod&apos;, &apos;cavemen&apos;, &apos;yet&apos;, &apos;manag&apos;, &apos;even&apos;, &apos;smirk&apos;, &apos;titter&apos;, &apos;guffaw&apos;, &apos;chortl&apos;, &apos;matter&apos;, &apos;fact&apos;, &apos;facial&apos;, &apos;movement&apos;, &apos;continu&apos;, &apos;punish&apos;, &apos;watch&apos;, &apos;everi&apos;, &apos;futur&apos;, &apos;episod&apos;, &apos;convinc&apos;, &apos;clearli&apos;, &apos;miss&apos;, &apos;someth&apos;, &apos;show&apos;, &apos;simpli&apos;, &apos;get&apos;, &apos;believ&apos;, &apos;comedi&apos;, &apos;major&apos;, &apos;tv&apos;, &apos;network&apos;, &apos;prime&apos;, &apos;time&apos;, &apos;funni&apos;, &apos;laugh&apos;, &apos;yet&apos;, &apos;way&apos;, &apos;abc&apos;, &apos;would&apos;, &apos;put&apos;, &apos;least&apos;, &apos;funniest&apos;, &apos;comedi&apos;, &apos;time&apos;, &apos;8&apos;, &apos;00&apos;, &apos;p&apos;, &apos;know&apos;, &apos;got&apos;, &apos;insid&apos;, &apos;joke&apos;, &apos;jive&apos;, &apos;brain&apos;, &apos;read&apos;, &apos;previou&apos;, &apos;comment&apos;, &apos;get&apos;, &apos;social&apos;, &apos;aspect&apos;, &apos;joke&apos;, &apos;shall&apos;, &apos;continu&apos;, &apos;suffer&apos;, &apos;least&apos;, &apos;30&apos;, &apos;minut&apos;, &apos;week&apos;, &apos;light&apos;, &apos;bulb&apos;, &apos;moment&apos;, &apos;smack&apos;, &apos;head&apos;, &apos;shout&apos;, &apos;eureka&apos;]</code></pre><p><strong>Question:</strong> Above we mentioned that <code>review_to_words</code> method removes html formatting and allows us to tokenize the words found in a review, for example, converting <em>entertained</em> and <em>entertaining</em> into <em>entertain</em> so that they are treated as though they are the same word. What else, if anything, does this method do to the input?</p>
<p><strong>Answer:</strong> This method convert all upper case to lower case and all punctuations are removed.</p>
<p>The method below applies the <code>review_to_words</code> method to each of the reviews in the training and testing datasets. In addition it caches the results. This is because performing this processing step can take a long time. This way if you are unable to complete the notebook in the current session, you can come back without needing to process the data a second time.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line">cache_dir = os.path.join(<span class="string">"../cache"</span>, <span class="string">"sentiment_analysis"</span>)  <span class="comment"># where to store cache files</span></span><br><span class="line">os.makedirs(cache_dir, exist_ok=<span class="literal">True</span>)  <span class="comment"># ensure cache directory exists</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_data</span><span class="params">(data_train, data_test, labels_train, labels_test,</span></span></span><br><span class="line"><span class="function"><span class="params">                    cache_dir=cache_dir, cache_file=<span class="string">"preprocessed_data.pkl"</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Convert each review to words; read from cache if available."""</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># If cache_file is not None, try to read from it first</span></span><br><span class="line">    cache_data = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> cache_file <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">with</span> open(os.path.join(cache_dir, cache_file), <span class="string">"rb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">                cache_data = pickle.load(f)</span><br><span class="line">            print(<span class="string">"Read preprocessed data from cache file:"</span>, cache_file)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">pass</span>  <span class="comment"># unable to read from cache, but that's okay</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># If cache is missing, then do the heavy lifting</span></span><br><span class="line">    <span class="keyword">if</span> cache_data <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># Preprocess training and test data to obtain words for each review</span></span><br><span class="line">        <span class="comment">#words_train = list(map(review_to_words, data_train))</span></span><br><span class="line">        <span class="comment">#words_test = list(map(review_to_words, data_test))</span></span><br><span class="line">        words_train = [review_to_words(review) <span class="keyword">for</span> review <span class="keyword">in</span> data_train]</span><br><span class="line">        words_test = [review_to_words(review) <span class="keyword">for</span> review <span class="keyword">in</span> data_test]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Write to cache file for future runs</span></span><br><span class="line">        <span class="keyword">if</span> cache_file <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            cache_data = dict(words_train=words_train, words_test=words_test,</span><br><span class="line">                              labels_train=labels_train, labels_test=labels_test)</span><br><span class="line">            <span class="keyword">with</span> open(os.path.join(cache_dir, cache_file), <span class="string">"wb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">                pickle.dump(cache_data, f)</span><br><span class="line">            print(<span class="string">"Wrote preprocessed data to cache file:"</span>, cache_file)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Unpack data loaded from cache file</span></span><br><span class="line">        words_train, words_test, labels_train, labels_test = (cache_data[<span class="string">'words_train'</span>],</span><br><span class="line">                cache_data[<span class="string">'words_test'</span>], cache_data[<span class="string">'labels_train'</span>], cache_data[<span class="string">'labels_test'</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> words_train, words_test, labels_train, labels_test</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Preprocess data</span></span><br><span class="line">train_X, test_X, train_y, test_y = preprocess_data(train_X, test_X, train_y, test_y)</span><br></pre></td></tr></table></figure>

<pre><code>Read preprocessed data from cache file: preprocessed_data.pkl</code></pre><h2 id="Transform-the-data"><a href="#Transform-the-data" class="headerlink" title="Transform the data"></a>Transform the data</h2><p>In the XGBoost notebook we transformed the data from its word representation to a bag-of-words feature representation. For the model we are going to construct in this notebook we will construct a feature representation which is very similar. To start, we will represent each word as an integer. Of course, some of the words that appear in the reviews occur very infrequently and so likely donâ€™t contain much information for the purposes of sentiment analysis. The way we will deal with this problem is that we will fix the size of our working vocabulary and we will only include the words that appear most frequently. We will then combine all of the infrequent words into a single category and, in our case, we will label it as <code>1</code>.</p>
<p>Since we will be using a recurrent neural network, it will be convenient if the length of each review is the same. To do this, we will fix a size for our reviews and then pad short reviews with the category â€˜no wordâ€™ (which we will label <code>0</code>) and truncate long reviews.</p>
<h3 id="TODO-Create-a-word-dictionary"><a href="#TODO-Create-a-word-dictionary" class="headerlink" title="(TODO) Create a word dictionary"></a>(TODO) Create a word dictionary</h3><p>To begin with, we need to construct a way to map words that appear in the reviews to integers. Here we fix the size of our vocabulary (including the â€˜no wordâ€™ and â€˜infrequentâ€™ categories) to be <code>5000</code> but you may wish to change this to see how it affects the model.</p>
<blockquote>
<p><strong>TODO:</strong> Complete the implementation for the <code>build_dict()</code> method below. Note that even though the vocab_size is set to <code>5000</code>, we only want to construct a mapping for the most frequently appearing <code>4998</code> words. This is because we want to reserve the special labels <code>0</code> for â€˜no wordâ€™ and <code>1</code> for â€˜infrequent wordâ€™.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_dict</span><span class="params">(data, vocab_size = <span class="number">5000</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Construct and return a dictionary mapping each of the most frequently appearing words to a unique integer."""</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Determine how often each word appears in `data`. Note that `data` is a list of sentences and that a</span></span><br><span class="line">    <span class="comment">#       sentence is a list of words.</span></span><br><span class="line">    </span><br><span class="line">    word_count = &#123;&#125; <span class="comment"># A dict storing the words that appear in the reviews along with how often they occur</span></span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> data:</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> sentence:</span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">in</span> word_count:</span><br><span class="line">                word_count[word] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">                word_count[word] = <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Sort the words found in `data` so that sorted_words[0] is the most frequently appearing word and</span></span><br><span class="line">    <span class="comment">#       sorted_words[-1] is the least frequently appearing word.</span></span><br><span class="line">    </span><br><span class="line">    sorted_words = sorted(word_count, key=word_count.get, reverse=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    word_dict = &#123;&#125; <span class="comment"># This is what we are building, a dictionary that translates words into integers</span></span><br><span class="line">    <span class="keyword">for</span> idx, word <span class="keyword">in</span> enumerate(sorted_words[:vocab_size - <span class="number">2</span>]): <span class="comment"># The -2 is so that we save room for the 'no word'</span></span><br><span class="line">        word_dict[word] = idx + <span class="number">2</span>                              <span class="comment"># 'infrequent' labels</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> word_dict</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">word_dict = build_dict(train_X)</span><br></pre></td></tr></table></figure>

<p><strong>Question:</strong> What are the five most frequently appearing (tokenized) words in the training set? Does it makes sense that these words appear frequently in the training set?</p>
<p><strong>Answer:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print([i <span class="keyword">for</span> i <span class="keyword">in</span> word_dict.keys()][:<span class="number">5</span>])</span><br></pre></td></tr></table></figure>

<pre><code>[&apos;movi&apos;, &apos;film&apos;, &apos;one&apos;, &apos;like&apos;, &apos;time&apos;]</code></pre><p>This makes sense since those words are common in movie reviews</p>
<h3 id="Save-word-dict"><a href="#Save-word-dict" class="headerlink" title="Save word_dict"></a>Save <code>word_dict</code></h3><p>Later on when we construct an endpoint which processes a submitted review we will need to make use of the <code>word_dict</code> which we have created. As such, we will save it to a file now for future use.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_dir = <span class="string">'../data/pytorch'</span> <span class="comment"># The folder we will use for storing data</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(data_dir): <span class="comment"># Make sure that the folder exists</span></span><br><span class="line">    os.makedirs(data_dir)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> open(os.path.join(data_dir, <span class="string">'word_dict.pkl'</span>), <span class="string">"wb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    pickle.dump(word_dict, f)</span><br></pre></td></tr></table></figure>

<h3 id="Transform-the-reviews"><a href="#Transform-the-reviews" class="headerlink" title="Transform the reviews"></a>Transform the reviews</h3><p>Now that we have our word dictionary which allows us to transform the words appearing in the reviews into integers, it is time to make use of it and convert our reviews to their integer sequence representation, making sure to pad or truncate to a fixed length, which in our case is <code>500</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_and_pad</span><span class="params">(word_dict, sentence, pad=<span class="number">500</span>)</span>:</span></span><br><span class="line">    NOWORD = <span class="number">0</span> <span class="comment"># We will use 0 to represent the 'no word' category</span></span><br><span class="line">    INFREQ = <span class="number">1</span> <span class="comment"># and we use 1 to represent the infrequent words, i.e., words not appearing in word_dict</span></span><br><span class="line">    </span><br><span class="line">    working_sentence = [NOWORD] * pad</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> word_index, word <span class="keyword">in</span> enumerate(sentence[:pad]):</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> word_dict:</span><br><span class="line">            working_sentence[word_index] = word_dict[word]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            working_sentence[word_index] = INFREQ</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> working_sentence, min(len(sentence), pad)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_and_pad_data</span><span class="params">(word_dict, data, pad=<span class="number">500</span>)</span>:</span></span><br><span class="line">    result = []</span><br><span class="line">    lengths = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> data:</span><br><span class="line">        converted, leng = convert_and_pad(word_dict, sentence, pad)</span><br><span class="line">        result.append(converted)</span><br><span class="line">        lengths.append(leng)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> np.array(result), np.array(lengths)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_X, train_X_len = convert_and_pad_data(word_dict, train_X)</span><br><span class="line">test_X, test_X_len = convert_and_pad_data(word_dict, test_X)</span><br></pre></td></tr></table></figure>

<p>As a quick check to make sure that things are working as intended, check to see what one of the reviews in the training set looks like after having been processeed. Does this look reasonable? What is the length of a review in the training set?</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Use this cell to examine one of the processed reviews to make sure everything is working as intended.</span></span><br><span class="line">print(<span class="string">'The processed review is:\n&#123;&#125;.'</span>.format(train_X[<span class="number">7</span>]))</span><br><span class="line">print(<span class="string">'The length of the review is:\n&#123;&#125;.'</span>.format(train_X_len[<span class="number">7</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>The processed review is:
[ 135    3   28   31  272 1280    1    1  669 1663    1 3417    3   94
   96  813  147    3   23    8   59   30 1477    1    4    1   43  110
  709    3   33 2437  230 4095  358    1   50  575    1   55   75 3439
    1 1770   43  260   43   68    1  331   94 1549  419   42  601  283
 1477   50  255 1310    1   55  420   50   40   33   75   29   50   22
  147  580   74   59   30    4  395    2  119  324 1053  232    0    0
    0    0    0    0    0    0    0    0    0    0    0    0    0    0
    0    0    0    0    0    0    0    0    0    0    0    0    0    0
    0    0    0    0    0    0    0    0    0    0    0    0    0    0
    0    0    0    0    0    0    0    0    0    0    0    0    0    0
    0    0    0    0    0    0    0    0    0    0    0    0    0    0
    0    0    0    0    0    0    0    0    0    0    0    0    0    0
    0    0    0    0    0    0    0    0    0    0    0    0    0    0
    0    0    0    0    0    0    0    0    0    0    0    0    0    0
    0    0    0    0    0    0    0    0    0    0    0    0    0    0
    0    0    0    0    0    0    0    0    0    0    0    0    0    0
    0    0    0    0    0    0    0    0    0    0    0    0    0    0
    0    0    0    0    0    0    0    0    0    0    0    0    0    0
    0    0    0    0    0    0    0    0    0    0    0    0    0    0
    0    0    0    0    0    0    0    0    0    0    0    0    0    0
    0    0    0    0    0    0    0    0    0    0    0    0    0    0
    0    0    0    0    0    0    0    0    0    0    0    0    0    0
    0    0    0    0    0    0    0    0    0    0    0    0    0    0
    0    0    0    0    0    0    0    0    0    0    0    0    0    0
    0    0    0    0    0    0    0    0    0    0    0    0    0    0
    0    0    0    0    0    0    0    0    0    0    0    0    0    0
    0    0    0    0    0    0    0    0    0    0    0    0    0    0
    0    0    0    0    0    0    0    0    0    0    0    0    0    0
    0    0    0    0    0    0    0    0    0    0    0    0    0    0
    0    0    0    0    0    0    0    0    0    0    0    0    0    0
    0    0    0    0    0    0    0    0    0    0    0    0    0    0
    0    0    0    0    0    0    0    0    0    0    0    0    0    0
    0    0    0    0    0    0    0    0    0    0    0    0    0    0
    0    0    0    0    0    0    0    0    0    0    0    0    0    0
    0    0    0    0    0    0    0    0    0    0    0    0    0    0
    0    0    0    0    0    0    0    0    0    0].
The length of the review is:
82.</code></pre><p>It looks reasonable, and the length of the review is 82.</p>
<p><strong>Question:</strong> In the cells above we use the <code>preprocess_data</code> and <code>convert_and_pad_data</code> methods to process both the training and testing set. Why or why not might this be a problem?</p>
<p><strong>Answer:</strong> It might not be a problem for training the model since all reviews are converted into the same length. However, it might require more memory since shorter reviews are converted into longer format.</p>
<h2 id="Step-3-Upload-the-data-to-S3"><a href="#Step-3-Upload-the-data-to-S3" class="headerlink" title="Step 3: Upload the data to S3"></a>Step 3: Upload the data to S3</h2><p>As in the XGBoost notebook, we will need to upload the training dataset to S3 in order for our training code to access it. For now we will save it locally and we will upload to S3 later on.</p>
<h3 id="Save-the-processed-training-dataset-locally"><a href="#Save-the-processed-training-dataset-locally" class="headerlink" title="Save the processed training dataset locally"></a>Save the processed training dataset locally</h3><p>It is important to note the format of the data that we are saving as we will need to know it when we write the training code. In our case, each row of the dataset has the form <code>label</code>, <code>length</code>, <code>review[500]</code> where <code>review[500]</code> is a sequence of <code>500</code> integers representing the words in the review.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">    </span><br><span class="line">pd.concat([pd.DataFrame(train_y), pd.DataFrame(train_X_len), pd.DataFrame(train_X)], axis=<span class="number">1</span>) \</span><br><span class="line">        .to_csv(os.path.join(data_dir, <span class="string">'train.csv'</span>), header=<span class="literal">False</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<h3 id="Uploading-the-training-data"><a href="#Uploading-the-training-data" class="headerlink" title="Uploading the training data"></a>Uploading the training data</h3><p>Next, we need to upload the training data to the SageMaker default S3 bucket so that we can provide access to it while training our model.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sagemaker</span><br><span class="line"></span><br><span class="line">sagemaker_session = sagemaker.Session()</span><br><span class="line"></span><br><span class="line">bucket = sagemaker_session.default_bucket()</span><br><span class="line">prefix = <span class="string">'sagemaker/sentiment_rnn'</span></span><br><span class="line"></span><br><span class="line">role = sagemaker.get_execution_role()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)</span><br></pre></td></tr></table></figure>

<p><strong>NOTE:</strong> The cell above uploads the entire contents of our data directory. This includes the <code>word_dict.pkl</code> file. This is fortunate as we will need this later on when we create an endpoint that accepts an arbitrary review. For now, we will just take note of the fact that it resides in the data directory (and so also in the S3 training bucket) and that we will need to make sure it gets saved in the model directory.</p>
<h2 id="Step-4-Build-and-Train-the-PyTorch-Model"><a href="#Step-4-Build-and-Train-the-PyTorch-Model" class="headerlink" title="Step 4: Build and Train the PyTorch Model"></a>Step 4: Build and Train the PyTorch Model</h2><p>In the XGBoost notebook we discussed what a model is in the SageMaker framework. In particular, a model comprises three objects</p>
<ul>
<li>Model Artifacts,</li>
<li>Training Code, and</li>
<li>Inference Code,</li>
</ul>
<p>each of which interact with one another. In the XGBoost example we used training and inference code that was provided by Amazon. Here we will still be using containers provided by Amazon with the added benefit of being able to include our own custom code.</p>
<p>We will start by implementing our own neural network in PyTorch along with a training script. For the purposes of this project we have provided the necessary model object in the <code>model.py</code> file, inside of the <code>train</code> folder. You can see the provided implementation by running the cell below.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!pygmentize train/model.py</span><br></pre></td></tr></table></figure>

<pre><code>[34mimport[39;49;00m [04m[36mtorch.nn[39;49;00m [34mas[39;49;00m [04m[36mnn[39;49;00m

[34mclass[39;49;00m [04m[32mLSTMClassifier[39;49;00m(nn.Module):
    [33m&quot;&quot;&quot;[39;49;00m
[33m    This is the simple RNN model we will be using to perform Sentiment Analysis.[39;49;00m
[33m    &quot;&quot;&quot;[39;49;00m

    [34mdef[39;49;00m [32m__init__[39;49;00m([36mself[39;49;00m, embedding_dim, hidden_dim, vocab_size):
        [33m&quot;&quot;&quot;[39;49;00m
[33m        Initialize the model by settingg up the various layers.[39;49;00m
[33m        &quot;&quot;&quot;[39;49;00m
        [36msuper[39;49;00m(LSTMClassifier, [36mself[39;49;00m).[32m__init__[39;49;00m()

        [36mself[39;49;00m.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=[34m0[39;49;00m)
        [36mself[39;49;00m.lstm = nn.LSTM(embedding_dim, hidden_dim)
        [36mself[39;49;00m.dense = nn.Linear(in_features=hidden_dim, out_features=[34m1[39;49;00m)
        [36mself[39;49;00m.sig = nn.Sigmoid()

        [36mself[39;49;00m.word_dict = [36mNone[39;49;00m

    [34mdef[39;49;00m [32mforward[39;49;00m([36mself[39;49;00m, x):
        [33m&quot;&quot;&quot;[39;49;00m
[33m        Perform a forward pass of our model on some input.[39;49;00m
[33m        &quot;&quot;&quot;[39;49;00m
        x = x.t()
        lengths = x[[34m0[39;49;00m,:]
        reviews = x[[34m1[39;49;00m:,:]
        embeds = [36mself[39;49;00m.embedding(reviews)
        lstm_out, _ = [36mself[39;49;00m.lstm(embeds)
        out = [36mself[39;49;00m.dense(lstm_out)
        out = out[lengths - [34m1[39;49;00m, [36mrange[39;49;00m([36mlen[39;49;00m(lengths))]
        [34mreturn[39;49;00m [36mself[39;49;00m.sig(out.squeeze())</code></pre><p>The important takeaway from the implementation provided is that there are three parameters that we may wish to tweak to improve the performance of our model. These are the embedding dimension, the hidden dimension and the size of the vocabulary. We will likely want to make these parameters configurable in the training script so that if we wish to modify them we do not need to modify the script itself. We will see how to do this later on. To start we will write some of the training code in the notebook so that we can more easily diagnose any issues that arise.</p>
<p>First we will load a small portion of the training data set to use as a sample. It would be very time consuming to try and train the model completely in the notebook as we do not have access to a gpu and the compute instance that we are using is not particularly powerful. However, we can work on a small bit of the data to get a feel for how our training script is behaving.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.utils.data</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read in only the first 250 rows</span></span><br><span class="line">train_sample = pd.read_csv(os.path.join(data_dir, <span class="string">'train.csv'</span>), header=<span class="literal">None</span>, names=<span class="literal">None</span>, nrows=<span class="number">250</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Turn the input pandas dataframe into tensors</span></span><br><span class="line">train_sample_y = torch.from_numpy(train_sample[[<span class="number">0</span>]].values).float().squeeze()</span><br><span class="line">train_sample_X = torch.from_numpy(train_sample.drop([<span class="number">0</span>], axis=<span class="number">1</span>).values).long()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build the dataset</span></span><br><span class="line">train_sample_ds = torch.utils.data.TensorDataset(train_sample_X, train_sample_y)</span><br><span class="line"><span class="comment"># Build the dataloader</span></span><br><span class="line">train_sample_dl = torch.utils.data.DataLoader(train_sample_ds, batch_size=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>

<h3 id="TODO-Writing-the-training-method"><a href="#TODO-Writing-the-training-method" class="headerlink" title="(TODO) Writing the training method"></a>(TODO) Writing the training method</h3><p>Next we need to write the training code itself. This should be very similar to training methods that you have written before to train PyTorch models. We will leave any difficult aspects such as model saving / loading and parameter loading until a little later.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(model, train_loader, epochs, optimizer, loss_fn, device)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, epochs + <span class="number">1</span>):</span><br><span class="line">        model.train()</span><br><span class="line">        total_loss = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> train_loader:         </span><br><span class="line">            batch_X, batch_y = batch</span><br><span class="line">            </span><br><span class="line">            batch_X = batch_X.to(device)</span><br><span class="line">            batch_y = batch_y.to(device)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># <span class="doctag">TODO:</span> Complete this train method to train the model provided.</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            output = model.forward(batch_X)</span><br><span class="line">            loss = loss_fn(output, batch_y)</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            </span><br><span class="line">            total_loss += loss.data.item()</span><br><span class="line">        print(<span class="string">"Epoch: &#123;&#125;, BCELoss: &#123;&#125;"</span>.format(epoch, total_loss / len(train_loader)))</span><br></pre></td></tr></table></figure>

<p>Supposing we have the training method above, we will test that it is working by writing a bit of code in the notebook that executes our training method on the small sample training set that we loaded earlier. The reason for doing this in the notebook is so that we have an opportunity to fix any errors that arise early when they are easier to diagnose.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> train.model <span class="keyword">import</span> LSTMClassifier</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">model = LSTMClassifier(<span class="number">32</span>, <span class="number">100</span>, <span class="number">5000</span>).to(device)</span><br><span class="line">optimizer = optim.Adam(model.parameters())</span><br><span class="line">loss_fn = torch.nn.BCELoss()</span><br><span class="line"></span><br><span class="line">train(model, train_sample_dl, <span class="number">5</span>, optimizer, loss_fn, device)</span><br></pre></td></tr></table></figure>

<pre><code>Epoch: 1, BCELoss: 0.6931784152984619
Epoch: 2, BCELoss: 0.683626139163971
Epoch: 3, BCELoss: 0.6755886435508728
Epoch: 4, BCELoss: 0.6667218565940857
Epoch: 5, BCELoss: 0.6559338450431824</code></pre><p>In order to construct a PyTorch model using SageMaker we must provide SageMaker with a training script. We may optionally include a directory which will be copied to the container and from which our training code will be run. When the training container is executed it will check the uploaded directory (if there is one) for a <code>requirements.txt</code> file and install any required Python libraries, after which the training script will be run.</p>
<h3 id="TODO-Training-the-model"><a href="#TODO-Training-the-model" class="headerlink" title="(TODO) Training the model"></a>(TODO) Training the model</h3><p>When a PyTorch model is constructed in SageMaker, an entry point must be specified. This is the Python file which will be executed when the model is trained. Inside of the <code>train</code> directory is a file called <code>train.py</code> which has been provided and which contains most of the necessary code to train our model. The only thing that is missing is the implementation of the <code>train()</code> method which you wrote earlier in this notebook.</p>
<p><strong>TODO</strong>: Copy the <code>train()</code> method written above and paste it into the <code>train/train.py</code> file where required.</p>
<p>The way that SageMaker passes hyperparameters to the training script is by way of arguments. These arguments can then be parsed and used in the training script. To see how this is done take a look at the provided <code>train/train.py</code> file.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sagemaker.pytorch <span class="keyword">import</span> PyTorch</span><br><span class="line"></span><br><span class="line">estimator = PyTorch(entry_point=<span class="string">"train.py"</span>,</span><br><span class="line">                    source_dir=<span class="string">"train"</span>,</span><br><span class="line">                    role=role,</span><br><span class="line">                    framework_version=<span class="string">'0.4.0'</span>,</span><br><span class="line">                    train_instance_count=<span class="number">1</span>,</span><br><span class="line">                    train_instance_type=<span class="string">'ml.p2.xlarge'</span>,</span><br><span class="line">                    hyperparameters=&#123;</span><br><span class="line">                        <span class="string">'epochs'</span>: <span class="number">10</span>,</span><br><span class="line">                        <span class="string">'hidden_dim'</span>: <span class="number">200</span>,</span><br><span class="line">                    &#125;)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">estimator.fit(&#123;<span class="string">'training'</span>: input_data&#125;)</span><br></pre></td></tr></table></figure>

<pre><code>2020-02-23 10:29:09 Starting - Starting the training job...
2020-02-23 10:29:11 Starting - Launching requested ML instances......
2020-02-23 10:30:10 Starting - Preparing the instances for training......
2020-02-23 10:31:12 Downloading - Downloading input data...
2020-02-23 10:31:56 Training - Downloading the training image..[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device[0m
[34mbash: no job control in this shell[0m
[34m2020-02-23 10:32:19,999 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training[0m
[34m2020-02-23 10:32:20,024 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.[0m

2020-02-23 10:32:19 Training - Training image download completed. Training in progress.[34m2020-02-23 10:32:26,248 sagemaker_pytorch_container.training INFO     Invoking user training script.[0m
[34m2020-02-23 10:32:26,469 sagemaker-containers INFO     Module train does not provide a setup.py. [0m
[34mGenerating setup.py[0m
[34m2020-02-23 10:32:26,470 sagemaker-containers INFO     Generating setup.cfg[0m
[34m2020-02-23 10:32:26,470 sagemaker-containers INFO     Generating MANIFEST.in[0m
[34m2020-02-23 10:32:26,470 sagemaker-containers INFO     Installing module with the following command:[0m
[34m/usr/bin/python -m pip install -U . -r requirements.txt[0m
[34mProcessing /opt/ml/code[0m
[34mCollecting pandas (from -r requirements.txt (line 1))[0m
[34m  Downloading https://files.pythonhosted.org/packages/74/24/0cdbf8907e1e3bc5a8da03345c23cbed7044330bb8f73bb12e711a640a00/pandas-0.24.2-cp35-cp35m-manylinux1_x86_64.whl (10.0MB)[0m
[34mCollecting numpy (from -r requirements.txt (line 2))
  Downloading https://files.pythonhosted.org/packages/52/e6/1715e592ef47f28f3f50065322423bb75619ed2f7c24be86380ecc93503c/numpy-1.18.1-cp35-cp35m-manylinux1_x86_64.whl (19.9MB)[0m
[34mCollecting nltk (from -r requirements.txt (line 3))
  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)[0m
[34mCollecting beautifulsoup4 (from -r requirements.txt (line 4))[0m
[34m  Downloading https://files.pythonhosted.org/packages/cb/a1/c698cf319e9cfed6b17376281bd0efc6bfc8465698f54170ef60a485ab5d/beautifulsoup4-4.8.2-py3-none-any.whl (106kB)[0m
[34mCollecting html5lib (from -r requirements.txt (line 5))
  Downloading https://files.pythonhosted.org/packages/a5/62/bbd2be0e7943ec8504b517e62bab011b4946e1258842bc159e5dfde15b96/html5lib-1.0.1-py2.py3-none-any.whl (117kB)[0m
[34mRequirement already satisfied, skipping upgrade: python-dateutil&gt;=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas-&gt;-r requirements.txt (line 1)) (2.7.5)[0m
[34mCollecting pytz&gt;=2011k (from pandas-&gt;-r requirements.txt (line 1))
  Downloading https://files.pythonhosted.org/packages/e7/f9/f0b53f88060247251bf481fa6ea62cd0d25bf1b11a87888e53ce5b7c8ad2/pytz-2019.3-py2.py3-none-any.whl (509kB)[0m
[34mRequirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.5/dist-packages (from nltk-&gt;-r requirements.txt (line 3)) (1.11.0)[0m
[34mCollecting soupsieve&gt;=1.2 (from beautifulsoup4-&gt;-r requirements.txt (line 4))
  Downloading https://files.pythonhosted.org/packages/05/cf/ea245e52f55823f19992447b008bcbb7f78efc5960d77f6c34b5b45b36dd/soupsieve-2.0-py2.py3-none-any.whl[0m
[34mCollecting webencodings (from html5lib-&gt;-r requirements.txt (line 5))
  Downloading https://files.pythonhosted.org/packages/f4/24/2a3e3df732393fed8b3ebf2ec078f05546de641fe1b667ee316ec1dcf3b7/webencodings-0.5.1-py2.py3-none-any.whl[0m
[34mBuilding wheels for collected packages: nltk, train
  Running setup.py bdist_wheel for nltk: started[0m
[34m  Running setup.py bdist_wheel for nltk: finished with status &apos;done&apos;
  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483
  Running setup.py bdist_wheel for train: started
  Running setup.py bdist_wheel for train: finished with status &apos;done&apos;
  Stored in directory: /tmp/pip-ephem-wheel-cache-1z_fwhiw/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3[0m
[34mSuccessfully built nltk train[0m
[34mInstalling collected packages: pytz, numpy, pandas, nltk, soupsieve, beautifulsoup4, webencodings, html5lib, train[0m
[34m  Found existing installation: numpy 1.15.4
    Uninstalling numpy-1.15.4:
      Successfully uninstalled numpy-1.15.4[0m
[34mSuccessfully installed beautifulsoup4-4.8.2 html5lib-1.0.1 nltk-3.4.5 numpy-1.18.1 pandas-0.24.2 pytz-2019.3 soupsieve-2.0 train-1.0.0 webencodings-0.5.1[0m
[34mYou are using pip version 18.1, however version 20.0.2 is available.[0m
[34mYou should consider upgrading via the &apos;pip install --upgrade pip&apos; command.[0m
[34m2020-02-23 10:32:39,087 sagemaker-containers INFO     Invoking user script
[0m
[34mTraining Env:
[0m
[34m{
    &quot;channel_input_dirs&quot;: {
        &quot;training&quot;: &quot;/opt/ml/input/data/training&quot;
    },
    &quot;current_host&quot;: &quot;algo-1&quot;,
    &quot;module_dir&quot;: &quot;s3://sagemaker-us-east-2-621192006701/sagemaker-pytorch-2020-02-23-10-29-09-269/source/sourcedir.tar.gz&quot;,
    &quot;user_entry_point&quot;: &quot;train.py&quot;,
    &quot;output_intermediate_dir&quot;: &quot;/opt/ml/output/intermediate&quot;,
    &quot;model_dir&quot;: &quot;/opt/ml/model&quot;,
    &quot;output_data_dir&quot;: &quot;/opt/ml/output/data&quot;,
    &quot;hyperparameters&quot;: {
        &quot;hidden_dim&quot;: 200,
        &quot;epochs&quot;: 10
    },
    &quot;framework_module&quot;: &quot;sagemaker_pytorch_container.training:main&quot;,
    &quot;resource_config&quot;: {
        &quot;network_interface_name&quot;: &quot;eth0&quot;,
        &quot;current_host&quot;: &quot;algo-1&quot;,
        &quot;hosts&quot;: [
            &quot;algo-1&quot;
        ]
    },
    &quot;num_gpus&quot;: 1,
    &quot;output_dir&quot;: &quot;/opt/ml/output&quot;,
    &quot;num_cpus&quot;: 4,
    &quot;log_level&quot;: 20,
    &quot;network_interface_name&quot;: &quot;eth0&quot;,
    &quot;input_config_dir&quot;: &quot;/opt/ml/input/config&quot;,
    &quot;additional_framework_parameters&quot;: {},
    &quot;job_name&quot;: &quot;sagemaker-pytorch-2020-02-23-10-29-09-269&quot;,
    &quot;input_data_config&quot;: {
        &quot;training&quot;: {
            &quot;TrainingInputMode&quot;: &quot;File&quot;,
            &quot;S3DistributionType&quot;: &quot;FullyReplicated&quot;,
            &quot;RecordWrapperType&quot;: &quot;None&quot;
        }
    },
    &quot;module_name&quot;: &quot;train&quot;,
    &quot;hosts&quot;: [
        &quot;algo-1&quot;
    ],
    &quot;input_dir&quot;: &quot;/opt/ml/input&quot;[0m
[34m}
[0m
[34mEnvironment variables:
[0m
[34mSM_NUM_CPUS=4[0m
[34mSM_HP_EPOCHS=10[0m
[34mSM_TRAINING_ENV={&quot;additional_framework_parameters&quot;:{},&quot;channel_input_dirs&quot;:{&quot;training&quot;:&quot;/opt/ml/input/data/training&quot;},&quot;current_host&quot;:&quot;algo-1&quot;,&quot;framework_module&quot;:&quot;sagemaker_pytorch_container.training:main&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;hyperparameters&quot;:{&quot;epochs&quot;:10,&quot;hidden_dim&quot;:200},&quot;input_config_dir&quot;:&quot;/opt/ml/input/config&quot;,&quot;input_data_config&quot;:{&quot;training&quot;:{&quot;RecordWrapperType&quot;:&quot;None&quot;,&quot;S3DistributionType&quot;:&quot;FullyReplicated&quot;,&quot;TrainingInputMode&quot;:&quot;File&quot;}},&quot;input_dir&quot;:&quot;/opt/ml/input&quot;,&quot;job_name&quot;:&quot;sagemaker-pytorch-2020-02-23-10-29-09-269&quot;,&quot;log_level&quot;:20,&quot;model_dir&quot;:&quot;/opt/ml/model&quot;,&quot;module_dir&quot;:&quot;s3://sagemaker-us-east-2-621192006701/sagemaker-pytorch-2020-02-23-10-29-09-269/source/sourcedir.tar.gz&quot;,&quot;module_name&quot;:&quot;train&quot;,&quot;network_interface_name&quot;:&quot;eth0&quot;,&quot;num_cpus&quot;:4,&quot;num_gpus&quot;:1,&quot;output_data_dir&quot;:&quot;/opt/ml/output/data&quot;,&quot;output_dir&quot;:&quot;/opt/ml/output&quot;,&quot;output_intermediate_dir&quot;:&quot;/opt/ml/output/intermediate&quot;,&quot;resource_config&quot;:{&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;},&quot;user_entry_point&quot;:&quot;train.py&quot;}[0m
[34mSM_MODULE_NAME=train[0m
[34mSM_MODEL_DIR=/opt/ml/model[0m
[34mSM_INPUT_DATA_CONFIG={&quot;training&quot;:{&quot;RecordWrapperType&quot;:&quot;None&quot;,&quot;S3DistributionType&quot;:&quot;FullyReplicated&quot;,&quot;TrainingInputMode&quot;:&quot;File&quot;}}[0m
[34mSM_NUM_GPUS=1[0m
[34mSM_NETWORK_INTERFACE_NAME=eth0[0m
[34mSM_HPS={&quot;epochs&quot;:10,&quot;hidden_dim&quot;:200}[0m
[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main[0m
[34mSM_OUTPUT_DIR=/opt/ml/output[0m
[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate[0m
[34mSM_FRAMEWORK_PARAMS={}[0m
[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-621192006701/sagemaker-pytorch-2020-02-23-10-29-09-269/source/sourcedir.tar.gz[0m
[34mPYTHONPATH=/usr/local/bin:/usr/lib/python35.zip:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/usr/lib/python3.5/lib-dynload:/usr/local/lib/python3.5/dist-packages:/usr/lib/python3/dist-packages[0m
[34mSM_CHANNELS=[&quot;training&quot;][0m
[34mSM_USER_ARGS=[&quot;--epochs&quot;,&quot;10&quot;,&quot;--hidden_dim&quot;,&quot;200&quot;][0m
[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training[0m
[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config[0m
[34mSM_HOSTS=[&quot;algo-1&quot;][0m
[34mSM_RESOURCE_CONFIG={&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;}[0m
[34mSM_USER_ENTRY_POINT=train.py[0m
[34mSM_HP_HIDDEN_DIM=200[0m
[34mSM_INPUT_DIR=/opt/ml/input[0m
[34mSM_CURRENT_HOST=algo-1[0m
[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data[0m
[34mSM_LOG_LEVEL=20
[0m
[34mInvoking script with the following command:
[0m
[34m/usr/bin/python -m train --epochs 10 --hidden_dim 200

[0m
[34mUsing device cuda.[0m
[34mGet train data loader.[0m
[34mModel loaded with embedding_dim 32, hidden_dim 200, vocab_size 5000.[0m
[34mEpoch: 1, BCELoss: 0.6749516141657926[0m
[34mEpoch: 2, BCELoss: 0.5924099878389009[0m
[34mEpoch: 3, BCELoss: 0.50403351017407[0m
[34mEpoch: 4, BCELoss: 0.4240985257284982[0m
[34mEpoch: 5, BCELoss: 0.38010544862066[0m
[34mEpoch: 6, BCELoss: 0.3434085663484067[0m
[34mEpoch: 7, BCELoss: 0.319852275811896[0m
[34mEpoch: 8, BCELoss: 0.32171173728242214[0m
[34mEpoch: 9, BCELoss: 0.29324838214991045[0m

2020-02-23 10:35:41 Uploading - Uploading generated training model[34mEpoch: 10, BCELoss: 0.2656841928861579[0m
[34m2020-02-23 10:35:40,457 sagemaker-containers INFO     Reporting training SUCCESS[0m

2020-02-23 10:35:48 Completed - Training job completed
Training seconds: 276
Billable seconds: 276</code></pre><h2 id="Step-5-Testing-the-model"><a href="#Step-5-Testing-the-model" class="headerlink" title="Step 5: Testing the model"></a>Step 5: Testing the model</h2><p>As mentioned at the top of this notebook, we will be testing this model by first deploying it and then sending the testing data to the deployed endpoint. We will do this so that we can make sure that the deployed model is working correctly.</p>
<h2 id="Step-6-Deploy-the-model-for-testing"><a href="#Step-6-Deploy-the-model-for-testing" class="headerlink" title="Step 6: Deploy the model for testing"></a>Step 6: Deploy the model for testing</h2><p>Now that we have trained our model, we would like to test it to see how it performs. Currently our model takes input of the form <code>review_length, review[500]</code> where <code>review[500]</code> is a sequence of <code>500</code> integers which describe the words present in the review, encoded using <code>word_dict</code>. Fortunately for us, SageMaker provides built-in inference code for models with simple inputs such as this.</p>
<p>There is one thing that we need to provide, however, and that is a function which loads the saved model. This function must be called <code>model_fn()</code> and takes as its only parameter a path to the directory where the model artifacts are stored. This function must also be present in the python file which we specified as the entry point. In our case the model loading function has been provided and so no changes need to be made.</p>
<p><strong>NOTE</strong>: When the built-in inference code is run it must import the <code>model_fn()</code> method from the <code>train.py</code> file. This is why the training code is wrapped in a main guard ( ie, <code>if __name__ == &#39;__main__&#39;:</code> )</p>
<p>Since we donâ€™t need to change anything in the code that was uploaded during training, we can simply deploy the current model as-is.</p>
<p><strong>NOTE:</strong> When deploying a model you are asking SageMaker to launch an compute instance that will wait for data to be sent to it. As a result, this compute instance will continue to run until <em>you</em> shut it down. This is important to know since the cost of a deployed endpoint depends on how long it has been running for.</p>
<p>In other words <strong>If you are no longer using a deployed endpoint, shut it down!</strong></p>
<p><strong>TODO:</strong> Deploy the trained model.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Deploy the trained model</span></span><br><span class="line">predictor = estimator.deploy(initial_instance_count=<span class="number">1</span>, instance_type=<span class="string">'ml.m4.xlarge'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>-----------!</code></pre><h2 id="Step-7-Use-the-model-for-testing"><a href="#Step-7-Use-the-model-for-testing" class="headerlink" title="Step 7 - Use the model for testing"></a>Step 7 - Use the model for testing</h2><p>Once deployed, we can read in the test data and send it off to our deployed model to get some results. Once we collect all of the results we can determine how accurate our model is.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_X = pd.concat([pd.DataFrame(test_X_len), pd.DataFrame(test_X)], axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># We split the data into chunks and send each chunk seperately, accumulating the results.</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(data, rows=<span class="number">512</span>)</span>:</span></span><br><span class="line">    split_array = np.array_split(data, int(data.shape[<span class="number">0</span>] / float(rows) + <span class="number">1</span>))</span><br><span class="line">    predictions = np.array([])</span><br><span class="line">    <span class="keyword">for</span> array <span class="keyword">in</span> split_array:</span><br><span class="line">        predictions = np.append(predictions, predictor.predict(array))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> predictions</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predictions = predict(test_X.values)</span><br><span class="line">predictions = [round(num) <span class="keyword">for</span> num <span class="keyword">in</span> predictions]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">accuracy_score(test_y, predictions)</span><br></pre></td></tr></table></figure>




<pre><code>0.8506</code></pre><p><strong>Question:</strong> How does this model compare to the XGBoost model you created earlier? Why might these two models perform differently on this dataset? Which do <em>you</em> think is better for sentiment analysis?</p>
<p><strong>Answer:</strong> The result from the RNN model is quite similar to the XGBoost model. Therefore, both models are preferred in sentiment analysis.</p>
<h3 id="TODO-More-testing"><a href="#TODO-More-testing" class="headerlink" title="(TODO) More testing"></a>(TODO) More testing</h3><p>We now have a trained model which has been deployed and which we can send processed reviews to and which returns the predicted sentiment. However, ultimately we would like to be able to send our model an unprocessed review. That is, we would like to send the review itself as a string. For example, suppose we wish to send the following review to our model.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_review = <span class="string">'The simplest pleasures in life are the best, and this film is one of them. Combining a rather basic storyline of love and adventure this movie transcends the usual weekend fair with wit and unmitigated charm.'</span></span><br></pre></td></tr></table></figure>

<p>The question we now need to answer is, how do we send this review to our model?</p>
<p>Recall in the first section of this notebook we did a bunch of data processing to the IMDb dataset. In particular, we did two specific things to the provided reviews.</p>
<ul>
<li>Removed any html tags and stemmed the input</li>
<li>Encoded the review as a sequence of integers using <code>word_dict</code></li>
</ul>
<p>In order process the review we will need to repeat these two steps.</p>
<p><strong>TODO</strong>: Using the <code>review_to_words</code> and <code>convert_and_pad</code> methods from section one, convert <code>test_review</code> into a numpy array <code>test_data</code> suitable to send to our model. Remember that our model expects input of the form <code>review_length, review[500]</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Convert test_review into a form usable by the model and save the results in test_data</span></span><br><span class="line">test_data = review_to_words(test_review)</span><br><span class="line">test_data = [np.array(convert_and_pad(word_dict, test_data)[<span class="number">0</span>])]</span><br></pre></td></tr></table></figure>

<p>Now that we have processed the review, we can send the resulting array to our model to predict the sentiment of the review.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predictor.predict(test_data)</span><br></pre></td></tr></table></figure>




<pre><code>array(0.6213178, dtype=float32)</code></pre><p>Since the return value of our model is close to <code>1</code>, we can be certain that the review we submitted is positive.</p>
<h3 id="Delete-the-endpoint"><a href="#Delete-the-endpoint" class="headerlink" title="Delete the endpoint"></a>Delete the endpoint</h3><p>Of course, just like in the XGBoost notebook, once weâ€™ve deployed an endpoint it continues to run until we tell it to shut down. Since we are done using our endpoint for now, we can delete it.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">estimator.delete_endpoint()</span><br></pre></td></tr></table></figure>

<h2 id="Step-6-again-Deploy-the-model-for-the-web-app"><a href="#Step-6-again-Deploy-the-model-for-the-web-app" class="headerlink" title="Step 6 (again) - Deploy the model for the web app"></a>Step 6 (again) - Deploy the model for the web app</h2><p>Now that we know that our model is working, itâ€™s time to create some custom inference code so that we can send the model a review which has not been processed and have it determine the sentiment of the review.</p>
<p>As we saw above, by default the estimator which we created, when deployed, will use the entry script and directory which we provided when creating the model. However, since we now wish to accept a string as input and our model expects a processed review, we need to write some custom inference code.</p>
<p>We will store the code that we write in the <code>serve</code> directory. Provided in this directory is the <code>model.py</code> file that we used to construct our model, a <code>utils.py</code> file which contains the <code>review_to_words</code> and <code>convert_and_pad</code> pre-processing functions which we used during the initial data processing, and <code>predict.py</code>, the file which will contain our custom inference code. Note also that <code>requirements.txt</code> is present which will tell SageMaker what Python libraries are required by our custom inference code.</p>
<p>When deploying a PyTorch model in SageMaker, you are expected to provide four functions which the SageMaker inference container will use.</p>
<ul>
<li><code>model_fn</code>: This function is the same function that we used in the training script and it tells SageMaker how to load our model.</li>
<li><code>input_fn</code>: This function receives the raw serialized input that has been sent to the modelâ€™s endpoint and its job is to de-serialize and make the input available for the inference code.</li>
<li><code>output_fn</code>: This function takes the output of the inference code and its job is to serialize this output and return it to the caller of the modelâ€™s endpoint.</li>
<li><code>predict_fn</code>: The heart of the inference script, this is where the actual prediction is done and is the function which you will need to complete.</li>
</ul>
<p>For the simple website that we are constructing during this project, the <code>input_fn</code> and <code>output_fn</code> methods are relatively straightforward. We only require being able to accept a string as input and we expect to return a single value as output. You might imagine though that in a more complex application the input or output may be image data or some other binary data which would require some effort to serialize.</p>
<h3 id="TODO-Writing-inference-code"><a href="#TODO-Writing-inference-code" class="headerlink" title="(TODO) Writing inference code"></a>(TODO) Writing inference code</h3><p>Before writing our custom inference code, we will begin by taking a look at the code which has been provided.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!pygmentize serve/predict.py</span><br></pre></td></tr></table></figure>

<pre><code>[34mimport[39;49;00m [04m[36margparse[39;49;00m
[34mimport[39;49;00m [04m[36mjson[39;49;00m
[34mimport[39;49;00m [04m[36mos[39;49;00m
[34mimport[39;49;00m [04m[36mpickle[39;49;00m
[34mimport[39;49;00m [04m[36msys[39;49;00m
[34mimport[39;49;00m [04m[36msagemaker_containers[39;49;00m
[34mimport[39;49;00m [04m[36mpandas[39;49;00m [34mas[39;49;00m [04m[36mpd[39;49;00m
[34mimport[39;49;00m [04m[36mnumpy[39;49;00m [34mas[39;49;00m [04m[36mnp[39;49;00m
[34mimport[39;49;00m [04m[36mtorch[39;49;00m
[34mimport[39;49;00m [04m[36mtorch.nn[39;49;00m [34mas[39;49;00m [04m[36mnn[39;49;00m
[34mimport[39;49;00m [04m[36mtorch.optim[39;49;00m [34mas[39;49;00m [04m[36moptim[39;49;00m
[34mimport[39;49;00m [04m[36mtorch.utils.data[39;49;00m

[34mfrom[39;49;00m [04m[36mmodel[39;49;00m [34mimport[39;49;00m LSTMClassifier

[34mfrom[39;49;00m [04m[36mutils[39;49;00m [34mimport[39;49;00m review_to_words, convert_and_pad

[34mdef[39;49;00m [32mmodel_fn[39;49;00m(model_dir):
    [33m&quot;&quot;&quot;Load the PyTorch model from the `model_dir` directory.&quot;&quot;&quot;[39;49;00m
    [34mprint[39;49;00m([33m&quot;[39;49;00m[33mLoading model.[39;49;00m[33m&quot;[39;49;00m)

    [37m# First, load the parameters used to create the model.[39;49;00m
    model_info = {}
    model_info_path = os.path.join(model_dir, [33m&apos;[39;49;00m[33mmodel_info.pth[39;49;00m[33m&apos;[39;49;00m)
    [34mwith[39;49;00m [36mopen[39;49;00m(model_info_path, [33m&apos;[39;49;00m[33mrb[39;49;00m[33m&apos;[39;49;00m) [34mas[39;49;00m f:
        model_info = torch.load(f)

    [34mprint[39;49;00m([33m&quot;[39;49;00m[33mmodel_info: {}[39;49;00m[33m&quot;[39;49;00m.format(model_info))

    [37m# Determine the device and construct the model.[39;49;00m
    device = torch.device([33m&quot;[39;49;00m[33mcuda[39;49;00m[33m&quot;[39;49;00m [34mif[39;49;00m torch.cuda.is_available() [34melse[39;49;00m [33m&quot;[39;49;00m[33mcpu[39;49;00m[33m&quot;[39;49;00m)
    model = LSTMClassifier(model_info[[33m&apos;[39;49;00m[33membedding_dim[39;49;00m[33m&apos;[39;49;00m], model_info[[33m&apos;[39;49;00m[33mhidden_dim[39;49;00m[33m&apos;[39;49;00m], model_info[[33m&apos;[39;49;00m[33mvocab_size[39;49;00m[33m&apos;[39;49;00m])

    [37m# Load the store model parameters.[39;49;00m
    model_path = os.path.join(model_dir, [33m&apos;[39;49;00m[33mmodel.pth[39;49;00m[33m&apos;[39;49;00m)
    [34mwith[39;49;00m [36mopen[39;49;00m(model_path, [33m&apos;[39;49;00m[33mrb[39;49;00m[33m&apos;[39;49;00m) [34mas[39;49;00m f:
        model.load_state_dict(torch.load(f))

    [37m# Load the saved word_dict.[39;49;00m
    word_dict_path = os.path.join(model_dir, [33m&apos;[39;49;00m[33mword_dict.pkl[39;49;00m[33m&apos;[39;49;00m)
    [34mwith[39;49;00m [36mopen[39;49;00m(word_dict_path, [33m&apos;[39;49;00m[33mrb[39;49;00m[33m&apos;[39;49;00m) [34mas[39;49;00m f:
        model.word_dict = pickle.load(f)

    model.to(device).eval()

    [34mprint[39;49;00m([33m&quot;[39;49;00m[33mDone loading model.[39;49;00m[33m&quot;[39;49;00m)
    [34mreturn[39;49;00m model

[34mdef[39;49;00m [32minput_fn[39;49;00m(serialized_input_data, content_type):
    [34mprint[39;49;00m([33m&apos;[39;49;00m[33mDeserializing the input data.[39;49;00m[33m&apos;[39;49;00m)
    [34mif[39;49;00m content_type == [33m&apos;[39;49;00m[33mtext/plain[39;49;00m[33m&apos;[39;49;00m:
        data = serialized_input_data.decode([33m&apos;[39;49;00m[33mutf-8[39;49;00m[33m&apos;[39;49;00m)
        [34mreturn[39;49;00m data
    [34mraise[39;49;00m [36mException[39;49;00m([33m&apos;[39;49;00m[33mRequested unsupported ContentType in content_type: [39;49;00m[33m&apos;[39;49;00m + content_type)

[34mdef[39;49;00m [32moutput_fn[39;49;00m(prediction_output, accept):
    [34mprint[39;49;00m([33m&apos;[39;49;00m[33mSerializing the generated output.[39;49;00m[33m&apos;[39;49;00m)
    [34mreturn[39;49;00m [36mstr[39;49;00m(prediction_output)

[34mdef[39;49;00m [32mpredict_fn[39;49;00m(input_data, model):
    [34mprint[39;49;00m([33m&apos;[39;49;00m[33mInferring sentiment of input data.[39;49;00m[33m&apos;[39;49;00m)

    device = torch.device([33m&quot;[39;49;00m[33mcuda[39;49;00m[33m&quot;[39;49;00m [34mif[39;49;00m torch.cuda.is_available() [34melse[39;49;00m [33m&quot;[39;49;00m[33mcpu[39;49;00m[33m&quot;[39;49;00m)

    [34mif[39;49;00m model.word_dict [35mis[39;49;00m [36mNone[39;49;00m:
        [34mraise[39;49;00m [36mException[39;49;00m([33m&apos;[39;49;00m[33mModel has not been loaded properly, no word_dict.[39;49;00m[33m&apos;[39;49;00m)

    [37m# TODO: Process input_data so that it is ready to be sent to our model.[39;49;00m
    [37m#       You should produce two variables:[39;49;00m
    [37m#         data_X   - A sequence of length 500 which represents the converted review[39;49;00m
    [37m#         data_len - The length of the review[39;49;00m

    data_X = [36mNone[39;49;00m
    data_len = [36mNone[39;49;00m

    [37m# Using data_X and data_len we construct an appropriate input tensor. Remember[39;49;00m
    [37m# that our model expects input data of the form &apos;len, review[500]&apos;.[39;49;00m
    data_pack = np.hstack((data_len, data_X))
    data_pack = data_pack.reshape([34m1[39;49;00m, -[34m1[39;49;00m)

    data = torch.from_numpy(data_pack)
    data = data.to(device)

    [37m# Make sure to put the model into evaluation mode[39;49;00m
    model.eval()

    [37m# TODO: Compute the result of applying the model to the input data. The variable `result` should[39;49;00m
    [37m#       be a numpy array which contains a single integer which is either 1 or 0[39;49;00m

    result = [36mNone[39;49;00m

    [34mreturn[39;49;00m result</code></pre><p>As mentioned earlier, the <code>model_fn</code> method is the same as the one provided in the training code and the <code>input_fn</code> and <code>output_fn</code> methods are very simple and your task will be to complete the <code>predict_fn</code> method. Make sure that you save the completed file as <code>predict.py</code> in the <code>serve</code> directory.</p>
<p><strong>TODO</strong>: Complete the <code>predict_fn()</code> method in the <code>serve/predict.py</code> file.</p>
<h3 id="Deploying-the-model"><a href="#Deploying-the-model" class="headerlink" title="Deploying the model"></a>Deploying the model</h3><p>Now that the custom inference code has been written, we will create and deploy our model. To begin with, we need to construct a new PyTorchModel object which points to the model artifacts created during training and also points to the inference code that we wish to use. Then we can call the deploy method to launch the deployment container.</p>
<p><strong>NOTE</strong>: The default behaviour for a deployed PyTorch model is to assume that any input passed to the predictor is a <code>numpy</code> array. In our case we want to send a string so we need to construct a simple wrapper around the <code>RealTimePredictor</code> class to accomodate simple strings. In a more complicated situation you may want to provide a serialization object, for example if you wanted to sent image data.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sagemaker.predictor <span class="keyword">import</span> RealTimePredictor</span><br><span class="line"><span class="keyword">from</span> sagemaker.pytorch <span class="keyword">import</span> PyTorchModel</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StringPredictor</span><span class="params">(RealTimePredictor)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, endpoint_name, sagemaker_session)</span>:</span></span><br><span class="line">        super(StringPredictor, self).__init__(endpoint_name, sagemaker_session, content_type=<span class="string">'text/plain'</span>)</span><br><span class="line"></span><br><span class="line">model = PyTorchModel(model_data=estimator.model_data,</span><br><span class="line">                     role = role,</span><br><span class="line">                     framework_version=<span class="string">'0.4.0'</span>,</span><br><span class="line">                     entry_point=<span class="string">'predict.py'</span>,</span><br><span class="line">                     source_dir=<span class="string">'serve'</span>,</span><br><span class="line">                     predictor_cls=StringPredictor)</span><br><span class="line">predictor = model.deploy(initial_instance_count=<span class="number">1</span>, instance_type=<span class="string">'ml.m4.xlarge'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>-----------!</code></pre><h3 id="Testing-the-model"><a href="#Testing-the-model" class="headerlink" title="Testing the model"></a>Testing the model</h3><p>Now that we have deployed our model with the custom inference code, we should test to see if everything is working. Here we test our model by loading the first <code>250</code> positive and negative reviews and send them to the endpoint, then collect the results. The reason for only sending some of the data is that the amount of time it takes for our model to process the input and then perform inference is quite long and so testing the entire data set would be prohibitive.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_reviews</span><span class="params">(data_dir=<span class="string">'../data/aclImdb'</span>, stop=<span class="number">250</span>)</span>:</span></span><br><span class="line">    </span><br><span class="line">    results = []</span><br><span class="line">    ground = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># We make sure to test both positive and negative reviews    </span></span><br><span class="line">    <span class="keyword">for</span> sentiment <span class="keyword">in</span> [<span class="string">'pos'</span>, <span class="string">'neg'</span>]:</span><br><span class="line">        </span><br><span class="line">        path = os.path.join(data_dir, <span class="string">'test'</span>, sentiment, <span class="string">'*.txt'</span>)</span><br><span class="line">        files = glob.glob(path)</span><br><span class="line">        </span><br><span class="line">        files_read = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        print(<span class="string">'Starting '</span>, sentiment, <span class="string">' files'</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Iterate through the files and send them to the predictor</span></span><br><span class="line">        <span class="keyword">for</span> f <span class="keyword">in</span> files:</span><br><span class="line">            <span class="keyword">with</span> open(f) <span class="keyword">as</span> review:</span><br><span class="line">                <span class="comment"># First, we store the ground truth (was the review positive or negative)</span></span><br><span class="line">                <span class="keyword">if</span> sentiment == <span class="string">'pos'</span>:</span><br><span class="line">                    ground.append(<span class="number">1</span>)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    ground.append(<span class="number">0</span>)</span><br><span class="line">                <span class="comment"># Read in the review and convert to 'utf-8' for transmission via HTTP</span></span><br><span class="line">                review_input = review.read().encode(<span class="string">'utf-8'</span>)</span><br><span class="line">                <span class="comment"># Send the review to the predictor and store the results</span></span><br><span class="line">                results.append(float(predictor.predict(review_input)))</span><br><span class="line">                </span><br><span class="line">            <span class="comment"># Sending reviews to our endpoint one at a time takes a while so we</span></span><br><span class="line">            <span class="comment"># only send a small number of reviews</span></span><br><span class="line">            files_read += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> files_read == stop:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> ground, results</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ground, results = test_reviews()</span><br></pre></td></tr></table></figure>

<pre><code>Starting  pos  files
Starting  neg  files</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">accuracy_score(ground, results)</span><br></pre></td></tr></table></figure>




<pre><code>0.854</code></pre><p>As an additional test, we can try sending the <code>test_review</code> that we looked at earlier.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predictor.predict(test_review)</span><br></pre></td></tr></table></figure>




<pre><code>b&apos;1.0&apos;</code></pre><p>Now that we know our endpoint is working as expected, we can set up the web page that will interact with it. If you donâ€™t have time to finish the project now, make sure to skip down to the end of this notebook and shut down your endpoint. You can deploy it again when you come back.</p>
<h2 id="Step-7-again-Use-the-model-for-the-web-app"><a href="#Step-7-again-Use-the-model-for-the-web-app" class="headerlink" title="Step 7 (again): Use the model for the web app"></a>Step 7 (again): Use the model for the web app</h2><blockquote>
<p><strong>TODO:</strong> This entire section and the next contain tasks for you to complete, mostly using the AWS console.</p>
</blockquote>
<p>So far we have been accessing our model endpoint by constructing a predictor object which uses the endpoint and then just using the predictor object to perform inference. What if we wanted to create a web app which accessed our model? The way things are set up currently makes that not possible since in order to access a SageMaker endpoint the app would first have to authenticate with AWS using an IAM role which included access to SageMaker endpoints. However, there is an easier way! We just need to use some additional AWS services.</p>
<img src="/2020/02/23/SageMakerProject/Web App Diagram.svg">

<p>The diagram above gives an overview of how the various services will work together. On the far right is the model which we trained above and which is deployed using SageMaker. On the far left is our web app that collects a userâ€™s movie review, sends it off and expects a positive or negative sentiment in return.</p>
<p>In the middle is where some of the magic happens. We will construct a Lambda function, which you can think of as a straightforward Python function that can be executed whenever a specified event occurs. We will give this function permission to send and recieve data from a SageMaker endpoint.</p>
<p>Lastly, the method we will use to execute the Lambda function is a new endpoint that we will create using API Gateway. This endpoint will be a url that listens for data to be sent to it. Once it gets some data it will pass that data on to the Lambda function and then return whatever the Lambda function returns. Essentially it will act as an interface that lets our web app communicate with the Lambda function.</p>
<h3 id="Setting-up-a-Lambda-function"><a href="#Setting-up-a-Lambda-function" class="headerlink" title="Setting up a Lambda function"></a>Setting up a Lambda function</h3><p>The first thing we are going to do is set up a Lambda function. This Lambda function will be executed whenever our public API has data sent to it. When it is executed it will receive the data, perform any sort of processing that is required, send the data (the review) to the SageMaker endpoint weâ€™ve created and then return the result.</p>
<h4 id="Part-A-Create-an-IAM-Role-for-the-Lambda-function"><a href="#Part-A-Create-an-IAM-Role-for-the-Lambda-function" class="headerlink" title="Part A: Create an IAM Role for the Lambda function"></a>Part A: Create an IAM Role for the Lambda function</h4><p>Since we want the Lambda function to call a SageMaker endpoint, we need to make sure that it has permission to do so. To do this, we will construct a role that we can later give the Lambda function.</p>
<p>Using the AWS Console, navigate to the <strong>IAM</strong> page and click on <strong>Roles</strong>. Then, click on <strong>Create role</strong>. Make sure that the <strong>AWS service</strong> is the type of trusted entity selected and choose <strong>Lambda</strong> as the service that will use this role, then click <strong>Next: Permissions</strong>.</p>
<p>In the search box type <code>sagemaker</code> and select the check box next to the <strong>AmazonSageMakerFullAccess</strong> policy. Then, click on <strong>Next: Review</strong>.</p>
<p>Lastly, give this role a name. Make sure you use a name that you will remember later on, for example <code>LambdaSageMakerRole</code>. Then, click on <strong>Create role</strong>.</p>
<h4 id="Part-B-Create-a-Lambda-function"><a href="#Part-B-Create-a-Lambda-function" class="headerlink" title="Part B: Create a Lambda function"></a>Part B: Create a Lambda function</h4><p>Now it is time to actually create the Lambda function.</p>
<p>Using the AWS Console, navigate to the AWS Lambda page and click on <strong>Create a function</strong>. When you get to the next page, make sure that <strong>Author from scratch</strong> is selected. Now, name your Lambda function, using a name that you will remember later on, for example <code>sentiment_analysis_func</code>. Make sure that the <strong>Python 3.6</strong> runtime is selected and then choose the role that you created in the previous part. Then, click on <strong>Create Function</strong>.</p>
<p>On the next page you will see some information about the Lambda function youâ€™ve just created. If you scroll down you should see an editor in which you can write the code that will be executed when your Lambda function is triggered. In our example, we will use the code below. </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># We need to use the low-level library to interact with SageMaker since the SageMaker API</span></span><br><span class="line"><span class="comment"># is not available natively through Lambda.</span></span><br><span class="line"><span class="keyword">import</span> boto3</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lambda_handler</span><span class="params">(event, context)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># The SageMaker runtime is what allows us to invoke the endpoint that we've created.</span></span><br><span class="line">    runtime = boto3.Session().client(<span class="string">'sagemaker-runtime'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Now we use the SageMaker runtime to invoke our endpoint, sending the review we were given</span></span><br><span class="line">    response = runtime.invoke_endpoint(EndpointName = <span class="string">'**ENDPOINT NAME HERE**'</span>,    <span class="comment"># The name of the endpoint we created</span></span><br><span class="line">                                       ContentType = <span class="string">'text/plain'</span>,                 <span class="comment"># The data format that is expected</span></span><br><span class="line">                                       Body = event[<span class="string">'body'</span>])                       <span class="comment"># The actual review</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># The response is an HTTP response whose body contains the result of our inference</span></span><br><span class="line">    result = response[<span class="string">'Body'</span>].read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">'statusCode'</span> : <span class="number">200</span>,</span><br><span class="line">        <span class="string">'headers'</span> : &#123; <span class="string">'Content-Type'</span> : <span class="string">'text/plain'</span>, <span class="string">'Access-Control-Allow-Origin'</span> : <span class="string">'*'</span> &#125;,</span><br><span class="line">        <span class="string">'body'</span> : result</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>Once you have copy and pasted the code above into the Lambda code editor, replace the <code>**ENDPOINT NAME HERE**</code> portion with the name of the endpoint that we deployed earlier. You can determine the name of the endpoint using the code cell below.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predictor.endpoint</span><br></pre></td></tr></table></figure>




<pre><code>&apos;sagemaker-pytorch-2020-02-23-11-11-09-175&apos;</code></pre><p>Once you have added the endpoint name to the Lambda function, click on <strong>Save</strong>. Your Lambda function is now up and running. Next we need to create a way for our web app to execute the Lambda function.</p>
<h3 id="Setting-up-API-Gateway"><a href="#Setting-up-API-Gateway" class="headerlink" title="Setting up API Gateway"></a>Setting up API Gateway</h3><p>Now that our Lambda function is set up, it is time to create a new API using API Gateway that will trigger the Lambda function we have just created.</p>
<p>Using AWS Console, navigate to <strong>Amazon API Gateway</strong> and then click on <strong>Get started</strong>.</p>
<p>On the next page, make sure that <strong>New API</strong> is selected and give the new api a name, for example, <code>sentiment_analysis_api</code>. Then, click on <strong>Create API</strong>.</p>
<p>Now we have created an API, however it doesnâ€™t currently do anything. What we want it to do is to trigger the Lambda function that we created earlier.</p>
<p>Select the <strong>Actions</strong> dropdown menu and click <strong>Create Method</strong>. A new blank method will be created, select its dropdown menu and select <strong>POST</strong>, then click on the check mark beside it.</p>
<p>For the integration point, make sure that <strong>Lambda Function</strong> is selected and click on the <strong>Use Lambda Proxy integration</strong>. This option makes sure that the data that is sent to the API is then sent directly to the Lambda function with no processing. It also means that the return value must be a proper response object as it will also not be processed by API Gateway.</p>
<p>Type the name of the Lambda function you created earlier into the <strong>Lambda Function</strong> text entry box and then click on <strong>Save</strong>. Click on <strong>OK</strong> in the pop-up box that then appears, giving permission to API Gateway to invoke the Lambda function you created.</p>
<p>The last step in creating the API Gateway is to select the <strong>Actions</strong> dropdown and click on <strong>Deploy API</strong>. You will need to create a new Deployment stage and name it anything you like, for example <code>prod</code>.</p>
<p>You have now successfully set up a public API to access your SageMaker model. Make sure to copy or write down the URL provided to invoke your newly created public API as this will be needed in the next step. This URL can be found at the top of the page, highlighted in blue next to the text <strong>Invoke URL</strong>.</p>
<h2 id="Step-8-Deploying-our-web-app"><a href="#Step-8-Deploying-our-web-app" class="headerlink" title="Step 8: Deploying our web app"></a>Step 8: Deploying our web app</h2><p>Now that we have a publicly available API, we can start using it in a web app. For our purposes, we have provided a simple static html file which can make use of the public api you created earlier.</p>
<p>In the <code>website</code> folder there should be a file called <code>index.html</code>. Download the file to your computer and open that file up in a text editor of your choice. There should be a line which contains <strong>**REPLACE WITH PUBLIC API URL**</strong>. Replace this string with the url that you wrote down in the last step and then save the file.</p>
<p>Now, if you open <code>index.html</code> on your local computer, your browser will behave as a local web server and you can use the provided site to interact with your SageMaker model.</p>
<p>If youâ€™d like to go further, you can host this html file anywhere youâ€™d like, for example using github or hosting a static site on Amazonâ€™s S3. Once you have done this you can share the link with anyone youâ€™d like and have them play with it too!</p>
<blockquote>
<p><strong>Important Note</strong> In order for the web app to communicate with the SageMaker endpoint, the endpoint has to actually be deployed and running. This means that you are paying for it. Make sure that the endpoint is running when you want to use the web app but that you shut it down when you donâ€™t need it, otherwise you will end up with a surprisingly large AWS bill.</p>
</blockquote>
<p><strong>TODO:</strong> Make sure that you include the edited <code>index.html</code> file in your project submission.</p>
<p>Now that your web app is working, trying playing around with it and see how well it works.</p>
<p><strong>Question</strong>: Give an example of a review that you entered into your web app. What was the predicted sentiment of your example review?</p>
<p><strong>Answer:</strong> I tried to use real reviews based on the movie â€˜1917â€™, the performance is good.</p>
<p><img src="/2020/02/23/SageMakerProject/reviews1.JPG" alt="review"></p>
<p><img src="/2020/02/23/SageMakerProject/reviews2.JPG" alt="review"></p>
<h3 id="Delete-the-endpoint-1"><a href="#Delete-the-endpoint-1" class="headerlink" title="Delete the endpoint"></a>Delete the endpoint</h3><p>Remember to always shut down your endpoint if you are no longer using it. You are charged for the length of time that the endpoint is running so if you forget and leave it on you could end up with an unexpectedly large bill.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predictor.delete_endpoint()</span><br></pre></td></tr></table></figure>


]]></content>
      <categories>
        <category>Pythonå­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
        <tag>æ·±åº¦å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title>ç¬¬ä¸€ä¸ªpython package -- actupac</title>
    <url>/2020/02/20/actupac_package/</url>
    <content><![CDATA[<font size="3">
ä¸€ä¸ªè‡ªå·±åšçš„ç²¾ç®—pythonåŒ…,å®žçŽ°å†…æŽ¨å’Œå¤–æŽ¨åˆ©çŽ‡,å¹¶ä¸”ç”»å‡ºåˆ©çŽ‡æ›²çº¿ã€‚è¯¥ç¯‡æ˜¯ä½¿ç”¨è¯´æ˜Žå’Œä¾‹å­ã€‚

<a id="more"></a>
<hr>
<h2 id="åŸºæœ¬ä»‹ç»"><a href="#åŸºæœ¬ä»‹ç»" class="headerlink" title="åŸºæœ¬ä»‹ç»"></a>åŸºæœ¬ä»‹ç»</h2><p>actupac æ˜¯ä¸€ä¸ªç²¾ç®—çš„pythonåŒ…,ç›®å‰çš„åŠŸèƒ½æ˜¯å†…æŽ¨å’Œå¤–æŽ¨åˆ©çŽ‡,å¹¶ä¸”ç”»å‡ºåˆ©çŽ‡æ›²çº¿ã€‚</p>
<h2 id="å®‰è£…æ–¹å¼"><a href="#å®‰è£…æ–¹å¼" class="headerlink" title="å®‰è£…æ–¹å¼"></a>å®‰è£…æ–¹å¼</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pip install actupac</span><br></pre></td></tr></table></figure>

<pre><code>Collecting actupac
  Downloading https://files.pythonhosted.org/packages/78/6d/100b147d64d2b2653f93818af3da6013f23b53120bdc66e1092133500ff3/actupac-0.1.tar.gz
Building wheels for collected packages: actupac
  Building wheel for actupac (setup.py): started
  Building wheel for actupac (setup.py): finished with status &apos;done&apos;
  Created wheel for actupac: filename=actupac-0.1-cp37-none-any.whl size=5979 sha256=7eb4984f5edbdd2af0de32a2fb795215bc159ce70f39eb398dead35d5f97950f
  Stored in directory: C:\Users\jasonguo\AppData\Local\pip\Cache\wheels\83\19\8e\472924dcac472b470f64c7e2fdd5553ea1af6857d213253170
Successfully built actupac
Installing collected packages: actupac
Successfully installed actupac-0.1
Note: you may need to restart the kernel to use updated packages.</code></pre><h2 id="å¯¼å…¥åŒ…"><a href="#å¯¼å…¥åŒ…" class="headerlink" title="å¯¼å…¥åŒ…"></a>å¯¼å…¥åŒ…</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> actupac <span class="keyword">import</span> Interpolation</span><br><span class="line"><span class="keyword">from</span> actupac <span class="keyword">import</span> Extrapolation</span><br></pre></td></tr></table></figure>

<h2 id="ä½¿ç”¨æ–¹æ³•"><a href="#ä½¿ç”¨æ–¹æ³•" class="headerlink" title="ä½¿ç”¨æ–¹æ³•"></a>ä½¿ç”¨æ–¹æ³•</h2><h3 id="1-å†…æŽ¨"><a href="#1-å†…æŽ¨" class="headerlink" title="1. å†…æŽ¨"></a>1. å†…æŽ¨</h3><p>ç›®å‰æœ‰ä¸‰ç§æ–¹æ³•:piecewise linear, piecewise constant, ä»¥åŠ cubic splineã€‚ å…¶ä¸­ cubic spline ä½¿ç”¨ä¸¤ç«¯äºŒæ¬¡å¯¼ä¸ºé›¶çš„æ–¹æ³•,æ‰€ä»¥æ˜¯natural splineã€‚</p>
<p>æ•°æ®éœ€è¦å­˜å‚¨åœ¨csvæ ¼å¼æ–‡ä»¶ä¸­,ç¬¬ä¸€åˆ—ä¸ºå¹´ä»½,ç¬¬äºŒåˆ—ä¸ºåˆ©çŽ‡å³å¯,ä¸éœ€è¦åˆ—åç­‰ã€‚</p>
<p>å¯ä»¥æ”¹å˜çš„å‚æ•°æ˜¯å†…æŽ¨ç‚¹çš„ä¸ªæ•°,ä»¥ä¸‹ä»¥9ä¸ªç‚¹ä¸ºä¾‹å­:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Read the dataset</span></span><br><span class="line">sample = Interpolation()</span><br><span class="line">df = sample.read_data(<span class="string">'interest.csv'</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Years</th>
      <th>Yields</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1</td>
      <td>1.129</td>
    </tr>
    <tr>
      <td>1</td>
      <td>2</td>
      <td>1.224</td>
    </tr>
    <tr>
      <td>2</td>
      <td>3</td>
      <td>1.364</td>
    </tr>
    <tr>
      <td>3</td>
      <td>4</td>
      <td>1.540</td>
    </tr>
    <tr>
      <td>4</td>
      <td>5</td>
      <td>1.707</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># calculate means and standard deviation and discount rates</span></span><br><span class="line">print(sample.mean)</span><br><span class="line">print(sample.stdev)</span><br><span class="line">discount = sample.discount_rate()</span><br><span class="line">discount[<span class="number">0</span>:<span class="number">5</span>]</span><br></pre></td></tr></table></figure>

<pre><code>2.3666
0.492





[0.9887734928804884,
 0.9758172050673466,
 0.9599059193497299,
 0.9402589151464628,
 0.9181908613542313]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># plot the distribution of the dataset</span></span><br><span class="line">sample.plot_histogram()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/02/20/actupac_package/output_11_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># interpolate with piecewise linear model with 9 interpolated points between each known year</span></span><br><span class="line">linear = sample.piecewise_linear(<span class="number">9</span>)</span><br><span class="line">print(linear.head())</span><br><span class="line">sample.piecewise_linear_plot()</span><br></pre></td></tr></table></figure>

<pre><code>   Years  Yields        Pt        Ft
0    1.0  1.1290  0.988773  1.234261
1    1.1  1.1385  0.987555  1.253285
2    1.2  1.1480  0.986318  1.272309
3    1.3  1.1575  0.985065  1.291333
4    1.4  1.1670  0.983795  1.310358</code></pre><p><img src="/2020/02/20/actupac_package/output_12_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># interpolate with piecewise constant model with 9 interpolated points between each known year</span></span><br><span class="line">constant = sample.piecewise_constant(<span class="number">9</span>)</span><br><span class="line">print(constant.head())</span><br><span class="line">sample.piecewise_constant_plot()</span><br></pre></td></tr></table></figure>

<pre><code>   Years    Yields        Pt       Ft
0    1.0  1.129000  0.988773  1.31987
1    1.1  1.146273  0.987470  1.31987
2    1.2  1.160667  0.986169  1.31987
3    1.3  1.172846  0.984869  1.31987
4    1.4  1.183286  0.983570  1.31987</code></pre><p><img src="/2020/02/20/actupac_package/output_13_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># interpolate with cubic spline (natural spline) model with 9 interpolated points between each known year</span></span><br><span class="line">cubic = sample.cubic(<span class="number">9</span>)</span><br><span class="line">print(cubic.head())</span><br><span class="line">sample.cubic_plot()</span><br></pre></td></tr></table></figure>

<pre><code>   Years    Yields        Pt        Ft
0    1.0  1.129000  0.988773  1.224226
1    1.1  1.137589  0.987564  1.242089
2    1.2  1.146233  0.986339  1.260836
3    1.3  1.154987  0.985097  1.280690
4    1.4  1.163908  0.983837  1.301870</code></pre><p><img src="/2020/02/20/actupac_package/output_14_1.png" alt="png"></p>
<h3 id="2-å¤–æŽ¨"><a href="#2-å¤–æŽ¨" class="headerlink" title="2.  å¤–æŽ¨"></a>2.  å¤–æŽ¨</h3><p>ç›®å‰åªæœ‰ Smith Wilson æ–¹æ³•è¿›è¡ŒåŒæ—¶å¤–æŽ¨å’Œå†…æŽ¨, éœ€è¦æ‰‹åŠ¨è¾“å…¥å·²çŸ¥æ•°æ®åˆ›å»ºä¸ºåˆ—è¡¨å½¢å¼å³å¯ã€‚ å¯å˜å‚æ•°æœ‰ä¸‰ä¸ª,næ˜¯æ­¥é•¿,0.1ä»£è¡¨0.1å¹´ä¸ºä¸€ä¸ªæ­¥é•¿;max_yearæ˜¯å¤–æŽ¨çš„æœ€å¤§å¹´ä»½, UFR æ˜¯ Ultimate forward rate å¯ä»¥æ ¹æ®éœ€è¦æ”¹å˜ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = list([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">10</span>,<span class="number">15</span>,<span class="number">20</span>,<span class="number">30</span>,<span class="number">50</span>])</span><br><span class="line">b = list([<span class="number">3.2870</span>,<span class="number">3.1280</span>,<span class="number">3.2240</span>,<span class="number">3.3680</span>,<span class="number">3.4710</span>,<span class="number">3.9860</span>,<span class="number">4.2070</span>,<span class="number">4.0540</span>,<span class="number">3.4320</span>,<span class="number">2.9310</span>])</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># read the data and creat a dataframe</span></span><br><span class="line">sample2 = Extrapolation(a,b)</span><br><span class="line">sample2.dataframe()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Years</th>
      <th>Yields</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.0</td>
      <td>3.287</td>
    </tr>
    <tr>
      <td>1</td>
      <td>2.0</td>
      <td>3.128</td>
    </tr>
    <tr>
      <td>2</td>
      <td>3.0</td>
      <td>3.224</td>
    </tr>
    <tr>
      <td>3</td>
      <td>4.0</td>
      <td>3.368</td>
    </tr>
    <tr>
      <td>4</td>
      <td>5.0</td>
      <td>3.471</td>
    </tr>
    <tr>
      <td>5</td>
      <td>10.0</td>
      <td>3.986</td>
    </tr>
    <tr>
      <td>6</td>
      <td>15.0</td>
      <td>4.207</td>
    </tr>
    <tr>
      <td>7</td>
      <td>20.0</td>
      <td>4.054</td>
    </tr>
    <tr>
      <td>8</td>
      <td>30.0</td>
      <td>3.432</td>
    </tr>
    <tr>
      <td>9</td>
      <td>50.0</td>
      <td>2.931</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># results </span></span><br><span class="line">sample2.Smith_Wilson(n=<span class="number">0.1</span>,max_year=<span class="number">70</span>,UFR=<span class="number">4.2</span>)</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Years</th>
      <th>Yields</th>
      <th>Discount rates</th>
      <th>Forward rates</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>1.000000</td>
      <td>3.401529</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.1</td>
      <td>3.395757</td>
      <td>0.996610</td>
      <td>3.395344</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.2</td>
      <td>3.392675</td>
      <td>0.993238</td>
      <td>3.382345</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.3</td>
      <td>3.387329</td>
      <td>0.989889</td>
      <td>3.362554</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.4</td>
      <td>3.379725</td>
      <td>0.986572</td>
      <td>3.335994</td>
    </tr>
    <tr>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <td>696</td>
      <td>69.6</td>
      <td>3.151262</td>
      <td>0.111550</td>
      <td>4.059759</td>
    </tr>
    <tr>
      <td>697</td>
      <td>69.7</td>
      <td>3.152554</td>
      <td>0.111099</td>
      <td>4.061264</td>
    </tr>
    <tr>
      <td>698</td>
      <td>69.8</td>
      <td>3.153844</td>
      <td>0.110650</td>
      <td>4.062754</td>
    </tr>
    <tr>
      <td>699</td>
      <td>69.9</td>
      <td>3.155132</td>
      <td>0.110202</td>
      <td>4.064228</td>
    </tr>
    <tr>
      <td>700</td>
      <td>70.0</td>
      <td>3.156419</td>
      <td>0.109756</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
<p>701 rows Ã— 4 columns</p>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># plots</span></span><br><span class="line">sample2.Smith_Wilson_plot()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/02/20/actupac_package/output_20_0.png" alt="png"></p>
</font>]]></content>
      <categories>
        <category>Pythonå­¦ä¹ </category>
      </categories>
      <tags>
        <tag>ç²¾ç®—</tag>
      </tags>
  </entry>
  <entry>
    <title>Identify Customer Segments</title>
    <url>/2020/02/05/Identify_Customer_Segments/</url>
    <content><![CDATA[<font size="3">
Identify segments of the population that form the core customer base for a mail-order sales company in Germany. These segments can then be used to direct marketing campaigns towards audiences that will have the highest expected rate of returns.

<a id="more"></a>
<hr>
<h1 id="Project-Identify-Customer-Segments"><a href="#Project-Identify-Customer-Segments" class="headerlink" title="Project: Identify Customer Segments"></a>Project: Identify Customer Segments</h1><p>In this project, you will apply unsupervised learning techniques to identify segments of the population that form the core customer base for a mail-order sales company in Germany. These segments can then be used to direct marketing campaigns towards audiences that will have the highest expected rate of returns. The data that you will use has been provided by our partners at Bertelsmann Arvato Analytics, and represents a real-life data science task.</p>
<p>This notebook will help you complete this task by providing a framework within which you will perform your analysis steps. In each step of the project, you will see some text describing the subtask that you will perform, followed by one or more code cells for you to complete your work. <strong>Feel free to add additional code and markdown cells as you go along so that you can explore everything in precise chunks.</strong> The code cells provided in the base template will outline only the major tasks, and will usually not be enough to cover all of the minor tasks that comprise it.</p>
<p>It should be noted that while there will be precise guidelines on how you should handle certain tasks in the project, there will also be places where an exact specification is not provided. <strong>There will be times in the project where you will need to make and justify your own decisions on how to treat the data.</strong> These are places where there may not be only one way to handle the data. In real-life tasks, there may be many valid ways to approach an analysis task. One of the most important things you can do is clearly document your approach so that other scientists can understand the decisions youâ€™ve made.</p>
<p>At the end of most sections, there will be a Markdown cell labeled <strong>Discussion</strong>. In these cells, you will report your findings for the completed section, as well as document the decisions that you made in your approach to each subtask. <strong>Your project will be evaluated not just on the code used to complete the tasks outlined, but also your communication about your observations and conclusions at each stage.</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import libraries here; add more as necessary</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="comment"># magic word for producing visualizations in notebook</span></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>

<h3 id="Step-0-Load-the-Data"><a href="#Step-0-Load-the-Data" class="headerlink" title="Step 0: Load the Data"></a>Step 0: Load the Data</h3><p>There are four files associated with this project (not including this one):</p>
<ul>
<li><code>Udacity_AZDIAS_Subset.csv</code>: Demographics data for the general population of Germany; 891211 persons (rows) x 85 features (columns).</li>
<li><code>Udacity_CUSTOMERS_Subset.csv</code>: Demographics data for customers of a mail-order company; 191652 persons (rows) x 85 features (columns).</li>
<li><code>Data_Dictionary.md</code>: Detailed information file about the features in the provided datasets.</li>
<li><code>AZDIAS_Feature_Summary.csv</code>: Summary of feature attributes for demographics data; 85 features (rows) x 4 columns</li>
</ul>
<p>Each row of the demographics files represents a single person, but also includes information outside of individuals, including information about their household, building, and neighborhood. You will use this information to cluster the general population into groups with similar demographic properties. Then, you will see how the people in the customers dataset fit into those created clusters. The hope here is that certain clusters are over-represented in the customers data, as compared to the general population; those over-represented clusters will be assumed to be part of the core userbase. This information can then be used for further applications, such as targeting for a marketing campaign.</p>
<p>To start off with, load in the demographics data for the general population into a pandas DataFrame, and do the same for the feature attributes summary. Note for all of the <code>.csv</code> data files in this project: theyâ€™re semicolon (<code>;</code>) delimited, so youâ€™ll need an additional argument in your <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html" target="_blank" rel="noopener"><code>read_csv()</code></a> call to read in the data properly. Also, considering the size of the main dataset, it may take some time for it to load completely.</p>
<p>Once the dataset is loaded, itâ€™s recommended that you take a little bit of time just browsing the general structure of the dataset and feature summary file. Youâ€™ll be getting deep into the innards of the cleaning in the first major step of the project, so gaining some general familiarity can help you get your bearings.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Load in the general demographics data.</span></span><br><span class="line">azdias = pd.read_csv(<span class="string">'Udacity_AZDIAS_Subset.csv'</span>,sep=<span class="string">';'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load in the feature summary file.</span></span><br><span class="line">feat_info = pd.read_csv(<span class="string">'AZDIAS_Feature_Summary.csv'</span>,sep=<span class="string">';'</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Check the structure of the data after it's loaded (e.g. print the number of</span></span><br><span class="line"><span class="comment"># rows and columns, print the first few rows).</span></span><br><span class="line"><span class="comment"># Info for general demographics data.</span></span><br><span class="line">print(<span class="string">"Total number of rows: &#123;&#125;"</span>.format(azdias.shape[<span class="number">0</span>]))</span><br><span class="line">print(<span class="string">"Total number of columns: &#123;&#125;"</span>.format(azdias.shape[<span class="number">1</span>]))</span><br><span class="line">azdias.head()</span><br></pre></td></tr></table></figure>

<pre><code>Total number of rows: 891221
Total number of columns: 85</code></pre><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AGER_TYP</th>
      <th>ALTERSKATEGORIE_GROB</th>
      <th>ANREDE_KZ</th>
      <th>CJT_GESAMTTYP</th>
      <th>FINANZ_MINIMALIST</th>
      <th>FINANZ_SPARER</th>
      <th>FINANZ_VORSORGER</th>
      <th>FINANZ_ANLEGER</th>
      <th>FINANZ_UNAUFFAELLIGER</th>
      <th>FINANZ_HAUSBAUER</th>
      <th>...</th>
      <th>PLZ8_ANTG1</th>
      <th>PLZ8_ANTG2</th>
      <th>PLZ8_ANTG3</th>
      <th>PLZ8_ANTG4</th>
      <th>PLZ8_BAUMAX</th>
      <th>PLZ8_HHZ</th>
      <th>PLZ8_GBZ</th>
      <th>ARBEIT</th>
      <th>ORTSGR_KLS9</th>
      <th>RELAT_AB</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>-1</td>
      <td>2</td>
      <td>1</td>
      <td>2.0</td>
      <td>3</td>
      <td>4</td>
      <td>3</td>
      <td>5</td>
      <td>5</td>
      <td>3</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <td>1</td>
      <td>-1</td>
      <td>1</td>
      <td>2</td>
      <td>5.0</td>
      <td>1</td>
      <td>5</td>
      <td>2</td>
      <td>5</td>
      <td>4</td>
      <td>5</td>
      <td>...</td>
      <td>2.0</td>
      <td>3.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>5.0</td>
      <td>4.0</td>
      <td>3.0</td>
      <td>5.0</td>
      <td>4.0</td>
    </tr>
    <tr>
      <td>2</td>
      <td>-1</td>
      <td>3</td>
      <td>2</td>
      <td>3.0</td>
      <td>1</td>
      <td>4</td>
      <td>1</td>
      <td>2</td>
      <td>3</td>
      <td>5</td>
      <td>...</td>
      <td>3.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>4.0</td>
      <td>4.0</td>
      <td>3.0</td>
      <td>5.0</td>
      <td>2.0</td>
    </tr>
    <tr>
      <td>3</td>
      <td>2</td>
      <td>4</td>
      <td>2</td>
      <td>2.0</td>
      <td>4</td>
      <td>2</td>
      <td>5</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>...</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>4.0</td>
      <td>2.0</td>
      <td>3.0</td>
      <td>3.0</td>
    </tr>
    <tr>
      <td>4</td>
      <td>-1</td>
      <td>3</td>
      <td>1</td>
      <td>5.0</td>
      <td>4</td>
      <td>3</td>
      <td>4</td>
      <td>1</td>
      <td>3</td>
      <td>2</td>
      <td>...</td>
      <td>2.0</td>
      <td>4.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>3.0</td>
      <td>3.0</td>
      <td>4.0</td>
      <td>6.0</td>
      <td>5.0</td>
    </tr>
  </tbody>
</table>
<p>5 rows Ã— 85 columns</p>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># info for the feature summary file</span></span><br><span class="line">print(<span class="string">"Total number of rows: &#123;&#125;"</span>.format(feat_info.shape[<span class="number">0</span>]))</span><br><span class="line">print(<span class="string">"Total number of columns: &#123;&#125;"</span>.format(feat_info.shape[<span class="number">1</span>]))</span><br><span class="line">feat_info.head()</span><br></pre></td></tr></table></figure>

<pre><code>Total number of rows: 85
Total number of columns: 4</code></pre><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>attribute</th>
      <th>information_level</th>
      <th>type</th>
      <th>missing_or_unknown</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>AGER_TYP</td>
      <td>person</td>
      <td>categorical</td>
      <td>[-1,0]</td>
    </tr>
    <tr>
      <td>1</td>
      <td>ALTERSKATEGORIE_GROB</td>
      <td>person</td>
      <td>ordinal</td>
      <td>[-1,0,9]</td>
    </tr>
    <tr>
      <td>2</td>
      <td>ANREDE_KZ</td>
      <td>person</td>
      <td>categorical</td>
      <td>[-1,0]</td>
    </tr>
    <tr>
      <td>3</td>
      <td>CJT_GESAMTTYP</td>
      <td>person</td>
      <td>categorical</td>
      <td>[0]</td>
    </tr>
    <tr>
      <td>4</td>
      <td>FINANZ_MINIMALIST</td>
      <td>person</td>
      <td>ordinal</td>
      <td>[-1]</td>
    </tr>
  </tbody>
</table>
</div>



<blockquote>
<p><strong>Tip</strong>: Add additional cells to keep everything in reasonably-sized chunks! Keyboard shortcut <code>esc --&gt; a</code> (press escape to enter command mode, then press the â€˜Aâ€™ key) adds a new cell before the active cell, and <code>esc --&gt; b</code> adds a new cell after the active cell. If you need to convert an active cell to a markdown cell, use <code>esc --&gt; m</code> and to convert to a code cell, use <code>esc --&gt; y</code>. </p>
</blockquote>
<h2 id="Step-1-Preprocessing"><a href="#Step-1-Preprocessing" class="headerlink" title="Step 1: Preprocessing"></a>Step 1: Preprocessing</h2><h3 id="Step-1-1-Assess-Missing-Data"><a href="#Step-1-1-Assess-Missing-Data" class="headerlink" title="Step 1.1: Assess Missing Data"></a>Step 1.1: Assess Missing Data</h3><p>The feature summary file contains a summary of properties for each demographics data column. You will use this file to help you make cleaning decisions during this stage of the project. First of all, you should assess the demographics data in terms of missing data. Pay attention to the following points as you perform your analysis, and take notes on what you observe. Make sure that you fill in the <strong>Discussion</strong> cell with your findings and decisions at the end of each step that has one!</p>
<h4 id="Step-1-1-1-Convert-Missing-Value-Codes-to-NaNs"><a href="#Step-1-1-1-Convert-Missing-Value-Codes-to-NaNs" class="headerlink" title="Step 1.1.1: Convert Missing Value Codes to NaNs"></a>Step 1.1.1: Convert Missing Value Codes to NaNs</h4><p>The fourth column of the feature attributes summary (loaded in above as <code>feat_info</code>) documents the codes from the data dictionary that indicate missing or unknown data. While the file encodes this as a list (e.g. <code>[-1,0]</code>), this will get read in as a string object. Youâ€™ll need to do a little bit of parsing to make use of it to identify and clean the data. Convert data that matches a â€˜missingâ€™ or â€˜unknownâ€™ value code into a numpy NaN value. You might want to see how much data takes on a â€˜missingâ€™ or â€˜unknownâ€™ code, and how much data is naturally missing, as a point of interest.</p>
<p><strong>As one more reminder, you are encouraged to add additional cells to break up your analysis into manageable chunks.</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># How much data is naturally missing in the dataset for each column.</span></span><br><span class="line">nature_missing = azdias.isna().sum()</span><br><span class="line">nature_missing[nature_missing != <span class="number">0</span>]</span><br></pre></td></tr></table></figure>




<pre><code>CJT_GESAMTTYP            4854
GFK_URLAUBERTYP          4854
LP_LEBENSPHASE_FEIN      4854
LP_LEBENSPHASE_GROB      4854
LP_FAMILIE_FEIN          4854
LP_FAMILIE_GROB          4854
LP_STATUS_FEIN           4854
LP_STATUS_GROB           4854
RETOURTYP_BK_S           4854
SOHO_KZ                 73499
TITEL_KZ                73499
ALTER_HH                73499
ANZ_PERSONEN            73499
ANZ_TITEL               73499
HH_EINKOMMEN_SCORE      18348
KK_KUNDENTYP           584612
W_KEIT_KIND_HH         107602
WOHNDAUER_2008          73499
ANZ_HAUSHALTE_AKTIV     93148
ANZ_HH_TITEL            97008
GEBAEUDETYP             93148
KONSUMNAEHE             73969
MIN_GEBAEUDEJAHR        93148
OST_WEST_KZ             93148
WOHNLAGE                93148
CAMEO_DEUG_2015         98979
CAMEO_DEU_2015          98979
CAMEO_INTL_2015         98979
KBA05_ANTG1            133324
KBA05_ANTG2            133324
KBA05_ANTG3            133324
KBA05_ANTG4            133324
KBA05_BAUMAX           133324
KBA05_GBZ              133324
BALLRAUM                93740
EWDICHTE                93740
INNENSTADT              93740
GEBAEUDETYP_RASTER      93155
KKK                    121196
MOBI_REGIO             133324
ONLINE_AFFINITAET        4854
REGIOTYP               121196
KBA13_ANZAHL_PKW       105800
PLZ8_ANTG1             116515
PLZ8_ANTG2             116515
PLZ8_ANTG3             116515
PLZ8_ANTG4             116515
PLZ8_BAUMAX            116515
PLZ8_HHZ               116515
PLZ8_GBZ               116515
ARBEIT                  97216
ORTSGR_KLS9             97216
RELAT_AB                97216
dtype: int64</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Identify missing or unknown data values and convert them to NaNs.</span></span><br><span class="line">pattern = re.compile(<span class="string">r'\-\d+|\d+'</span>) <span class="comment"># find all numbers</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range (len(feat_info)):</span><br><span class="line">    index = pattern.findall(feat_info[<span class="string">'missing_or_unknown'</span>][i])</span><br><span class="line">    index = list(map(int, index))</span><br><span class="line">    azdias.iloc[:,i] = azdias.iloc[:,i].replace(index,np.nan)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pattern = re.compile(<span class="string">r'[A-Z]+'</span>) <span class="comment"># find all XX</span></span><br><span class="line">i = np.arange(<span class="number">57</span>,<span class="number">60</span>)</span><br><span class="line"><span class="keyword">for</span> each <span class="keyword">in</span> i:</span><br><span class="line">    index = pattern.findall(feat_info[<span class="string">'missing_or_unknown'</span>][each])[<span class="number">0</span>]</span><br><span class="line">    azdias.iloc[:,each] = azdias.iloc[:,each].replace(index,np.nan)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># How mcuh data takes on a 'missing' or 'unknown' code</span></span><br><span class="line">indicated_miss = azdias.isna().sum()-nature_missing</span><br><span class="line">indicated_miss[indicated_miss != <span class="number">0</span>]</span><br></pre></td></tr></table></figure>




<pre><code>AGER_TYP                 685843
ALTERSKATEGORIE_GROB       2881
GEBURTSJAHR              392318
HEALTH_TYP               111196
LP_LEBENSPHASE_FEIN       92778
LP_LEBENSPHASE_GROB       89718
LP_FAMILIE_FEIN           72938
LP_FAMILIE_GROB           72938
NATIONALITAET_KZ         108315
PRAEGENDE_JUGENDJAHRE    108164
SHOPPER_TYP              111196
TITEL_KZ                 815562
VERS_TYP                 111196
ALTER_HH                 236768
W_KEIT_KIND_HH            40386
ANZ_HAUSHALTE_AKTIV        6463
CAMEO_DEUG_2015             373
CAMEO_DEU_2015              373
CAMEO_INTL_2015             373
KBA05_BAUMAX             343200
KKK                       36868
REGIOTYP                  36868
ARBEIT                      159
ORTSGR_KLS9                  58
RELAT_AB                    159
dtype: int64</code></pre><h4 id="Step-1-1-2-Assess-Missing-Data-in-Each-Column"><a href="#Step-1-1-2-Assess-Missing-Data-in-Each-Column" class="headerlink" title="Step 1.1.2: Assess Missing Data in Each Column"></a>Step 1.1.2: Assess Missing Data in Each Column</h4><p>How much missing data is present in each column? There are a few columns that are outliers in terms of the proportion of values that are missing. You will want to use matplotlibâ€™s <a href="https://matplotlib.org/api/_as_gen/matplotlib.pyplot.hist.html" target="_blank" rel="noopener"><code>hist()</code></a> function to visualize the distribution of missing value counts to find these columns. Identify and document these columns. While some of these columns might have justifications for keeping or re-encoding the data, for this project you should just remove them from the dataframe. (Feel free to make remarks about these outlier columns in the discussion, however!)</p>
<p>For the remaining features, are there any patterns in which columns have, or share, missing data?</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Perform an assessment of how much missing data there is in each column of the dataset.</span></span><br><span class="line"></span><br><span class="line">count_nan = azdias.isna().sum().sort_values(ascending = <span class="literal">False</span>)</span><br><span class="line">count_nan = count_nan[count_nan != <span class="number">0</span>] <span class="comment"># sort the counts</span></span><br><span class="line">plt.hist(count_nan,bins=<span class="number">30</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/02/05/Identify_Customer_Segments/output_12_0.png" alt="png"></p>
<p>Based on the graph we can notice most counts of missing values are centered around 150,000, and some are actually have a lot of missing values.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">count_nan = (count_nan/<span class="number">891221</span>)[<span class="number">0</span>:<span class="number">10</span>]</span><br><span class="line">names = list(count_nan.index)</span><br><span class="line">plt.figure()</span><br><span class="line">plt.bar(np.arange(len(names)),count_nan,  alpha=<span class="number">0.5</span>)</span><br><span class="line">plt.xticks(np.arange(len(names)),names,rotation = <span class="number">60</span>,fontsize=<span class="number">10</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Percentage'</span>)</span><br><span class="line">plt.title(<span class="string">'Percentage of missing values'</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/02/05/Identify_Customer_Segments/output_14_0.png" alt="png"></p>
<p>From the graph above we can see top six variables have over 30% values are missing, those should be removed.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Remove the outlier columns from the dataset. (You'll perform other data</span></span><br><span class="line"><span class="comment"># engineering tasks such as re-encoding and imputation later.)</span></span><br><span class="line">remove_names = list(count_nan[<span class="number">0</span>:<span class="number">6</span>].index)</span><br><span class="line">deleted_var = azdias[remove_names] <span class="comment"># save deleted variables</span></span><br><span class="line">azdias.drop(remove_names,axis=<span class="number">1</span>,inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">"Total number of columns: &#123;&#125;"</span>.format(azdias.shape[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>Total number of columns: 79</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Investigate patterns in the amount of missing data in each column.</span></span><br><span class="line">count_nan = azdias.isna().sum().sort_values(ascending = <span class="literal">False</span>)</span><br><span class="line">count_nan = count_nan[count_nan != <span class="number">0</span>] </span><br><span class="line">count_nan</span><br></pre></td></tr></table></figure>




<pre><code>KKK                      158064
REGIOTYP                 158064
W_KEIT_KIND_HH           147988
MOBI_REGIO               133324
KBA05_ANTG4              133324
KBA05_ANTG3              133324
KBA05_ANTG2              133324
KBA05_ANTG1              133324
KBA05_GBZ                133324
PLZ8_ANTG2               116515
PLZ8_ANTG1               116515
PLZ8_ANTG3               116515
PLZ8_ANTG4               116515
PLZ8_BAUMAX              116515
PLZ8_HHZ                 116515
PLZ8_GBZ                 116515
HEALTH_TYP               111196
SHOPPER_TYP              111196
VERS_TYP                 111196
NATIONALITAET_KZ         108315
PRAEGENDE_JUGENDJAHRE    108164
KBA13_ANZAHL_PKW         105800
ANZ_HAUSHALTE_AKTIV       99611
CAMEO_INTL_2015           99352
CAMEO_DEU_2015            99352
CAMEO_DEUG_2015           99352
LP_LEBENSPHASE_FEIN       97632
RELAT_AB                  97375
ARBEIT                    97375
ORTSGR_KLS9               97274
ANZ_HH_TITEL              97008
LP_LEBENSPHASE_GROB       94572
INNENSTADT                93740
BALLRAUM                  93740
EWDICHTE                  93740
GEBAEUDETYP_RASTER        93155
OST_WEST_KZ               93148
MIN_GEBAEUDEJAHR          93148
WOHNLAGE                  93148
GEBAEUDETYP               93148
LP_FAMILIE_FEIN           77792
LP_FAMILIE_GROB           77792
KONSUMNAEHE               73969
ANZ_PERSONEN              73499
ANZ_TITEL                 73499
WOHNDAUER_2008            73499
SOHO_KZ                   73499
HH_EINKOMMEN_SCORE        18348
CJT_GESAMTTYP              4854
GFK_URLAUBERTYP            4854
RETOURTYP_BK_S             4854
ONLINE_AFFINITAET          4854
LP_STATUS_FEIN             4854
LP_STATUS_GROB             4854
ALTERSKATEGORIE_GROB       2881
dtype: int64</code></pre><h4 id="Discussion-1-1-2-Assess-Missing-Data-in-Each-Column"><a href="#Discussion-1-1-2-Assess-Missing-Data-in-Each-Column" class="headerlink" title="Discussion 1.1.2: Assess Missing Data in Each Column"></a>Discussion 1.1.2: Assess Missing Data in Each Column</h4><p>Some columns have exactly the same amount of missing values, which means they might also share those missing values. The variables which have more than 30% missing values are deleted. The deleted variables are TITEL_KZ, AGER_TYP, KK_KUNDENTYP, KBA05_BAUMAX, GEBURTSJAHR, ALTER_HH.</p>
<h4 id="Step-1-1-3-Assess-Missing-Data-in-Each-Row"><a href="#Step-1-1-3-Assess-Missing-Data-in-Each-Row" class="headerlink" title="Step 1.1.3: Assess Missing Data in Each Row"></a>Step 1.1.3: Assess Missing Data in Each Row</h4><p>Now, youâ€™ll perform a similar assessment for the rows of the dataset. How much data is missing in each row? As with the columns, you should see some groups of points that have a very different numbers of missing values. Divide the data into two subsets: one for data points that are above some threshold for missing values, and a second subset for points below that threshold.</p>
<p>In order to know what to do with the outlier rows, we should see if the distribution of data values on columns that are not missing data (or are missing very little data) are similar or different between the two groups. Select at least five of these columns and compare the distribution of values.</p>
<ul>
<li>You can use seabornâ€™s <a href="https://seaborn.pydata.org/generated/seaborn.countplot.html" target="_blank" rel="noopener"><code>countplot()</code></a> function to create a bar chart of code frequencies and matplotlibâ€™s <a href="https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html" target="_blank" rel="noopener"><code>subplot()</code></a> function to put bar charts for the two subplots side by side.</li>
<li>To reduce repeated code, you might want to write a function that can perform this comparison, taking as one of its arguments a column to be compared.</li>
</ul>
<p>Depending on what you observe in your comparison, this will have implications on how you approach your conclusions later in the analysis. If the distributions of non-missing features look similar between the data with many missing values and the data with few or no missing values, then we could argue that simply dropping those points from the analysis wonâ€™t present a major issue. On the other hand, if the data with many missing values looks very different from the data with few or no missing values, then we should make a note on those data as special. Weâ€™ll revisit these data later on. <strong>Either way, you should continue your analysis for now using just the subset of the data with few or no missing values.</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># How much data is missing in each row of the dataset?</span></span><br><span class="line">row_miss = azdias.isna().sum(axis = <span class="number">1</span>)</span><br><span class="line">sns.countplot(row_miss)</span><br><span class="line">plt.xticks(rotation = <span class="number">90</span>,fontsize=<span class="number">10</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/02/05/Identify_Customer_Segments/output_21_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Write code to divide the data into two subsets based on the number of missing</span></span><br><span class="line"><span class="comment"># values in each row.</span></span><br><span class="line">azdias_no_miss = azdias[azdias.isna().sum(axis = <span class="number">1</span>) == <span class="number">0</span>]</span><br><span class="line">azdias_with_miss = azdias[azdias.isna().sum(axis = <span class="number">1</span>) &gt;= <span class="number">1</span>]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Compare the distribution of values for at least five columns where there are</span></span><br><span class="line"><span class="comment"># no missing values, between the two subsets.</span></span><br><span class="line"></span><br><span class="line">list(azdias.isna().sum().sort_values().head(<span class="number">20</span>).index) <span class="comment"># choose columns with no missing values</span></span><br></pre></td></tr></table></figure>




<pre><code>[&apos;ZABEOTYP&apos;,
 &apos;SEMIO_REL&apos;,
 &apos;SEMIO_MAT&apos;,
 &apos;SEMIO_VERT&apos;,
 &apos;SEMIO_LUST&apos;,
 &apos;SEMIO_ERL&apos;,
 &apos;SEMIO_KULT&apos;,
 &apos;SEMIO_RAT&apos;,
 &apos;SEMIO_KRIT&apos;,
 &apos;SEMIO_DOM&apos;,
 &apos;SEMIO_KAEM&apos;,
 &apos;GREEN_AVANTGARDE&apos;,
 &apos;SEMIO_PFLICHT&apos;,
 &apos;FINANZTYP&apos;,
 &apos;FINANZ_HAUSBAUER&apos;,
 &apos;FINANZ_UNAUFFAELLIGER&apos;,
 &apos;FINANZ_ANLEGER&apos;,
 &apos;FINANZ_VORSORGER&apos;,
 &apos;FINANZ_SPARER&apos;,
 &apos;FINANZ_MINIMALIST&apos;]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># define a function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compare_dist</span><span class="params">(name)</span>:</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>,<span class="number">5</span>))</span><br><span class="line">    ax = plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">    plt.hist(azdias_no_miss[name],bins = <span class="number">30</span>)</span><br><span class="line">    ax2 = plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">    plt.hist(azdias_with_miss[name],bins = <span class="number">30</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 'ZABEOTYP'</span></span><br><span class="line">compare_dist(<span class="string">'ZABEOTYP'</span>)</span><br></pre></td></tr></table></figure>


<p><img src="/2020/02/05/Identify_Customer_Segments/output_25_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 'SEMIO_LUST'</span></span><br><span class="line">compare_dist(<span class="string">'SEMIO_LUST'</span>)</span><br></pre></td></tr></table></figure>


<p><img src="/2020/02/05/Identify_Customer_Segments/output_26_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 'GREEN_AVANTGARDE'</span></span><br><span class="line">compare_dist(<span class="string">'GREEN_AVANTGARDE'</span>)</span><br></pre></td></tr></table></figure>


<p><img src="/2020/02/05/Identify_Customer_Segments/output_27_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 'FINANZ_HAUSBAUER'</span></span><br><span class="line">compare_dist(<span class="string">'FINANZ_HAUSBAUER'</span>)</span><br></pre></td></tr></table></figure>


<p><img src="/2020/02/05/Identify_Customer_Segments/output_28_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 'SEMIO_RAT'</span></span><br><span class="line">compare_dist(<span class="string">'SEMIO_RAT'</span>)</span><br></pre></td></tr></table></figure>


<p><img src="/2020/02/05/Identify_Customer_Segments/output_29_0.png" alt="png"></p>
<h4 id="Discussion-1-1-3-Assess-Missing-Data-in-Each-Row"><a href="#Discussion-1-1-3-Assess-Missing-Data-in-Each-Row" class="headerlink" title="Discussion 1.1.3: Assess Missing Data in Each Row"></a>Discussion 1.1.3: Assess Missing Data in Each Row</h4><p>I divide the dataset in two subsets, one without missing values and the other with missing values. I did this because rows without missing values takes the big amount.</p>
<p>The data with missing values is not qualitatively different from the data with no missing values, based on five bar charts above.</p>
<h3 id="Step-1-2-Select-and-Re-Encode-Features"><a href="#Step-1-2-Select-and-Re-Encode-Features" class="headerlink" title="Step 1.2: Select and Re-Encode Features"></a>Step 1.2: Select and Re-Encode Features</h3><p>Checking for missing data isnâ€™t the only way in which you can prepare a dataset for analysis. Since the unsupervised learning techniques to be used will only work on data that is encoded numerically, you need to make a few encoding changes or additional assumptions to be able to make progress. In addition, while almost all of the values in the dataset are encoded using numbers, not all of them represent numeric values. Check the third column of the feature summary (<code>feat_info</code>) for a summary of types of measurement.</p>
<ul>
<li>For numeric and interval data, these features can be kept without changes.</li>
<li>Most of the variables in the dataset are ordinal in nature. While ordinal values may technically be non-linear in spacing, make the simplifying assumption that the ordinal variables can be treated as being interval in nature (that is, kept without any changes).</li>
<li>Special handling may be necessary for the remaining two variable types: categorical, and â€˜mixedâ€™.</li>
</ul>
<p>In the first two parts of this sub-step, you will perform an investigation of the categorical and mixed-type features and make a decision on each of them, whether you will keep, drop, or re-encode each. Then, in the last part, you will create a new data frame with only the selected and engineered columns.</p>
<p>Data wrangling is often the trickiest part of the data analysis process, and thereâ€™s a lot of it to be done here. But stick with it: once youâ€™re done with this step, youâ€™ll be ready to get to the machine learning parts of the project!</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># How many features are there of each data type?</span></span><br><span class="line"><span class="comment"># azdias_less_miss # use dataset with less missing values</span></span><br><span class="line"><span class="keyword">for</span> each <span class="keyword">in</span> remove_names:</span><br><span class="line">    feat_info = feat_info[feat_info[<span class="string">'attribute'</span>] != each]</span><br><span class="line">    </span><br><span class="line">categorical_v = feat_info[feat_info[<span class="string">'type'</span>] == <span class="string">'categorical'</span>]</span><br><span class="line">ordinal_v = feat_info[feat_info[<span class="string">'type'</span>] == <span class="string">'ordinal'</span>]</span><br><span class="line">numeric_v = feat_info[feat_info[<span class="string">'type'</span>] == <span class="string">'numeric'</span>]</span><br><span class="line">mixed_v = feat_info[feat_info[<span class="string">'type'</span>] == <span class="string">'mixed'</span>]</span><br><span class="line">interval_v = feat_info[feat_info[<span class="string">'type'</span>] == <span class="string">'interval'</span>]</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Total number of numeric variables: &#123;&#125;"</span>.format(len(numeric_v)))</span><br><span class="line">print(<span class="string">"Total number of interval variables: &#123;&#125;"</span>.format(len(interval_v)))</span><br><span class="line">print(<span class="string">"Total number of ordinal variables: &#123;&#125;"</span>.format(len(ordinal_v)))</span><br><span class="line">print(<span class="string">"Total number of categorical variables: &#123;&#125;"</span>.format(len(categorical_v)))</span><br><span class="line">print(<span class="string">"Total number of mixed variables: &#123;&#125;"</span>.format(len(mixed_v)))</span><br></pre></td></tr></table></figure>

<pre><code>Total number of numeric variables: 6
Total number of interval variables: 0
Total number of ordinal variables: 49
Total number of categorical variables: 18
Total number of mixed variables: 6</code></pre><h4 id="Step-1-2-1-Re-Encode-Categorical-Features"><a href="#Step-1-2-1-Re-Encode-Categorical-Features" class="headerlink" title="Step 1.2.1: Re-Encode Categorical Features"></a>Step 1.2.1: Re-Encode Categorical Features</h4><p>For categorical data, you would ordinarily need to encode the levels as dummy variables. Depending on the number of categories, perform one of the following:</p>
<ul>
<li>For binary (two-level) categoricals that take numeric values, you can keep them without needing to do anything.</li>
<li>There is one binary variable that takes on non-numeric values. For this one, you need to re-encode the values as numbers or create a dummy variable.</li>
<li>For multi-level categoricals (three or more values), you can choose to encode the values using multiple dummy variables (e.g. via <a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html" target="_blank" rel="noopener">OneHotEncoder</a>), or (to keep things straightforward) just drop them from the analysis. As always, document your choices in the Discussion section.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Assess categorical variables: which are binary, which are multi-level, and</span></span><br><span class="line"><span class="comment"># which one needs to be re-encoded?</span></span><br><span class="line">binary_v = [<span class="string">'ANREDE_KZ'</span>, <span class="string">'GREEN_AVANTGARDE'</span>,<span class="string">'SOHO_KZ'</span>,<span class="string">'VERS_TYP'</span>,<span class="string">'OST_WEST_KZ'</span>] <span class="comment"># all binary variables and 'OST_WEST_KZ' should be re-encoded</span></span><br><span class="line">multi_level_v=categorical_v</span><br><span class="line"><span class="keyword">for</span> each <span class="keyword">in</span> binary_v:</span><br><span class="line">    multi_level_v = multi_level_v[multi_level_v[<span class="string">'attribute'</span>] != each]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Re-encode categorical variable(s) to be kept in the analysis.</span></span><br><span class="line"><span class="comment"># re-encode the binary variable</span></span><br><span class="line">azdias_no_miss.loc[:, <span class="string">'OST_WEST_KZ'</span>] = azdias_no_miss.loc[:, <span class="string">'OST_WEST_KZ'</span>].replace(<span class="string">'O'</span>,<span class="number">0</span>)</span><br><span class="line">azdias_no_miss.loc[:, <span class="string">'OST_WEST_KZ'</span>] = azdias_no_miss.loc[:, <span class="string">'OST_WEST_KZ'</span>].replace(<span class="string">'W'</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># re-encode the multi-level variables</span></span><br><span class="line">multi_level_row = azdias_no_miss[multi_level_v[<span class="string">'attribute'</span>]]</span><br><span class="line">multi_level_full = pd.get_dummies(multi_level_row.astype(str))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Change the original dataset</span></span><br><span class="line">names = list(multi_level_v[<span class="string">'attribute'</span>])</span><br><span class="line">azdias_no_miss.drop(names,axis=<span class="number">1</span>,inplace = <span class="literal">True</span>)</span><br><span class="line">azdias_no_miss = pd.concat([azdias_no_miss, multi_level_full],axis=<span class="number">1</span> )</span><br><span class="line">encoded = list(azdias_no_miss.columns)</span><br><span class="line">print(<span class="string">"&#123;&#125; total features after one-hot encoding."</span>.format(len(encoded)))</span><br></pre></td></tr></table></figure>

<pre><code>191 total features after one-hot encoding.</code></pre><h4 id="Discussion-1-2-1-Re-Encode-Categorical-Features"><a href="#Discussion-1-2-1-Re-Encode-Categorical-Features" class="headerlink" title="Discussion 1.2.1: Re-Encode Categorical Features"></a>Discussion 1.2.1: Re-Encode Categorical Features</h4><p>For binary variable â€˜OST_WEST_KZâ€™ I transform it to numeric values, For multi-level variables I transform all of them into dummy variables.</p>
<h4 id="Step-1-2-2-Engineer-Mixed-Type-Features"><a href="#Step-1-2-2-Engineer-Mixed-Type-Features" class="headerlink" title="Step 1.2.2: Engineer Mixed-Type Features"></a>Step 1.2.2: Engineer Mixed-Type Features</h4><p>There are a handful of features that are marked as â€œmixedâ€ in the feature summary that require special treatment in order to be included in the analysis. There are two in particular that deserve attention; the handling of the rest are up to your own choices:</p>
<ul>
<li>â€œPRAEGENDE_JUGENDJAHREâ€ combines information on three dimensions: generation by decade, movement (mainstream vs. avantgarde), and nation (east vs. west). While there arenâ€™t enough levels to disentangle east from west, you should create two new variables to capture the other two dimensions: an interval-type variable for decade, and a binary variable for movement.</li>
<li>â€œCAMEO_INTL_2015â€ combines information on two axes: wealth and life stage. Break up the two-digit codes by their â€˜tensâ€™-place and â€˜onesâ€™-place digits into two new ordinal variables (which, for the purposes of this project, is equivalent to just treating them as their raw numeric values).</li>
<li>If you decide to keep or engineer new features around the other mixed-type features, make sure you note your steps in the Discussion section.</li>
</ul>
<p>Be sure to check <code>Data_Dictionary.md</code> for the details needed to finish these tasks.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Investigate "PRAEGENDE_JUGENDJAHRE" and engineer two new variables.</span></span><br><span class="line"><span class="comment"># movement: 1 for Mainstream, 0 for Avantgarde</span></span><br><span class="line">movement = azdias_no_miss[<span class="string">'PRAEGENDE_JUGENDJAHRE'</span>] == <span class="number">1</span> | <span class="number">3</span> | <span class="number">5</span> | <span class="number">8</span> | <span class="number">10</span> | <span class="number">12</span> | <span class="number">14</span></span><br><span class="line">movement = movement.astype(int)</span><br><span class="line">decade = []</span><br><span class="line">i = <span class="number">0</span></span><br><span class="line">temp = azdias_no_miss[<span class="string">'PRAEGENDE_JUGENDJAHRE'</span>]</span><br><span class="line"><span class="keyword">for</span> each <span class="keyword">in</span> temp:</span><br><span class="line">    <span class="keyword">if</span> each == <span class="number">1</span> <span class="keyword">or</span> each == <span class="number">2</span>:</span><br><span class="line">        i = <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> each == <span class="number">3</span> <span class="keyword">or</span> each == <span class="number">4</span>:</span><br><span class="line">        i = <span class="number">2</span> </span><br><span class="line">    <span class="keyword">elif</span> each == <span class="number">5</span> <span class="keyword">or</span> each == <span class="number">6</span> <span class="keyword">or</span> each == <span class="number">7</span>:</span><br><span class="line">        i = <span class="number">3</span></span><br><span class="line">    <span class="keyword">elif</span> each == <span class="number">8</span> <span class="keyword">or</span> each ==<span class="number">9</span>:</span><br><span class="line">        i = <span class="number">4</span></span><br><span class="line">    <span class="keyword">elif</span> each == <span class="number">10</span> <span class="keyword">or</span> each ==<span class="number">11</span> <span class="keyword">or</span> each ==<span class="number">12</span> <span class="keyword">or</span> each ==<span class="number">13</span>:</span><br><span class="line">        i = <span class="number">5</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        i = <span class="number">6</span></span><br><span class="line">    decade.append(i)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Investigate "CAMEO_INTL_2015" and engineer two new variables.</span></span><br><span class="line">wealth = azdias_no_miss[<span class="string">'CAMEO_INTL_2015'</span>].astype(int)/<span class="number">10</span></span><br><span class="line">wealth = wealth.astype(int)</span><br><span class="line">life_stage = azdias_no_miss[<span class="string">'CAMEO_INTL_2015'</span>].astype(int) % <span class="number">10</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Drop original columns and add the new columns</span></span><br><span class="line">names = list(mixed_v[<span class="string">'attribute'</span>])</span><br><span class="line">azdias_no_miss.drop(names,axis=<span class="number">1</span>,inplace = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">azdias_no_miss[<span class="string">'movement'</span>] = movement</span><br><span class="line">azdias_no_miss[<span class="string">'decade'</span>] = decade</span><br><span class="line">azdias_no_miss[<span class="string">'wealth'</span>] = wealth</span><br><span class="line">azdias_no_miss[<span class="string">'life_stage'</span>] = life_stage</span><br></pre></td></tr></table></figure>

<h4 id="Discussion-1-2-2-Engineer-Mixed-Type-Features"><a href="#Discussion-1-2-2-Engineer-Mixed-Type-Features" class="headerlink" title="Discussion 1.2.2: Engineer Mixed-Type Features"></a>Discussion 1.2.2: Engineer Mixed-Type Features</h4><p>I transformed â€œPRAEGENDE_JUGENDJAHREâ€  and â€œCAMEO_INTL_2015â€ . For other variables I just dropped them since some are overlapped with existed variables, for example, â€œLP_LEBENSPHASE_GROBâ€.</p>
<h4 id="Step-1-2-3-Complete-Feature-Selection"><a href="#Step-1-2-3-Complete-Feature-Selection" class="headerlink" title="Step 1.2.3: Complete Feature Selection"></a>Step 1.2.3: Complete Feature Selection</h4><p>In order to finish this step up, you need to make sure that your data frame now only has the columns that you want to keep. To summarize, the dataframe should consist of the following:</p>
<ul>
<li>All numeric, interval, and ordinal type columns from the original dataset.</li>
<li>Binary categorical features (all numerically-encoded).</li>
<li>Engineered features from other multi-level categorical features and mixed features.</li>
</ul>
<p>Make sure that for any new columns that you have engineered, that youâ€™ve excluded the original columns from the final dataset. Otherwise, their values will interfere with the analysis later on the project. For example, you should not keep â€œPRAEGENDE_JUGENDJAHREâ€, since its values wonâ€™t be useful for the algorithm: only the values derived from it in the engineered features you created should be retained. As a reminder, your data should only be from <strong>the subset with few or no missing values</strong>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># If there are other re-engineering tasks you need to perform, make sure you</span></span><br><span class="line"><span class="comment"># take care of them here. (Dealing with missing data will come in step 2.1.)</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Do whatever you need to in order to ensure that the dataframe only contains</span></span><br><span class="line"><span class="comment"># the columns that should be passed to the algorithm functions.</span></span><br></pre></td></tr></table></figure>

<h3 id="Step-1-3-Create-a-Cleaning-Function"><a href="#Step-1-3-Create-a-Cleaning-Function" class="headerlink" title="Step 1.3: Create a Cleaning Function"></a>Step 1.3: Create a Cleaning Function</h3><p>Even though youâ€™ve finished cleaning up the general population demographics data, itâ€™s important to look ahead to the future and realize that youâ€™ll need to perform the same cleaning steps on the customer demographics data. In this substep, complete the function below to execute the main feature selection, encoding, and re-engineering steps you performed above. Then, when it comes to looking at the customer data in Step 3, you can just run this function on that DataFrame to get the trimmed dataset in a single step.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clean_data</span><span class="params">(df)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Perform feature trimming, re-encoding, and engineering for demographics</span></span><br><span class="line"><span class="string">    data</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    INPUT: Demographics DataFrame</span></span><br><span class="line"><span class="string">    OUTPUT: Trimmed and cleaned demographics DataFrame</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    feat_info = pd.read_csv(<span class="string">'AZDIAS_Feature_Summary.csv'</span>,sep=<span class="string">';'</span>)</span><br><span class="line">    <span class="comment"># Put in code here to execute all main cleaning steps:</span></span><br><span class="line">    <span class="comment"># convert missing value codes into NaNs, ...</span></span><br><span class="line">    <span class="comment"># Identify missing or unknown data values and convert them to NaNs.</span></span><br><span class="line">    pattern = re.compile(<span class="string">r'\-\d+|\d+'</span>) <span class="comment"># find all numbers</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range (len(feat_info)):</span><br><span class="line">        index = pattern.findall(feat_info[<span class="string">'missing_or_unknown'</span>][i])</span><br><span class="line">        index = list(map(int, index))</span><br><span class="line">        df.iloc[:,i] = df.iloc[:,i].replace(index,np.nan)</span><br><span class="line">    pattern = re.compile(<span class="string">r'[A-Z]+'</span>) <span class="comment"># find all XX</span></span><br><span class="line">    i = np.arange(<span class="number">57</span>,<span class="number">60</span>)</span><br><span class="line">    <span class="keyword">for</span> each <span class="keyword">in</span> i:</span><br><span class="line">        index = pattern.findall(feat_info[<span class="string">'missing_or_unknown'</span>][each])[<span class="number">0</span>]</span><br><span class="line">        df.iloc[:,each] = df.iloc[:,each].replace(index,np.nan)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># remove selected columns and rows, ...</span></span><br><span class="line">    count_nan = df.isna().sum().sort_values(ascending = <span class="literal">False</span>)</span><br><span class="line">    count_nan = count_nan[count_nan != <span class="number">0</span>] <span class="comment"># sort the counts</span></span><br><span class="line">    remove_names = [<span class="string">'TITEL_KZ'</span>,<span class="string">'AGER_TYP'</span>,<span class="string">'KK_KUNDENTYP'</span>,<span class="string">'KBA05_BAUMAX'</span>,<span class="string">'GEBURTSJAHR'</span>,<span class="string">'ALTER_HH'</span>]<span class="comment"># save deleted variables</span></span><br><span class="line">    df.drop(remove_names,axis=<span class="number">1</span>,inplace=<span class="literal">True</span>)</span><br><span class="line">    azdias_no_miss = df[df.isna().sum(axis = <span class="number">1</span>) == <span class="number">0</span>]</span><br><span class="line">    azdias_with_miss = df[df.isna().sum(axis = <span class="number">1</span>) &gt;= <span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># select, re-encode, and engineer column values.</span></span><br><span class="line">    <span class="keyword">for</span> each <span class="keyword">in</span> remove_names:</span><br><span class="line">        feat_info = feat_info[feat_info[<span class="string">'attribute'</span>] != each]</span><br><span class="line">    </span><br><span class="line">    categorical_v = feat_info[feat_info[<span class="string">'type'</span>] == <span class="string">'categorical'</span>]</span><br><span class="line">    ordinal_v = feat_info[feat_info[<span class="string">'type'</span>] == <span class="string">'ordinal'</span>]</span><br><span class="line">    numeric_v = feat_info[feat_info[<span class="string">'type'</span>] == <span class="string">'numeric'</span>]</span><br><span class="line">    mixed_v = feat_info[feat_info[<span class="string">'type'</span>] == <span class="string">'mixed'</span>]</span><br><span class="line">    interval_v = feat_info[feat_info[<span class="string">'type'</span>] == <span class="string">'interval'</span>]</span><br><span class="line">    </span><br><span class="line">    binary_v = [<span class="string">'ANREDE_KZ'</span>, <span class="string">'GREEN_AVANTGARDE'</span>,<span class="string">'SOHO_KZ'</span>,<span class="string">'VERS_TYP'</span>,<span class="string">'OST_WEST_KZ'</span>] <span class="comment"># all binary variables and 'OST_WEST_KZ' should be re-encoded</span></span><br><span class="line">    multi_level_v=categorical_v</span><br><span class="line">    <span class="keyword">for</span> each <span class="keyword">in</span> binary_v:</span><br><span class="line">        multi_level_v = multi_level_v[multi_level_v[<span class="string">'attribute'</span>] != each]</span><br><span class="line">    azdias_no_miss.loc[:, <span class="string">'OST_WEST_KZ'</span>] = azdias_no_miss.loc[:, <span class="string">'OST_WEST_KZ'</span>].replace(<span class="string">'O'</span>,<span class="number">0</span>)</span><br><span class="line">    azdias_no_miss.loc[:, <span class="string">'OST_WEST_KZ'</span>] = azdias_no_miss.loc[:, <span class="string">'OST_WEST_KZ'</span>].replace(<span class="string">'W'</span>,<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># re-encode the multi-level variables</span></span><br><span class="line">    multi_level_row = azdias_no_miss[multi_level_v[<span class="string">'attribute'</span>]]</span><br><span class="line">    multi_level_full = pd.get_dummies(multi_level_row.astype(str))</span><br><span class="line">    <span class="comment"># Change the original dataset</span></span><br><span class="line">    names = list(multi_level_v[<span class="string">'attribute'</span>])</span><br><span class="line">    azdias_no_miss.drop(names,axis=<span class="number">1</span>,inplace = <span class="literal">True</span>)</span><br><span class="line">    azdias_no_miss = pd.concat([azdias_no_miss, multi_level_full],axis=<span class="number">1</span> )</span><br><span class="line">    </span><br><span class="line">    movement = azdias_no_miss[<span class="string">'PRAEGENDE_JUGENDJAHRE'</span>] == <span class="number">1</span> | <span class="number">3</span> | <span class="number">5</span> | <span class="number">8</span> | <span class="number">10</span> | <span class="number">12</span> | <span class="number">14</span></span><br><span class="line">    movement = movement.astype(int)</span><br><span class="line">    decade = []</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    temp = azdias_no_miss[<span class="string">'PRAEGENDE_JUGENDJAHRE'</span>]</span><br><span class="line">    <span class="keyword">for</span> each <span class="keyword">in</span> temp:</span><br><span class="line">        <span class="keyword">if</span> each == <span class="number">1</span> <span class="keyword">or</span> each == <span class="number">2</span>:</span><br><span class="line">            i = <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> each == <span class="number">3</span> <span class="keyword">or</span> each == <span class="number">4</span>:</span><br><span class="line">            i = <span class="number">2</span> </span><br><span class="line">        <span class="keyword">elif</span> each == <span class="number">5</span> <span class="keyword">or</span> each == <span class="number">6</span> <span class="keyword">or</span> each == <span class="number">7</span>:</span><br><span class="line">            i = <span class="number">3</span></span><br><span class="line">        <span class="keyword">elif</span> each == <span class="number">8</span> <span class="keyword">or</span> each ==<span class="number">9</span>:</span><br><span class="line">            i = <span class="number">4</span></span><br><span class="line">        <span class="keyword">elif</span> each == <span class="number">10</span> <span class="keyword">or</span> each ==<span class="number">11</span> <span class="keyword">or</span> each ==<span class="number">12</span> <span class="keyword">or</span> each ==<span class="number">13</span>:</span><br><span class="line">            i = <span class="number">5</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            i = <span class="number">6</span></span><br><span class="line">        decade.append(i)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Investigate "CAMEO_INTL_2015" and engineer two new variables.</span></span><br><span class="line">    wealth = azdias_no_miss[<span class="string">'CAMEO_INTL_2015'</span>].astype(int)/<span class="number">10</span></span><br><span class="line">    wealth = wealth.astype(int)</span><br><span class="line">    life_stage = azdias_no_miss[<span class="string">'CAMEO_INTL_2015'</span>].astype(int) % <span class="number">10</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Drop original columns and add the new columns</span></span><br><span class="line">    names = list(mixed_v[<span class="string">'attribute'</span>])</span><br><span class="line">    azdias_no_miss.drop(names,axis=<span class="number">1</span>,inplace = <span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    azdias_no_miss[<span class="string">'movement'</span>] = movement</span><br><span class="line">    azdias_no_miss[<span class="string">'decade'</span>] = decade</span><br><span class="line">    azdias_no_miss[<span class="string">'wealth'</span>] = wealth</span><br><span class="line">    azdias_no_miss[<span class="string">'life_stage'</span>] = life_stage</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Return the cleaned dataframe.</span></span><br><span class="line">    <span class="keyword">return</span> azdias_no_miss</span><br></pre></td></tr></table></figure>

<h2 id="Step-2-Feature-Transformation"><a href="#Step-2-Feature-Transformation" class="headerlink" title="Step 2: Feature Transformation"></a>Step 2: Feature Transformation</h2><h3 id="Step-2-1-Apply-Feature-Scaling"><a href="#Step-2-1-Apply-Feature-Scaling" class="headerlink" title="Step 2.1: Apply Feature Scaling"></a>Step 2.1: Apply Feature Scaling</h3><p>Before we apply dimensionality reduction techniques to the data, we need to perform feature scaling so that the principal component vectors are not influenced by the natural differences in scale for features. Starting from this part of the project, youâ€™ll want to keep an eye on the <a href="http://scikit-learn.org/stable/modules/classes.html" target="_blank" rel="noopener">API reference page for sklearn</a> to help you navigate to all of the classes and functions that youâ€™ll need. In this substep, youâ€™ll need to check the following:</p>
<ul>
<li>sklearn requires that data not have missing values in order for its estimators to work properly. So, before applying the scaler to your data, make sure that youâ€™ve cleaned the DataFrame of the remaining missing values. This can be as simple as just removing all data points with missing data, or applying an <a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html" target="_blank" rel="noopener">Imputer</a> to replace all missing values. You might also try a more complicated procedure where you temporarily remove missing values in order to compute the scaling parameters before re-introducing those missing values and applying imputation. Think about how much missing data you have and what possible effects each approach might have on your analysis, and justify your decision in the discussion section below.</li>
<li>For the actual scaling function, a <a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html" target="_blank" rel="noopener">StandardScaler</a> instance is suggested, scaling each feature to mean 0 and standard deviation 1.</li>
<li>For these classes, you can make use of the <code>.fit_transform()</code> method to both fit a procedure to the data as well as apply the transformation to the data at the same time. Donâ€™t forget to keep the fit sklearn objects handy, since youâ€™ll be applying them to the customer demographics data towards the end of the project.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># If you've not yet cleaned the dataset of all NaN values, then investigate and</span></span><br><span class="line"><span class="comment"># do that now.</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Apply feature scaling to the general population demographics data.</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">azdias_trans = pd.DataFrame(scaler.fit_transform(azdias_no_miss))</span><br><span class="line">azdias_trans.columns = azdias_no_miss.columns</span><br></pre></td></tr></table></figure>

<h3 id="Discussion-2-1-Apply-Feature-Scaling"><a href="#Discussion-2-1-Apply-Feature-Scaling" class="headerlink" title="Discussion 2.1: Apply Feature Scaling"></a>Discussion 2.1: Apply Feature Scaling</h3><p>I removed all missing values brfore, and I transformed all features with StandardScaler.</p>
<h3 id="Step-2-2-Perform-Dimensionality-Reduction"><a href="#Step-2-2-Perform-Dimensionality-Reduction" class="headerlink" title="Step 2.2: Perform Dimensionality Reduction"></a>Step 2.2: Perform Dimensionality Reduction</h3><p>On your scaled data, you are now ready to apply dimensionality reduction techniques.</p>
<ul>
<li>Use sklearnâ€™s <a href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" target="_blank" rel="noopener">PCA</a> class to apply principal component analysis on the data, thus finding the vectors of maximal variance in the data. To start, you should not set any parameters (so all components are computed) or set a number of components that is at least half the number of features (so thereâ€™s enough features to see the general trend in variability).</li>
<li>Check out the ratio of variance explained by each principal component as well as the cumulative variance explained. Try plotting the cumulative or sequential values using matplotlibâ€™s <a href="https://matplotlib.org/api/_as_gen/matplotlib.pyplot.plot.html" target="_blank" rel="noopener"><code>plot()</code></a> function. Based on what you find, select a value for the number of transformed features youâ€™ll retain for the clustering part of the project.</li>
<li>Once youâ€™ve made a choice for the number of components to keep, make sure you re-fit a PCA instance to perform the decided-on transformation.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Apply PCA to the data.</span></span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line">pca = PCA()</span><br><span class="line">azdias_pca = pca.fit_transform(azdias_trans)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Investigate the variance accounted for by each principal component.</span></span><br><span class="line">var_ratio = pca.explained_variance_ratio_</span><br><span class="line">plt.plot(var_ratio)</span><br></pre></td></tr></table></figure>




<pre><code>[&lt;matplotlib.lines.Line2D at 0x1f0a9327e08&gt;]</code></pre><p><img src="/2020/02/05/Identify_Customer_Segments/output_56_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.plot(np.cumsum(var_ratio))</span><br></pre></td></tr></table></figure>




<pre><code>[&lt;matplotlib.lines.Line2D at 0x1f0a8f7dc08&gt;]</code></pre><p><img src="/2020/02/05/Identify_Customer_Segments/output_57_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Re-apply PCA to the data while selecting for number of components to retain.</span></span><br><span class="line">pca = PCA(n_components=<span class="number">100</span>)</span><br><span class="line">azdias_pca = pca.fit_transform(azdias_trans)</span><br></pre></td></tr></table></figure>

<h3 id="Discussion-2-2-Perform-Dimensionality-Reduction"><a href="#Discussion-2-2-Perform-Dimensionality-Reduction" class="headerlink" title="Discussion 2.2: Perform Dimensionality Reduction"></a>Discussion 2.2: Perform Dimensionality Reduction</h3><p>With 100 components, over 80% variance can be explained. Thus I choose 100 components.</p>
<h3 id="Step-2-3-Interpret-Principal-Components"><a href="#Step-2-3-Interpret-Principal-Components" class="headerlink" title="Step 2.3: Interpret Principal Components"></a>Step 2.3: Interpret Principal Components</h3><p>Now that we have our transformed principal components, itâ€™s a nice idea to check out the weight of each variable on the first few components to see if they can be interpreted in some fashion.</p>
<p>As a reminder, each principal component is a unit vector that points in the direction of highest variance (after accounting for the variance captured by earlier principal components). The further a weight is from zero, the more the principal component is in the direction of the corresponding feature. If two features have large weights of the same sign (both positive or both negative), then increases in one tend expect to be associated with increases in the other. To contrast, features with different signs can be expected to show a negative correlation: increases in one variable should result in a decrease in the other.</p>
<ul>
<li>To investigate the features, you should map each weight to their corresponding feature name, then sort the features according to weight. The most interesting features for each principal component, then, will be those at the beginning and end of the sorted list. Use the data dictionary document to help you understand these most prominent features, their relationships, and what a positive or negative value on the principal component might indicate.</li>
<li>You should investigate and interpret feature associations from the first three principal components in this substep. To help facilitate this, you should write a function that you can call at any time to print the sorted list of feature weights, for the <em>i</em>-th principal component. This might come in handy in the next step of the project, when you interpret the tendencies of the discovered clusters.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Map weights for the first principal component to corresponding feature names</span></span><br><span class="line"><span class="comment"># and then print the linked values, sorted by weight.</span></span><br><span class="line"><span class="comment"># HINT: Try defining a function here or in a new cell that you can reuse in the</span></span><br><span class="line"><span class="comment"># other cells.</span></span><br><span class="line">feature_select = pd.DataFrame(pca.components_,columns=azdias_trans.columns).T</span><br><span class="line">feature_select[<span class="number">0</span>].sort_values()</span><br></pre></td></tr></table></figure>




<pre><code>MOBI_REGIO           -0.202737
FINANZ_MINIMALIST    -0.195380
KBA05_ANTG1          -0.190736
PLZ8_ANTG1           -0.180144
KBA05_GBZ            -0.179441
                        ...   
PLZ8_ANTG4            0.172431
PLZ8_ANTG3            0.178166
HH_EINKOMMEN_SCORE    0.178693
wealth                0.180169
LP_STATUS_GROB_1.0    0.192540
Name: 0, Length: 189, dtype: float64</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Map weights for the second principal component to corresponding feature names</span></span><br><span class="line"><span class="comment"># and then print the linked values, sorted by weight.</span></span><br><span class="line"></span><br><span class="line">feature_select[<span class="number">1</span>].sort_values()</span><br></pre></td></tr></table></figure>




<pre><code>decade                  -0.228497
SEMIO_REL               -0.213832
FINANZ_SPARER           -0.212535
FINANZ_UNAUFFAELLIGER   -0.207468
SEMIO_PFLICHT           -0.205000
                           ...   
RETOURTYP_BK_S           0.156204
SEMIO_ERL                0.181138
ZABEOTYP_3               0.197800
FINANZ_VORSORGER         0.205081
ALTERSKATEGORIE_GROB     0.225616
Name: 1, Length: 189, dtype: float64</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Map weights for the third principal component to corresponding feature names</span></span><br><span class="line"><span class="comment"># and then print the linked values, sorted by weight.</span></span><br><span class="line">feature_select[<span class="number">2</span>].sort_values()</span><br></pre></td></tr></table></figure>




<pre><code>ANREDE_KZ     -0.346752
SEMIO_KAEM    -0.319185
SEMIO_DOM     -0.286983
SEMIO_KRIT    -0.262582
SEMIO_ERL     -0.199994
                 ...   
FINANZTYP_5    0.135987
SEMIO_KULT     0.246536
SEMIO_SOZ      0.257146
SEMIO_FAM      0.260536
SEMIO_VERT     0.324047
Name: 2, Length: 189, dtype: float64</code></pre><h3 id="Discussion-2-3-Interpret-Principal-Components"><a href="#Discussion-2-3-Interpret-Principal-Components" class="headerlink" title="Discussion 2.3: Interpret Principal Components"></a>Discussion 2.3: Interpret Principal Components</h3><p>Component 1: Movement patterns and social status are negatively correlated, which means higher movement higher income. Also wealth and social status are positively correlated. Therefore, the first component is correalted with peopleâ€™s wealth.</p>
<p>Component 2: Estimated age based on given name analysis is negatively correlated with variable decade, since they are negatively correlated by nature. Thus this one is correlated with ages.</p>
<p>Component 3: People who is not likely to be dreamful is more likely to be a male. This Component is related to the gender.</p>
<h2 id="Step-3-Clustering"><a href="#Step-3-Clustering" class="headerlink" title="Step 3: Clustering"></a>Step 3: Clustering</h2><h3 id="Step-3-1-Apply-Clustering-to-General-Population"><a href="#Step-3-1-Apply-Clustering-to-General-Population" class="headerlink" title="Step 3.1: Apply Clustering to General Population"></a>Step 3.1: Apply Clustering to General Population</h3><p>Youâ€™ve assessed and cleaned the demographics data, then scaled and transformed them. Now, itâ€™s time to see how the data clusters in the principal components space. In this substep, you will apply k-means clustering to the dataset and use the average within-cluster distances from each point to their assigned clusterâ€™s centroid to decide on a number of clusters to keep.</p>
<ul>
<li>Use sklearnâ€™s <a href="http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans" target="_blank" rel="noopener">KMeans</a> class to perform k-means clustering on the PCA-transformed data.</li>
<li>Then, compute the average difference from each point to its assigned clusterâ€™s center. <strong>Hint</strong>: The KMeans objectâ€™s <code>.score()</code> method might be useful here, but note that in sklearn, scores tend to be defined so that larger is better. Try applying it to a small, toy dataset, or use an internet search to help your understanding.</li>
<li>Perform the above two steps for a number of different cluster counts. You can then see how the average distance decreases with an increasing number of clusters. However, each additional cluster provides a smaller net benefit. Use this fact to select a final number of clusters in which to group the data. <strong>Warning</strong>: because of the large size of the dataset, it can take a long time for the algorithm to resolve. The more clusters to fit, the longer the algorithm will take. You should test for cluster counts through at least 10 clusters to get the full picture, but you shouldnâ€™t need to test for a number of clusters above about 30.</li>
<li>Once youâ€™ve selected a final number of clusters to use, re-fit a KMeans instance to perform the clustering operation. Make sure that you also obtain the cluster assignments for the general demographics data, since youâ€™ll be using them in the final Step 3.3.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Function to calculate K-Means score for a centroid</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_kmeans_score</span><span class="params">(data, center)</span>:</span></span><br><span class="line">    </span><br><span class="line">    kmeans = KMeans(n_clusters = center, random_state=<span class="number">2020</span>)</span><br><span class="line">    model = kmeans.fit(data)</span><br><span class="line">    score = np.abs(model.score(data))</span><br><span class="line">    <span class="keyword">return</span> score</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">score = []</span><br><span class="line">centroids = [<span class="number">2</span>,<span class="number">5</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>,<span class="number">13</span>,<span class="number">14</span>,<span class="number">16</span>,<span class="number">20</span>,<span class="number">23</span>,<span class="number">25</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> each <span class="keyword">in</span> centroids:</span><br><span class="line">    score.append(get_kmeans_score(azdias_pca,each))</span><br><span class="line">    print(<span class="string">'Finished for:'</span>,each)</span><br></pre></td></tr></table></figure>

<pre><code>Finished for: 2
Finished for: 5
Finished for: 10
Finished for: 11
Finished for: 12
Finished for: 13
Finished for: 14
Finished for: 16
Finished for: 20
Finished for: 23
Finished for: 25</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Investigate the change in within-cluster distance across number of clusters.</span></span><br><span class="line"><span class="comment"># HINT: Use matplotlib's plot function to visualize this relationship.</span></span><br><span class="line"></span><br><span class="line">plt.plot(centroids,score,color=<span class="string">'blue'</span>)</span><br></pre></td></tr></table></figure>




<pre><code>[&lt;matplotlib.lines.Line2D at 0x1f0a955a4c8&gt;]</code></pre><p><img src="/2020/02/05/Identify_Customer_Segments/output_68_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Re-fit the k-means model with the selected number of clusters and obtain</span></span><br><span class="line"><span class="comment"># cluster predictions for the general population demographics data.</span></span><br><span class="line">kmeans = KMeans(n_clusters = <span class="number">11</span>, random_state=<span class="number">2020</span>)</span><br><span class="line">result = kmeans.fit_predict(azdias_pca)</span><br></pre></td></tr></table></figure>

<h3 id="Discussion-3-1-Apply-Clustering-to-General-Population"><a href="#Discussion-3-1-Apply-Clustering-to-General-Population" class="headerlink" title="Discussion 3.1: Apply Clustering to General Population"></a>Discussion 3.1: Apply Clustering to General Population</h3><p>The first elbow occurs at â€˜n_clusters = 5â€™, the second elbow occurs at 11, therefore I choose 11 centroids. </p>
<h3 id="Step-3-2-Apply-All-Steps-to-the-Customer-Data"><a href="#Step-3-2-Apply-All-Steps-to-the-Customer-Data" class="headerlink" title="Step 3.2: Apply All Steps to the Customer Data"></a>Step 3.2: Apply All Steps to the Customer Data</h3><p>Now that you have clusters and cluster centers for the general population, itâ€™s time to see how the customer data maps on to those clusters. Take care to not confuse this for re-fitting all of the models to the customer data. Instead, youâ€™re going to use the fits from the general population to clean, transform, and cluster the customer data. In the last step of the project, you will interpret how the general population fits apply to the customer data.</p>
<ul>
<li>Donâ€™t forget when loading in the customers data, that it is semicolon (<code>;</code>) delimited.</li>
<li>Apply the same feature wrangling, selection, and engineering steps to the customer demographics using the <code>clean_data()</code> function you created earlier. (You can assume that the customer demographics data has similar meaning behind missing data patterns as the general demographics data.)</li>
<li>Use the sklearn objects from the general demographics data, and apply their transformations to the customers data. That is, you should not be using a <code>.fit()</code> or <code>.fit_transform()</code> method to re-fit the old objects, nor should you be creating new sklearn objects! Carry the data through the feature scaling, PCA, and clustering steps, obtaining cluster assignments for all of the data in the customer demographics data.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Load in the customer demographics data.</span></span><br><span class="line">customers = pd.read_csv(<span class="string">'Udacity_CUSTOMERS_Subset.csv'</span>,sep=<span class="string">';'</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Apply preprocessing, feature transformation, and clustering from the general</span></span><br><span class="line"><span class="comment"># demographics onto the customer data, obtaining cluster predictions for the</span></span><br><span class="line"><span class="comment"># customer demographics data.</span></span><br><span class="line">customers.shape</span><br></pre></td></tr></table></figure>




<pre><code>(191652, 85)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">customers_clean = clean_data(customers)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">customers_clean.shape</span><br></pre></td></tr></table></figure>




<pre><code>(115643, 188)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># feature transformation</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">customers_clean_trans = pd.DataFrame(scaler.fit_transform(customers_clean))</span><br><span class="line">customers_clean_trans.columns = customers_clean.columns</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pca</span></span><br><span class="line">pca = PCA(n_components=<span class="number">100</span>)</span><br><span class="line">customers_pca = pca.fit_transform(customers_clean_trans)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cluster predictions for the customer data.</span></span><br><span class="line">kmeans = KMeans(n_clusters = <span class="number">11</span>, random_state=<span class="number">2020</span>)</span><br><span class="line">result_customer = kmeans.fit_predict(customers_pca)</span><br></pre></td></tr></table></figure>

<h3 id="Step-3-3-Compare-Customer-Data-to-Demographics-Data"><a href="#Step-3-3-Compare-Customer-Data-to-Demographics-Data" class="headerlink" title="Step 3.3: Compare Customer Data to Demographics Data"></a>Step 3.3: Compare Customer Data to Demographics Data</h3><p>At this point, you have clustered data based on demographics of the general population of Germany, and seen how the customer data for a mail-order sales company maps onto those demographic clusters. In this final substep, you will compare the two cluster distributions to see where the strongest customer base for the company is.</p>
<p>Consider the proportion of persons in each cluster for the general population, and the proportions for the customers. If we think the companyâ€™s customer base to be universal, then the cluster assignment proportions should be fairly similar between the two. If there are only particular segments of the population that are interested in the companyâ€™s products, then we should see a mismatch from one to the other. If there is a higher proportion of persons in a cluster for the customer data compared to the general population (e.g. 5% of persons are assigned to a cluster for the general population, but 15% of the customer data is closest to that clusterâ€™s centroid) then that suggests the people in that cluster to be a target audience for the company. On the other hand, the proportion of the data in a cluster being larger in the general population than the customer data (e.g. only 2% of customers closest to a population centroid that captures 6% of the data) suggests that group of persons to be outside of the target demographics.</p>
<p>Take a look at the following points in this step:</p>
<ul>
<li>Compute the proportion of data points in each cluster for the general population and the customer data. Visualizations will be useful here: both for the individual dataset proportions, but also to visualize the ratios in cluster representation between groups. Seabornâ€™s <a href="https://seaborn.pydata.org/generated/seaborn.countplot.html" target="_blank" rel="noopener"><code>countplot()</code></a> or <a href="https://seaborn.pydata.org/generated/seaborn.barplot.html" target="_blank" rel="noopener"><code>barplot()</code></a> function could be handy.<ul>
<li>Recall the analysis you performed in step 1.1.3 of the project, where you separated out certain data points from the dataset if they had more than a specified threshold of missing values. If you found that this group was qualitatively different from the main bulk of the data, you should treat this as an additional data cluster in this analysis. Make sure that you account for the number of data points in this subset, for both the general population and customer datasets, when making your computations!</li>
</ul>
</li>
<li>Which cluster or clusters are overrepresented in the customer dataset compared to the general population? Select at least one such cluster and infer what kind of people might be represented by that cluster. Use the principal component interpretations from step 2.3 or look at additional components to help you make this inference. Alternatively, you can use the <code>.inverse_transform()</code> method of the PCA and StandardScaler objects to transform centroids back to the original data space and interpret the retrieved values directly.</li>
<li>Perform a similar investigation for the underrepresented clusters. Which cluster or clusters are underrepresented in the customer dataset compared to the general population, and what kinds of people are typified by these clusters?</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Compare the proportion of data in each cluster for the customer data to the</span></span><br><span class="line"><span class="comment"># proportion of data in each cluster for the general population.</span></span><br><span class="line">result_customer = pd.Series(result_customer)</span><br><span class="line">customer_dist = result_customer.value_counts().sort_index()</span><br><span class="line">result = pd.Series(result)</span><br><span class="line">population_dist = result.value_counts().sort_index()</span><br><span class="line">final_df = pd.DataFrame([population_dist,customer_dist]).T</span><br><span class="line">final_df.columns = [<span class="string">'population_count'</span>,<span class="string">'customer_count'</span>]</span><br><span class="line">final_df</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>population_count</th>
      <th>customer_count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>90763</td>
      <td>4965</td>
    </tr>
    <tr>
      <td>1</td>
      <td>22018</td>
      <td>7866</td>
    </tr>
    <tr>
      <td>2</td>
      <td>42219</td>
      <td>19958</td>
    </tr>
    <tr>
      <td>3</td>
      <td>89706</td>
      <td>12487</td>
    </tr>
    <tr>
      <td>4</td>
      <td>87209</td>
      <td>3989</td>
    </tr>
    <tr>
      <td>5</td>
      <td>53588</td>
      <td>8490</td>
    </tr>
    <tr>
      <td>6</td>
      <td>54843</td>
      <td>13308</td>
    </tr>
    <tr>
      <td>7</td>
      <td>77417</td>
      <td>14154</td>
    </tr>
    <tr>
      <td>8</td>
      <td>69022</td>
      <td>2203</td>
    </tr>
    <tr>
      <td>9</td>
      <td>28866</td>
      <td>10496</td>
    </tr>
    <tr>
      <td>10</td>
      <td>7558</td>
      <td>17727</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fig = plt.figure(figsize=(<span class="number">12</span>,<span class="number">4</span>))</span><br><span class="line">ax1 = fig.add_subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">sns.barplot(x=final_df.index,y=<span class="string">'population_count'</span>, data = final_df)</span><br><span class="line">plt.title(<span class="string">"Distribution of General Population"</span>)</span><br><span class="line">ax2 = fig.add_subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">sns.barplot(x=final_df.index,y=<span class="string">'customer_count'</span>, data = final_df)</span><br><span class="line">plt.title(<span class="string">"Distribution of Customer data"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/02/05/Identify_Customer_Segments/output_81_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">temp_po = final_df[<span class="string">'population_count'</span>]/final_df[<span class="string">'population_count'</span>].sum()</span><br><span class="line">temp2_cu = final_df[<span class="string">'customer_count'</span>]/final_df[<span class="string">'customer_count'</span>].sum()</span><br><span class="line">final_df_percent = pd.DataFrame([temp_po,temp2_cu]).T</span><br><span class="line">final_df_percent.columns = [<span class="string">'population_percent'</span>,<span class="string">'customer_percent'</span>]</span><br><span class="line">diff = final_df_percent[<span class="string">'population_percent'</span>]-final_df_percent[<span class="string">'customer_percent'</span>]</span><br><span class="line">diff.sort_values()</span><br></pre></td></tr></table></figure>




<pre><code>10   -0.141163
2    -0.104838
9    -0.044444
1    -0.032690
6    -0.027077
7     0.001829
5     0.012572
3     0.035963
8     0.091703
0     0.102704
4     0.105441
dtype: float64</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># What kinds of people are part of a cluster that is overrepresented in the</span></span><br><span class="line"><span class="comment"># customer data compared to the general population? Category 10</span></span><br><span class="line">cc10 = kmeans.cluster_centers_[<span class="number">10</span>]</span><br><span class="line">cc10 = pd.Series(cc10)</span><br><span class="line">cc10.sort_values(ascending=<span class="literal">False</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">cc10.head()</span><br></pre></td></tr></table></figure>




<pre><code>2     1.776055
4     1.090018
3     0.780223
10    0.501790
6     0.247969
dtype: float64</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">feature_select[<span class="number">2</span>].sort_values()</span><br></pre></td></tr></table></figure>




<pre><code>ANREDE_KZ     -0.346752
SEMIO_KAEM    -0.319185
SEMIO_DOM     -0.286983
SEMIO_KRIT    -0.262582
SEMIO_ERL     -0.199994
                 ...   
FINANZTYP_5    0.135987
SEMIO_KULT     0.246536
SEMIO_SOZ      0.257146
SEMIO_FAM      0.260536
SEMIO_VERT     0.324047
Name: 2, Length: 189, dtype: float64</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">feature_select[<span class="number">4</span>].sort_values()</span><br></pre></td></tr></table></figure>




<pre><code>LP_FAMILIE_FEIN_1.0    -0.327135
LP_FAMILIE_GROB_1.0    -0.327135
W_KEIT_KIND_HH         -0.220503
GREEN_AVANTGARDE       -0.140996
LP_STATUS_GROB_2.0     -0.136988
                          ...   
REGIOTYP                0.155014
LP_FAMILIE_FEIN_10.0    0.192443
KKK                     0.204874
LP_FAMILIE_GROB_5.0     0.253148
ANZ_PERSONEN            0.285127
Name: 4, Length: 189, dtype: float64</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># What kinds of people are part of a cluster that is underrepresented in the</span></span><br><span class="line"><span class="comment"># customer data compared to the general population? Category 4</span></span><br><span class="line">cc4 = kmeans.cluster_centers_[<span class="number">4</span>]</span><br><span class="line">cc4 = pd.Series(cc4)</span><br><span class="line">cc4.sort_values(ascending=<span class="literal">False</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">cc4.head()</span><br></pre></td></tr></table></figure>




<pre><code>1     6.917713
6     3.135301
3     1.403937
8     1.179948
22    1.159860
dtype: float64</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">feature_select[<span class="number">1</span>].sort_values()</span><br></pre></td></tr></table></figure>




<pre><code>decade                  -0.228497
SEMIO_REL               -0.213832
FINANZ_SPARER           -0.212535
FINANZ_UNAUFFAELLIGER   -0.207468
SEMIO_PFLICHT           -0.205000
                           ...   
RETOURTYP_BK_S           0.156204
SEMIO_ERL                0.181138
ZABEOTYP_3               0.197800
FINANZ_VORSORGER         0.205081
ALTERSKATEGORIE_GROB     0.225616
Name: 1, Length: 189, dtype: float64</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">feature_select[<span class="number">6</span>].sort_values()</span><br></pre></td></tr></table></figure>




<pre><code>GEBAEUDETYP_1.0      -0.236870
GEBAEUDETYP_RASTER   -0.209850
LP_STATUS_GROB_2.0   -0.195907
CAMEO_DEUG_2015_6    -0.169896
KBA05_ANTG3          -0.151434
                        ...   
KBA05_ANTG4           0.139343
LP_STATUS_GROB_4.0    0.145008
LP_STATUS_FEIN_6.0    0.264746
GEBAEUDETYP_3.0       0.290512
LP_STATUS_GROB_3.0    0.295362
Name: 6, Length: 189, dtype: float64</code></pre><h3 id="Discussion-3-3-Compare-Customer-Data-to-Demographics-Data"><a href="#Discussion-3-3-Compare-Customer-Data-to-Demographics-Data" class="headerlink" title="Discussion 3.3: Compare Customer Data to Demographics Data"></a>Discussion 3.3: Compare Customer Data to Demographics Data</h3><p>Overrepresented: The category 10, which represents females who are dreamful and might be single parent with child of full age.<br>Underrepresented: The category 4, which represents elderly independent workers with age greater than 60, financial typology is â€˜be preparedâ€™.</p>
<blockquote>
<p>Congratulations on making it this far in the project! Before you finish, make sure to check through the entire notebook from top to bottom to make sure that your analysis follows a logical flow and all of your findings are documented in <strong>Discussion</strong> cells. Once youâ€™ve checked over all of your work, you should export the notebook as an HTML document to submit for evaluation. You can do this from the menu, navigating to <strong>File -&gt; Download as -&gt; HTML (.html)</strong>. You will submit both that document and this notebook for your project submission.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
</font>]]></content>
      <categories>
        <category>Pythonå­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
        <tag>æ— ç›‘ç£å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo å¦‚ä½•ä¼˜é›…çš„åŠ å…¥Rmarkdownç”Ÿæˆçš„html/pdfæ–‡ä»¶</title>
    <url>/2020/01/25/Hexo%20%E5%A6%82%E4%BD%95%E4%BC%98%E9%9B%85%E7%9A%84%E5%8A%A0%E5%85%A5Rmarkdown%E7%94%9F%E6%88%90%E7%9A%84html%E6%96%87%E4%BB%B6/</url>
    <content><![CDATA[<font size="3">

<p>å¦‚ä½•åŠ å…¥è‡ªå·±åˆ›å»ºçš„htmlæ–‡ä»¶,ä»¥åŠå¦‚ä½•å¿½ç•¥åœ¨_sourceæ–‡ä»¶å¤¹ä¸­çš„.md æˆ–è€… .htmlæ–‡ä»¶ã€‚</p>
<a id="more"></a>
<hr>
<p>åœ¨åŠ å…¥Rmarkdownç”Ÿæˆçš„htmlçš„æ—¶å€™æˆ‘å‘çŽ°è‡ªèº«çš„æŽ’ç‰ˆéžå¸¸éš¾çœ‹,ä¹Ÿä¸ä¼šè‡ªåŠ¨åœ¨åšå®¢ç”Ÿæˆæ ‡ç­¾å’Œåˆ†ç±»,é‚£ä¹ˆæœ‰ä¸€ä¸ªè§£å†³åŠžæ³•å°±æ˜¯æ’å…¥htmlæ–‡ä»¶åœ¨åšå®¢é‡Œã€‚æ’å…¥çš„æ–¹æ³•æ˜¯åœ¨markdownä¸­ä½¿ç”¨iframe:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;iframe src&#x3D;&quot;filename.html&quot; width&#x3D;&quot;700&quot; height&#x3D;&quot;800&quot;&gt;&lt;&#x2F;iframe&gt;</span><br></pre></td></tr></table></figure>

<p>åŒæ ·å’Œæ’å…¥å›¾ç‰‡æ–¹å¼å¤§åŒå°å¼‚æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå’Œmdæ–‡ä»¶ä¸€æ¨¡ä¸€æ ·åå­—çš„æ–‡ä»¶å¤¹,æŠŠè¦æ’å…¥çš„htmlæ–‡ä»¶æ”¾å…¥å…¶ä¸­å°±è¡Œ,ä¸éœ€è¦ç»å¯¹è·¯å¾„ã€‚</p>
<p>ä½†æ˜¯è¿™æ—¶å€™å°±æœ‰ä¸€ä¸ªé—®é¢˜,hexoä¼šè‡ªåŠ¨æ‰«ææ‰€æœ‰_sourceæ–‡ä»¶å¤¹ä¸­çš„æ–‡ä»¶,é‚£ä¹ˆhtmlæ–‡ä»¶å°±ä¼šè¢«æ‰«æåˆ°ä»Žè€Œè¢«æ¸²æŸ“åŽä¸Šä¼ åˆ°åšå®¢é‡Œ,æˆ‘ä»¬ä¸å¸Œæœ›æœ‰é‡å¤çš„å†…å®¹,é‚£ä¹ˆåº”è¯¥å¦‚ä½•è®©hexoè·³è¿‡æ‰«æå‘¢?</p>
<p>ç½‘ä¸Šæœ‰å‡ ç§æ–¹æ³•,ä¸€ç§æ˜¯åœ¨htmlæ–‡ä»¶çš„å¼€å¤´åŠ ä¸Š</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---  </span><br><span class="line">layout: false </span><br><span class="line">---</span><br></pre></td></tr></table></figure>
<p>ä½†æ˜¯æµ‹è¯•åŽå‘çŽ°å¹¶æ²¡æœ‰ä»€ä¹ˆç”¨ã€‚äºŽæ˜¯æµ‹è¯•ç¬¬äºŒç§æ–¹æ³•,åœ¨ç«™ç‚¹é…ç½®æ–‡ä»¶ä¸‹:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">skip_render: &quot;filename.html&quot;</span><br></pre></td></tr></table></figure>
<p>åŒæ ·æµ‹è¯•å‘çŽ°è¡Œä¸é€š,æŸ¥é˜…å®˜æ–¹æ–‡æ¡£åŽå‘çŽ°å¯èƒ½å› ä¸ºç‰ˆæœ¬é—®é¢˜è·¯å¾„ä¸å¯¹,åº”è¯¥ä¸º:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">skip_render: &quot;_posts&#x2F;æ–‡ä»¶å¤¹å&#x2F;æ–‡ä»¶å.html&quot;&quot;</span><br></pre></td></tr></table></figure>
<p>ä»Ž_postså¼€å§‹,å¦‚æ­¤ä¸€æ¥å°±å¯ä»¥å¿½ç•¥è¿™ä¸ªæ–‡ä»¶æ¸²æŸ“äº†,å½“ç„¶è¿˜å¯ä»¥å¿½ç•¥å…¶ä»–æ–‡ä»¶,ä¹Ÿå¯ä»¥åˆ©ç”¨æ­£åˆ™è¡¨è¾¾å¼æ¥å¿½ç•¥æŸç±»å‘½åçš„æ–‡ä»¶,ç›®å‰è¿˜ç”¨ä¸åˆ°ã€‚</p>
<p>å¸Œæœ›å¤§å®¶å°‘èµ°å¼¯è·¯!</p>
<p>æœ€åŽé™„ä¸Šä¸€ä¸ªæ’å…¥pdfçš„æ–¹æ³•:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;% pdf .&#x2F;filename.pdf %&#125;</span><br></pre></td></tr></table></figure>
<p>åŒæ ·æŠŠæ–‡ä»¶æ”¾åœ¨mdåŒåæ–‡ä»¶å¤¹ä¸‹å°±è¡Œäº†ã€‚</p>
<hr>
<p>ç›®å½•: <a href="https://www.guojingde.cn/2020/01/19/%E7%94%A8GitHub%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/" target="_blank" rel="noopener">ç”¨GitHubæ­å»ºä¸ªäººåšå®¢(ç›®å½•)</a></p>
</font>]]></content>
      <categories>
        <category>åšå®¢æ­å»º</category>
      </categories>
      <tags>
        <tag>åšå®¢æ­å»º</tag>
      </tags>
  </entry>
  <entry>
    <title>é“¶è¡Œå®¢æˆ·ç”»åƒåˆ†æž2-æœºå™¨å­¦ä¹  with Python</title>
    <url>/2020/01/25/bank_user_profile2/</url>
    <content><![CDATA[<p>åˆ†æžé“¶è¡Œå®¢æˆ·ç”»åƒå¹¶é¢„æµ‹æ˜¯å¦ä¼šåœ¨è¯¥é“¶è¡Œå­˜æ¬¾,æœ¬ç¯‡åŸºäºŽä¸Šä¸€ç¯‡,ä½¿ç”¨Pythonè¿›è¡Œæ¨¡åž‹å»ºç«‹,æ¨¡åž‹é€‰æ‹©,å‚æ•°ä¼˜åŒ–,ç‰¹å¾é€‰æ‹©ç­‰ã€‚</p>
<a id="more"></a>
<hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Import libraries necessary for this project</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> display <span class="comment"># Allows the use of display() for DataFrames</span></span><br><span class="line"><span class="comment"># Import train_test_split</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = pd.read_csv(<span class="string">"processed_data1.csv"</span>)</span><br><span class="line">target = data[<span class="string">'target'</span>]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = data.drop(<span class="string">'Unnamed: 0'</span>, axis = <span class="number">1</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">features = data.drop(<span class="string">'target'</span>, axis = <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<h3 id="åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†"><a href="#åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†" class="headerlink" title="åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†"></a>åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Split the 'features' and 'income' data into training and testing sets</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(features, </span><br><span class="line">                                                    target, </span><br><span class="line">                                                    test_size = <span class="number">0.2</span>, </span><br><span class="line">                                                    random_state = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show the results of the split</span></span><br><span class="line">print(<span class="string">"Training set has &#123;&#125; samples."</span>.format(X_train.shape[<span class="number">0</span>]))</span><br><span class="line">print(<span class="string">"Testing set has &#123;&#125; samples."</span>.format(X_test.shape[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>Training set has 32950 samples.
Testing set has 8238 samples.</code></pre><h3 id="å»ºç«‹æ¨¡åž‹"><a href="#å»ºç«‹æ¨¡åž‹" class="headerlink" title="å»ºç«‹æ¨¡åž‹"></a>å»ºç«‹æ¨¡åž‹</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> fbeta_score</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_predict</span><span class="params">(learner, sample_size, X_train, y_train, X_test, y_test)</span>:</span> </span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    inputs:</span></span><br><span class="line"><span class="string">       - learner: the learning algorithm to be trained and predicted on</span></span><br><span class="line"><span class="string">       - sample_size: the size of samples (number) to be drawn from training set</span></span><br><span class="line"><span class="string">       - X_train: features training set</span></span><br><span class="line"><span class="string">       - y_train: income training set</span></span><br><span class="line"><span class="string">       - X_test: features testing set</span></span><br><span class="line"><span class="string">       - y_test: income testing set</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    results = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Fit the learner to the training data using slicing with 'sample_size' using .fit(training_features[:], training_labels[:])</span></span><br><span class="line">    start = time() <span class="comment"># Get start time</span></span><br><span class="line">    learner = learner.fit(X_train[:sample_size:], y_train[:sample_size].values.ravel())</span><br><span class="line">    end = time() <span class="comment"># Get end time</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Calculate the training time</span></span><br><span class="line">    results[<span class="string">'train_time'</span>] = end - start</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Get the predictions on the test set(X_test),</span></span><br><span class="line">    <span class="comment">#       then get predictions on the first 300 training samples(X_train) using .predict()</span></span><br><span class="line">    start = time() <span class="comment"># Get start time</span></span><br><span class="line">    predictions_test = learner.predict(X_test)</span><br><span class="line">    predictions_train = learner.predict(X_train[:<span class="number">300</span>])</span><br><span class="line">    end = time() <span class="comment"># Get end time</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Calculate the total prediction time</span></span><br><span class="line">    results[<span class="string">'pred_time'</span>] = end - start</span><br><span class="line">            </span><br><span class="line">    <span class="comment"># Compute accuracy on the first 300 training samples which is y_train[:300]</span></span><br><span class="line">    results[<span class="string">'acc_train'</span>] = accuracy_score(y_train[:<span class="number">300</span>], predictions_train)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Compute accuracy on test set using accuracy_score()</span></span><br><span class="line">    results[<span class="string">'acc_test'</span>] = accuracy_score(y_test, predictions_test)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute F-score on the the first 300 training samples using fbeta_score()</span></span><br><span class="line">    results[<span class="string">'f_train'</span>] = fbeta_score(y_train[:<span class="number">300</span>], predictions_train, beta = <span class="number">0.5</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Compute F-score on the test set which is y_test</span></span><br><span class="line">    results[<span class="string">'f_test'</span>] = fbeta_score(y_test, predictions_test, beta = <span class="number">0.5</span>)</span><br><span class="line">       </span><br><span class="line">    <span class="comment"># Success</span></span><br><span class="line">    print(<span class="string">"&#123;&#125; trained on &#123;&#125; samples."</span>.format(learner.__class__.__name__, sample_size))</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Return the results</span></span><br><span class="line">    <span class="keyword">return</span> results</span><br></pre></td></tr></table></figure>

<h3 id="è®­ç»ƒæ¨¡åž‹"><a href="#è®­ç»ƒæ¨¡åž‹" class="headerlink" title="è®­ç»ƒæ¨¡åž‹"></a>è®­ç»ƒæ¨¡åž‹</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Import the three supervised learning models from sklearn</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier </span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line">random.seed(<span class="number">2020</span>)</span><br><span class="line"><span class="comment"># Initialize the three models</span></span><br><span class="line">clf_A = LogisticRegression(solver = <span class="string">'liblinear'</span>) <span class="comment"># set the default value manually in order to get rid of warnings</span></span><br><span class="line">clf_B = RandomForestClassifier(n_estimators = <span class="number">100</span>) <span class="comment"># set the default value manually in order to get rid of warnings</span></span><br><span class="line">clf_C = KNeighborsClassifier() <span class="comment"># set the default value manually in order to get rid of warnings</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate the number of samples for 1%, 10%, and 100% of the training data</span></span><br><span class="line">samples_100 = len(y_train)</span><br><span class="line">samples_10 = int(len(y_train)*<span class="number">0.1</span>)</span><br><span class="line">samples_1 = int(len(y_train)*<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Collect results on the learners</span></span><br><span class="line">results = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> clf <span class="keyword">in</span> [clf_A, clf_B, clf_C]:</span><br><span class="line">    clf_name = clf.__class__.__name__</span><br><span class="line">    results[clf_name] = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i, samples <span class="keyword">in</span> enumerate([samples_1, samples_10, samples_100]):</span><br><span class="line">        results[clf_name][i] = \</span><br><span class="line">        train_predict(clf, samples, X_train, y_train, X_test, y_test)</span><br></pre></td></tr></table></figure>

<pre><code>LogisticRegression trained on 329 samples.
LogisticRegression trained on 3295 samples.
LogisticRegression trained on 32950 samples.
RandomForestClassifier trained on 329 samples.
RandomForestClassifier trained on 3295 samples.
RandomForestClassifier trained on 32950 samples.
KNeighborsClassifier trained on 329 samples.
KNeighborsClassifier trained on 3295 samples.
KNeighborsClassifier trained on 32950 samples.</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">results</span><br></pre></td></tr></table></figure>




<pre><code>{&apos;LogisticRegression&apos;: {0: {&apos;train_time&apos;: 0.0019948482513427734,
   &apos;pred_time&apos;: 0.004987239837646484,
   &apos;acc_train&apos;: 0.8866666666666667,
   &apos;acc_test&apos;: 0.8869871327992231,
   &apos;f_train&apos;: 0.6201550387596898,
   &apos;f_test&apos;: 0.39245863793766833},
  1: {&apos;train_time&apos;: 0.01795196533203125,
   &apos;pred_time&apos;: 0.003989219665527344,
   &apos;acc_train&apos;: 0.86,
   &apos;acc_test&apos;: 0.8990046127700898,
   &apos;f_train&apos;: 0.4744525547445255,
   &apos;f_test&apos;: 0.4798698657991053},
  2: {&apos;train_time&apos;: 0.3007650375366211,
   &apos;pred_time&apos;: 0.003989219665527344,
   &apos;acc_train&apos;: 0.8533333333333334,
   &apos;acc_test&apos;: 0.8980335032774945,
   &apos;f_train&apos;: 0.42635658914728686,
   &apos;f_test&apos;: 0.45267489711934156}},
 &apos;RandomForestClassifier&apos;: {0: {&apos;train_time&apos;: 0.0638282299041748,
   &apos;pred_time&apos;: 0.0827479362487793,
   &apos;acc_train&apos;: 1.0,
   &apos;acc_test&apos;: 0.8874726875455208,
   &apos;f_train&apos;: 1.0,
   &apos;f_test&apos;: 0.35665914221218964},
  1: {&apos;train_time&apos;: 0.2373659610748291,
   &apos;pred_time&apos;: 0.10571742057800293,
   &apos;acc_train&apos;: 0.9966666666666667,
   &apos;acc_test&apos;: 0.8958485069191552,
   &apos;f_train&apos;: 0.9954751131221717,
   &apos;f_test&apos;: 0.4736275565123789},
  2: {&apos;train_time&apos;: 2.7166779041290283,
   &apos;pred_time&apos;: 0.16755199432373047,
   &apos;acc_train&apos;: 0.99,
   &apos;acc_test&apos;: 0.896091284292304,
   &apos;f_train&apos;: 0.9859154929577465,
   &apos;f_test&apos;: 0.47669868374244045}},
 &apos;KNeighborsClassifier&apos;: {0: {&apos;train_time&apos;: 0.0019948482513427734,
   &apos;pred_time&apos;: 0.3361320495605469,
   &apos;acc_train&apos;: 0.8833333333333333,
   &apos;acc_test&apos;: 0.888443797038116,
   &apos;f_train&apos;: 0.5982905982905984,
   &apos;f_test&apos;: 0.3638017280582083},
  1: {&apos;train_time&apos;: 0.007975578308105469,
   &apos;pred_time&apos;: 1.4731249809265137,
   &apos;acc_train&apos;: 0.88,
   &apos;acc_test&apos;: 0.8903860160233066,
   &apos;f_train&apos;: 0.5813953488372093,
   &apos;f_test&apos;: 0.430752453653217},
  2: {&apos;train_time&apos;: 0.4777207374572754,
   &apos;pred_time&apos;: 6.75963830947876,
   &apos;acc_train&apos;: 0.8633333333333333,
   &apos;acc_test&apos;: 0.8956057295460063,
   &apos;f_train&apos;: 0.5202312138728323,
   &apos;f_test&apos;: 0.47895997263085877}}}</code></pre><p>æœ€å¥½çš„æ˜¯KNN,åœ¨è®­ç»ƒæ‰€æœ‰è®­ç»ƒé›†åŽ,å‡†ç¡®çŽ‡è¾¾åˆ°89.6%,ä½†æ˜¯$F_{0.5}$ score åªæœ‰0.48,æ„å‘³ç€åœ¨é¢„æµ‹éžå­˜æ¬¾å¯¹è±¡çš„å‡†ç¡®åº¦è¿œé«˜äºŽé¢„æµ‹å­˜æ¬¾å¯¹è±¡çš„å‡†ç¡®åº¦,è¿™ä¸åˆ©äºŽæˆ‘ä»¬æ‰¾å‡ºæ½œåœ¨çš„å­˜æ¬¾å¯¹è±¡,æˆ‘ä»¬ç»§ç»­åˆ©ç”¨ç½‘æ ¼æœç´¢æ¥ä¼˜åŒ–æˆ‘ä»¬çš„å‚æ•°å’Œæ¨¡åž‹ã€‚</p>
<h3 id="å…¶ä»–æ•°æ®é›†"><a href="#å…¶ä»–æ•°æ®é›†" class="headerlink" title="å…¶ä»–æ•°æ®é›†"></a>å…¶ä»–æ•°æ®é›†</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">i = [<span class="string">'2'</span>,<span class="string">'3'</span>,<span class="string">'4'</span>,<span class="string">'5'</span>]</span><br><span class="line">accuracy_all = []</span><br><span class="line">f_all = []</span><br><span class="line"><span class="keyword">for</span> each <span class="keyword">in</span> i:</span><br><span class="line">    data = pd.read_csv(<span class="string">"processed_data"</span>+ each +<span class="string">".csv"</span>)</span><br><span class="line">    target = data[<span class="string">'target'</span>]</span><br><span class="line">    data = data.drop(<span class="string">'Unnamed: 0'</span>, axis = <span class="number">1</span>)</span><br><span class="line">    features = data.drop(<span class="string">'target'</span>, axis = <span class="number">1</span>)</span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(features, </span><br><span class="line">                                                        target, </span><br><span class="line">                                                        test_size = <span class="number">0.2</span>, </span><br><span class="line">                                                        random_state = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    clf_C = KNeighborsClassifier()</span><br><span class="line">    learner = clf_C.fit(X_train, y_train.values.ravel())</span><br><span class="line">    predictions_test = learner.predict(X_test)</span><br><span class="line">    accuracy_all.append(accuracy_score(y_test, predictions_test))</span><br><span class="line">    f_all.append(fbeta_score(y_test, predictions_test, beta = <span class="number">0.5</span>))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(accuracy_all)</span><br><span class="line">print(f_all)</span><br></pre></td></tr></table></figure>

<pre><code>[0.895241563486283, 0.8958485069191552, 0.8969410050983249, 0.8958485069191552]
[0.4761904761904762, 0.481064483111566, 0.48780487804878053, 0.47987616099071206]</code></pre><p>å‡†ç¡®æ€§å’Œ$F_{0.5}$ score åœ¨ä¸åŒæ•°æ®é›†å·®ä¸å¤š,è¯´æ˜Žcartæ¥å¡«è¡¥ç¼ºå¤±å€¼çš„æ–¹æ³•æœ‰ä¸€è‡´æ€§ã€‚</p>
<h3 id="ç½‘æ ¼æœç´¢"><a href="#ç½‘æ ¼æœç´¢" class="headerlink" title="ç½‘æ ¼æœç´¢"></a>ç½‘æ ¼æœç´¢</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> make_scorer</span><br><span class="line">random.seed(<span class="number">42</span>)</span><br><span class="line"><span class="comment"># Initialize the classifier</span></span><br><span class="line">clf = KNeighborsClassifier()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the parameters list you wish to tune, using a dictionary if needed.</span></span><br><span class="line">k_range = list(range(<span class="number">1</span>,<span class="number">10</span>))</span><br><span class="line">parameters = &#123; <span class="string">'algorithm'</span> : [<span class="string">'auto'</span>,<span class="string">'ball_tree'</span>,<span class="string">'kd_tree'</span>,<span class="string">'brute'</span>], <span class="string">'weights'</span> : [<span class="string">'uniform'</span>,<span class="string">'distance'</span>], <span class="string">'n_neighbors'</span> : k_range&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make an fbeta_score scoring object using make_scorer()</span></span><br><span class="line">scorer = make_scorer(fbeta_score, beta = <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()</span></span><br><span class="line">grid_obj = GridSearchCV(clf, parameters, scoring=scorer, cv = <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit the grid search object to the training data and find the optimal parameters using fit()</span></span><br><span class="line">grid_fit = grid_obj.fit(X_train, y_train.values.ravel())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the estimator</span></span><br><span class="line">best_clf = grid_fit.best_estimator_</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make predictions using the unoptimized and model</span></span><br><span class="line">predictions = (clf.fit(X_train, y_train.values.ravel())).predict(X_test)</span><br><span class="line">best_predictions = best_clf.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Report the before-and-afterscores</span></span><br><span class="line">print(<span class="string">"Unoptimized model\n------"</span>)</span><br><span class="line">print(<span class="string">"Accuracy score on testing data: &#123;:.4f&#125;"</span>.format(accuracy_score(y_test, predictions)))</span><br><span class="line">print(<span class="string">"F-score on testing data: &#123;:.4f&#125;"</span>.format(fbeta_score(y_test, predictions, beta = <span class="number">0.5</span>)))</span><br><span class="line">print(<span class="string">"\nOptimized Model\n------"</span>)</span><br><span class="line">print(<span class="string">"Final accuracy score on the testing data: &#123;:.4f&#125;"</span>.format(accuracy_score(y_test, best_predictions)))</span><br><span class="line">print(<span class="string">"Final F-score on the testing data: &#123;:.4f&#125;"</span>.format(fbeta_score(y_test, best_predictions, beta = <span class="number">0.5</span>)))</span><br><span class="line">print(<span class="string">'best params are:'</span>,str(grid_obj.best_params_))</span><br></pre></td></tr></table></figure>

<pre><code>Unoptimized model
------
Accuracy score on testing data: 0.8958
F-score on testing data: 0.4799

Optimized Model
------
Final accuracy score on the testing data: 0.8989
Final F-score on the testing data: 0.4920
best params are: {&apos;algorithm&apos;: &apos;brute&apos;, &apos;n_neighbors&apos;: 9, &apos;weights&apos;: &apos;uniform&apos;}</code></pre><h3 id="Cutoff-value"><a href="#Cutoff-value" class="headerlink" title="Cutoff value"></a>Cutoff value</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = pd.read_csv(<span class="string">"processed_data5.csv"</span>)</span><br><span class="line">target = data[<span class="string">'target'</span>]</span><br><span class="line">data = data.drop(<span class="string">'Unnamed: 0'</span>, axis = <span class="number">1</span>)</span><br><span class="line">features = data.drop(<span class="string">'target'</span>, axis = <span class="number">1</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># accurate and f score in test set with cutoff = 0.5</span></span><br><span class="line">random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(features, </span><br><span class="line">                                                    target, </span><br><span class="line">                                                    test_size = <span class="number">0.1</span>, </span><br><span class="line">                                                    random_state = <span class="number">0</span>)</span><br><span class="line">X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=<span class="number">0.1</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">clf_C = KNeighborsClassifier(algorithm = <span class="string">'brute'</span>,n_neighbors = <span class="number">9</span>, weights = <span class="string">'uniform'</span>)</span><br><span class="line">learner = clf_C.fit(X_train, y_train.values.ravel())</span><br><span class="line">predictions_test = learner.predict(X_test)</span><br><span class="line">print(accuracy_score(y_test, predictions_test))</span><br><span class="line">print(fbeta_score(y_test, predictions_test, beta = <span class="number">0.5</span>))</span><br></pre></td></tr></table></figure>

<pre><code>0.8990046127700898
0.4892086330935252</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> recall_score</span><br><span class="line">confusion_matrix(y_test, predictions_test) <span class="comment"># é»˜è®¤cutoffä¸ºp = 0.5</span></span><br></pre></td></tr></table></figure>




<pre><code>array([[3567,   98],
       [ 318,  136]], dtype=int64)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predictions_test_p = learner.predict_proba(X_test)</span><br><span class="line">predictions_test_p=pd.DataFrame(predictions_test_p)[<span class="number">1</span>]</span><br><span class="line"><span class="comment"># æˆ‘ä»¬çœ‹åˆ°é€‰å–0.3 cutoff çœŸçš„1çš„é¢„æµ‹æ•°æ˜Žæ˜¾ä¸Šå‡</span></span><br><span class="line">predictions_test_pfinal = predictions_test_p&gt;=<span class="number">0.3</span></span><br><span class="line">predictions_test_pfinal=predictions_test_pfinal.astype(int)</span><br><span class="line">confusion_matrix(y_test, predictions_test_pfinal)</span><br></pre></td></tr></table></figure>




<pre><code>array([[3377,  288],
       [ 230,  224]], dtype=int64)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># æ‰¾åˆ°æœ€ä½³cutoff</span></span><br><span class="line">i = [<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.3</span>,<span class="number">0.4</span>,<span class="number">0.5</span>,<span class="number">0.6</span>,<span class="number">0.7</span>,<span class="number">0.8</span>,<span class="number">0.9</span>,<span class="number">1</span>]</span><br><span class="line">accuracy_s = []</span><br><span class="line">f_s = []</span><br><span class="line">precision_s = []</span><br><span class="line">recall_s=[]</span><br><span class="line"><span class="keyword">for</span> each <span class="keyword">in</span> i:</span><br><span class="line">    predictions_test_pfinal = predictions_test_p&gt;=each</span><br><span class="line">    predictions_test_pfinal=predictions_test_pfinal.astype(int)</span><br><span class="line">    f_s.append(fbeta_score(y_test, predictions_test_pfinal, beta = <span class="number">1</span>))</span><br><span class="line">    accuracy_s.append(accuracy_score(y_test, predictions_test_pfinal))</span><br><span class="line">    precision_s.append(precision_score(y_test, predictions_test_pfinal))</span><br><span class="line">    recall_s.append(recall_score(y_test, predictions_test_pfinal))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.plot(i,precision_s, color=<span class="string">'green'</span>, label=<span class="string">'precision'</span>)</span><br><span class="line">plt.plot(i,recall_s, color=<span class="string">'red'</span>, label=<span class="string">'recall'</span>)</span><br><span class="line">plt.plot(i,accuracy_s, color=<span class="string">'blue'</span>, label=<span class="string">'accuracy'</span>)</span><br><span class="line">plt.plot(i,f_s,color=<span class="string">'black'</span>,label=<span class="string">'F1 score'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel(<span class="string">'cutoff value'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'rate'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/01/25/bank_user_profile2/output_24_0.png" alt="png"></p>
<p>æ ¹æ®å›¾è¡¨æˆ‘ä»¬å¯ä»¥çœ‹å‡ºåœ¨ cutoff valueå¤§è‡´åœ¨0.33å·¦å³æˆ‘ä»¬å¤§è‡´å¾—åˆ°ä¸€ä¸ªæ¯”è¾ƒå‡è¡¡çš„é¢„æµ‹ç»“æžœ,å¦‚æžœæˆ‘ä»¬è¦æé«˜precision,ä¹Ÿå°±æ˜¯ä¿è¯ç²¾å‡†çš„æ‰¾åˆ°æ„¿æ„å­˜æ¬¾çš„äºº,é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥é€‰æ‹©è¾ƒé«˜çš„cutoff valueæ¥ä¿æŒç²¾å‡†å®šä½,ä½†åŒæ—¶ä¹Ÿä¼šæ”¾å¼ƒæ›´å¤šçš„æ½œåœ¨çš„å®¢æˆ·(èŠ‚çº¦æˆæœ¬),å¦‚æžœæˆ‘ä»¬è¦æé«˜recall,ä¹Ÿå°±æ˜¯å¸Œæœ›è¦†ç›–æ›´å¤šçš„æ½œåœ¨å®¢æˆ·,é‚£ä¹ˆå°±è¦é€‰æ‹©è¾ƒä½Žçš„cutoff value,è¿™æ ·æˆæœ¬ä¼šä¸Šå‡ã€‚</p>
<p>æœ€åŽæˆ‘ä»¬åœ¨éªŒè¯é›†ä¸­éªŒè¯æˆ‘ä»¬çš„ç»“è®º,æˆ‘ä»¬å…ˆé€‰æ‹©p=0.18</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># validation set</span></span><br><span class="line"></span><br><span class="line">predictions_val_p = learner.predict_proba(X_val)</span><br><span class="line">predictions_val_p=pd.DataFrame(predictions_val_p)[<span class="number">1</span>]</span><br><span class="line">predictions_val_pfinal = predictions_val_p&gt;=<span class="number">0.18</span></span><br><span class="line">predictions_val_pfinal=predictions_val_pfinal.astype(int)</span><br><span class="line">print(confusion_matrix(y_val, predictions_val_pfinal))</span><br><span class="line">print(recall_score(y_val, predictions_val_pfinal))</span><br><span class="line">print(precision_score(y_val, predictions_val_pfinal))</span><br></pre></td></tr></table></figure>

<pre><code>[[2724  542]
 [ 173  268]]
0.6077097505668935
0.3308641975308642</code></pre><p>å¯ä»¥å‘çŽ°åœ¨æ€»å…±441ä½æ½œåœ¨å®¢æˆ·é‡Œæˆ‘ä»¬é¢„æµ‹åˆ°äº†268ä½,è¾¾åˆ°äº†60.7%ã€‚ä½†æ˜¯æˆ‘ä»¬é¢„æµ‹çš„810ä½é¡¾å®¢é‡Œåªæœ‰268ä½æ˜¯æ­£ç¡®çš„,ç²¾å‡†åº¦ä¸º33%ã€‚å½“ç„¶ä¸ä½¿ç”¨æ¨¡åž‹çš„ç²¾å‡†åº¦ä¸º11.9%ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># validation set</span></span><br><span class="line">predictions_val_p = learner.predict_proba(X_val)</span><br><span class="line">predictions_val_p=pd.DataFrame(predictions_val_p)[<span class="number">1</span>]</span><br><span class="line">predictions_val_pfinal = predictions_val_p&gt;=<span class="number">0.8</span></span><br><span class="line">predictions_val_pfinal=predictions_val_pfinal.astype(int)</span><br><span class="line">print(confusion_matrix(y_val, predictions_val_pfinal))</span><br><span class="line">print(recall_score(y_val, predictions_val_pfinal))</span><br><span class="line">print(precision_score(y_val, predictions_val_pfinal))</span><br></pre></td></tr></table></figure>

<pre><code>[[3257    9]
 [ 419   22]]
0.049886621315192746
0.7096774193548387</code></pre><p>å¯ä»¥å‘çŽ°è°ƒé«˜cutoff valueåŽ,æˆ‘ä»¬çš„é¢„æµ‹å˜å¾—æ›´åŠ è°¨æ…Ž,åœ¨é¢„æµ‹çš„31ä½æ½œåœ¨å®¢æˆ·é‡Œæœ‰22ä½æ˜¯çœŸçš„æ½œåœ¨å®¢æˆ·,è¾¾åˆ°äº†71%çš„æ­£ç¡®çŽ‡ã€‚ä½†æ˜¯è¦†ç›–é¢ä¸å¤Ÿå¹¿,æŸå¤±äº†å¤§å¤šæ•°çš„æ½œåœ¨å®¢æˆ·ã€‚</p>
<h3 id="Feature-Selection"><a href="#Feature-Selection" class="headerlink" title="Feature Selection"></a>Feature Selection</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Import a supervised learning model that has 'feature_importances_'</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> RandomizedSearchCV</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the supervised model on the training set using .fit(X_train, y_train)</span></span><br><span class="line">param_dist = &#123;<span class="string">"max_depth"</span>: [<span class="number">3</span>, <span class="literal">None</span>],</span><br><span class="line">              <span class="string">"n_estimators"</span>: list(range(<span class="number">10</span>, <span class="number">200</span>)),</span><br><span class="line">              <span class="string">"max_features"</span>: list(range(<span class="number">1</span>, X_test.shape[<span class="number">1</span>]+<span class="number">1</span>)),</span><br><span class="line">              <span class="string">"min_samples_split"</span>: list(range(<span class="number">2</span>, <span class="number">11</span>)),</span><br><span class="line">              <span class="string">"min_samples_leaf"</span>: list(range(<span class="number">1</span>, <span class="number">11</span>)),</span><br><span class="line">              <span class="string">"bootstrap"</span>: [<span class="literal">True</span>, <span class="literal">False</span>],</span><br><span class="line">              <span class="string">"criterion"</span>: [<span class="string">"gini"</span>, <span class="string">"entropy"</span>]&#125;</span><br><span class="line">model = RandomizedSearchCV(clf_B, param_distributions=param_dist)</span><br><span class="line">model.fit(X_train, y_train.values.ravel())</span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Extract the feature importances using .feature_importances_ </span></span><br><span class="line">importances = model.best_estimator_.feature_importances_</span><br></pre></td></tr></table></figure>

<pre><code>C:\Users\jasonguo\Anaconda3\lib\site-packages\sklearn\model_selection\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.
  warnings.warn(CV_WARNING, FutureWarning)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Plot</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> pl</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feature_plot</span><span class="params">(importances, X_train, y_train)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Display the five most important features</span></span><br><span class="line">    indices = np.argsort(importances)[::<span class="number">-1</span>]</span><br><span class="line">    columns = X_train.columns.values[indices[:<span class="number">5</span>]]</span><br><span class="line">    values = importances[indices][:<span class="number">5</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Creat the plot</span></span><br><span class="line">    fig = pl.figure(figsize = (<span class="number">9</span>,<span class="number">5</span>))</span><br><span class="line">    pl.title(<span class="string">"Normalized Weights for First Five Most Predictive Features"</span>, fontsize = <span class="number">16</span>)</span><br><span class="line">    pl.bar(np.arange(<span class="number">5</span>), values, width = <span class="number">0.6</span>, align=<span class="string">"center"</span>, color = <span class="string">'#00A000'</span>, \</span><br><span class="line">          label = <span class="string">"Feature Weight"</span>)</span><br><span class="line">    pl.bar(np.arange(<span class="number">5</span>) - <span class="number">0.3</span>, np.cumsum(values), width = <span class="number">0.2</span>, align = <span class="string">"center"</span>, color = <span class="string">'#00A0A0'</span>, \</span><br><span class="line">          label = <span class="string">"Cumulative Feature Weight"</span>)</span><br><span class="line">    pl.xticks(np.arange(<span class="number">5</span>), columns)</span><br><span class="line">    pl.xlim((<span class="number">-0.5</span>, <span class="number">4.5</span>))</span><br><span class="line">    pl.ylabel(<span class="string">"Weight"</span>, fontsize = <span class="number">12</span>)</span><br><span class="line">    pl.xlabel(<span class="string">"Feature"</span>, fontsize = <span class="number">12</span>)</span><br><span class="line">    </span><br><span class="line">    pl.legend(loc = <span class="string">'upper center'</span>)</span><br><span class="line">    pl.tight_layout()</span><br><span class="line">    pl.show()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">feature_plot(importances, X_train, y_train)</span><br></pre></td></tr></table></figure>


<p><img src="/2020/01/25/bank_user_profile2/output_34_0.png" alt="png"></p>
<p>å¯ä»¥çœ‹åˆ°å‰äº”ä¸ªé‡è¦çš„æŒ‡æ ‡,ç¬¬ä¸€ä¸ªæ˜¯é›‡ä½£å‘˜å·¥æ•°,ç¬¬äºŒä¸ªæ˜¯æ¬§æ´²é“¶è¡Œé—´åˆ©çŽ‡,ç¬¬ä¸‰ä¸ªæ˜¯å¹´é¾„,ç¬¬å››ä¸ªå’Œç¬¬äº”ä¸ªéƒ½æ˜¯å—è®¿ç›¸å…³å‚æ•°ã€‚ç§ä»¥ä¸º,æœºå™¨å­¦ä¹ å’Œè®¡é‡ç»æµŽå­¦çš„æ–¹æ³•ä¸åŒä¹‹å¤„åœ¨äºŽæœºå™¨å­¦ä¹ æ›´åŠ æ³¨é‡é¢„æµ‹ç»“æžœçš„å‡†ç¡®æ€§,è€Œå¿½è§†äº†é¢„æµ‹ç»“æžœå’Œå‚æ•°ç›´æŽ¥çš„å› æžœå…³ç³»,è¿™äº›å› æžœå…³ç³»æ˜¯å¾ˆéš¾é€šè¿‡æœºå™¨å­¦ä¹ çš„ä¸€äº›æ–¹æ³•è¢«è§£é‡Šçš„,å³ä½¿é¢„æµ‹ç»“æžœéžå¸¸å‡†ç¡®,ä½†æ˜¯ä¸€äº›å‚æ•°å¯èƒ½å’Œé¢„æµ‹ç›®æ ‡æ²¡æœ‰å®žé™…çš„ç›¸å…³æ€§ã€‚æŽ¥ä¸‹æ¥å°±ä½¿ç”¨è®¡é‡ç»æµŽå­¦çš„æ–¹æ³•æ¥ç ”ç©¶å“ªäº›å› ç´ å¯èƒ½å¯¼è‡´ç”¨æˆ·å­˜æ¬¾ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> clone</span><br><span class="line"></span><br><span class="line"><span class="comment"># Reduce the feature space</span></span><br><span class="line">X_train_reduced = X_train[X_train.columns.values[(np.argsort(importances)[::<span class="number">-1</span>])[:<span class="number">5</span>]]]</span><br><span class="line">X_test_reduced = X_test[X_test.columns.values[(np.argsort(importances)[::<span class="number">-1</span>])[:<span class="number">5</span>]]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train on the "best" model found from grid search earlier</span></span><br><span class="line">clf = (clone(best_clf)).fit(X_train_reduced, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make new predictions</span></span><br><span class="line">reduced_predictions = clf.predict(X_test_reduced)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Report scores from the final model using both versions of data</span></span><br><span class="line">print(<span class="string">"Final Model trained on full data\n------"</span>)</span><br><span class="line">print(<span class="string">"Accuracy on testing data: &#123;:.4f&#125;"</span>.format(accuracy_score(y_test, best_predictions)))</span><br><span class="line">print(<span class="string">"F-score on testing data: &#123;:.4f&#125;"</span>.format(fbeta_score(y_test, best_predictions, beta = <span class="number">0.5</span>)))</span><br><span class="line">print(<span class="string">"\nFinal Model trained on reduced data\n------"</span>)</span><br><span class="line">print(<span class="string">"Accuracy on testing data: &#123;:.4f&#125;"</span>.format(accuracy_score(y_test, reduced_predictions)))</span><br><span class="line">print(<span class="string">"F-score on testing data: &#123;:.4f&#125;"</span>.format(fbeta_score(y_test, reduced_predictions, beta = <span class="number">0.5</span>)))</span><br></pre></td></tr></table></figure>

<pre><code>Final Model trained on full data
------
Accuracy on testing data: 0.8989
F-score on testing data: 0.4920

Final Model trained on reduced data
------
Accuracy on testing data: 0.8934
F-score on testing data: 0.4420</code></pre><p>å¯ä»¥å‘çŽ°åªå–æŽ’åå‰äº”ä¸ªå‚æ•°çš„æ¨¡åž‹,F score ä¸‹é™äº†ä¸€äº›, æ„å‘³ç€é¢„æµ‹æœ‰å­˜æ¬¾æ„å‘çš„äººçš„å‡†ç¡®åº¦å˜ä½Žäº†ä¸€äº›,ä½†ä¸æ˜¯å¾ˆå¤šã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Pythonå­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title>é“¶è¡Œå®¢æˆ·ç”»åƒåˆ†æž1-æ•°æ®é¢„å¤„ç† with R</title>
    <url>/2020/01/24/bank_user_profile/</url>
    <content><![CDATA[<p>åˆ†æžé“¶è¡Œå®¢æˆ·ç”»åƒå¹¶é¢„æµ‹æ˜¯å¦ä¼šåœ¨è¯¥é“¶è¡Œå­˜æ¬¾,æœ¬ç¯‡ä½¿ç”¨Rè¯­è¨€è¿›è¡Œæ•°æ®é¢„å¤„ç†,ä½¿ç”¨CARTæ–¹æ³•å¡«è¡¥ç¼ºå¤±å€¼,æ•°æ®æ ‡å‡†åŒ–ä»¥åŠone-hot encodingã€‚</p>
<a id="more"></a>
<hr>
<iframe src="bank.html" width="700" height="800"></iframe>
















]]></content>
      <categories>
        <category>Rå­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title>ç›‘ç£å­¦ä¹ -å¯»æ‰¾æ½œåœ¨çš„ææ¬¾è€…</title>
    <url>/2020/01/22/%E5%AF%BB%E6%89%BE%E6%BD%9C%E5%9C%A8%E7%9A%84%E6%8D%90%E6%AC%BE%E8%80%85/</url>
    <content><![CDATA[<h2 id="Supervised-Learning"><a href="#Supervised-Learning" class="headerlink" title="Supervised Learning"></a>Supervised Learning</h2><h2 id="Project-Finding-Donors-for-CharityML"><a href="#Project-Finding-Donors-for-CharityML" class="headerlink" title="Project: Finding Donors for CharityML"></a>Project: Finding Donors for <em>CharityML</em></h2><p>In this project, you will employ several supervised algorithms of your choice to accurately model individualsâ€™ income using data collected from the 1994 U.S. Census. You will then choose the best candidate algorithm from preliminary results and further optimize this algorithm to best model the data. Your goal with this implementation is to construct a model that accurately predicts whether an individual makes more than $50,000. </p>
<a id="more"></a>
<p>This sort of task can arise in a non-profit setting, where organizations survive on donations.  Understanding an individualâ€™s income can help a non-profit better understand how large of a donation to request, or whether or not they should reach out to begin with.  While it can be difficult to determine an individualâ€™s general income bracket directly from public sources, we can (as we will see) infer this value from other publically available features. </p>
<p>The dataset for this project originates from the <a href="https://archive.ics.uci.edu/ml/datasets/Census+Income" target="_blank" rel="noopener">UCI Machine Learning Repository</a>. The datset was donated by Ron Kohavi and Barry Becker, after being published in the article <em>â€œScaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybridâ€</em>. You can find the article by Ron Kohavi <a href="https://www.aaai.org/Papers/KDD/1996/KDD96-033.pdf" target="_blank" rel="noopener">online</a>. The data we investigate here consists of small changes to the original dataset, such as removing the <code>&#39;fnlwgt&#39;</code> feature and records with missing or ill-formatted entries.</p>
<hr>
<h2 id="Exploring-the-Data"><a href="#Exploring-the-Data" class="headerlink" title="Exploring the Data"></a>Exploring the Data</h2><p>Run the code cell below to load necessary Python libraries and load the census data. Note that the last column from this dataset, <code>&#39;income&#39;</code>, will be our target label (whether an individual makes more than, or at most, $50,000 annually). All other columns are features about each individual in the census database.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Import libraries necessary for this project</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> display <span class="comment"># Allows the use of display() for DataFrames</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Import supplementary visualization code visuals.py</span></span><br><span class="line"><span class="keyword">import</span> visuals <span class="keyword">as</span> vs</span><br><span class="line"></span><br><span class="line"><span class="comment"># Pretty display for notebooks</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the Census dataset</span></span><br><span class="line">data = pd.read_csv(<span class="string">"census.csv"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Success - Display the first record</span></span><br><span class="line">display(data.head(n=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>workclass</th>
      <th>education_level</th>
      <th>education-num</th>
      <th>marital-status</th>
      <th>occupation</th>
      <th>relationship</th>
      <th>race</th>
      <th>sex</th>
      <th>capital-gain</th>
      <th>capital-loss</th>
      <th>hours-per-week</th>
      <th>native-country</th>
      <th>income</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>39</td>
      <td>State-gov</td>
      <td>Bachelors</td>
      <td>13.0</td>
      <td>Never-married</td>
      <td>Adm-clerical</td>
      <td>Not-in-family</td>
      <td>White</td>
      <td>Male</td>
      <td>2174.0</td>
      <td>0.0</td>
      <td>40.0</td>
      <td>United-States</td>
      <td>&lt;=50K</td>
    </tr>
  </tbody>
</table>
</div>


<h3 id="Implementation-Data-Exploration"><a href="#Implementation-Data-Exploration" class="headerlink" title="Implementation: Data Exploration"></a>Implementation: Data Exploration</h3><p>A cursory investigation of the dataset will determine how many individuals fit into either group, and will tell us about the percentage of these individuals making more than $50,000. In the code cell below, you will need to compute the following:</p>
<ul>
<li>The total number of records, <code>&#39;n_records&#39;</code></li>
<li>The number of individuals making more than $50,000 annually, <code>&#39;n_greater_50k&#39;</code>.</li>
<li>The number of individuals making at most $50,000 annually, <code>&#39;n_at_most_50k&#39;</code>.</li>
<li>The percentage of individuals making more than $50,000 annually, <code>&#39;greater_percent&#39;</code>.</li>
</ul>
<p>** HINT: ** You may need to look at the table above to understand how the <code>&#39;income&#39;</code> entries are formatted. </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Total number of records</span></span><br><span class="line">n_records = len(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Number of records where individual's income is more than $50,000</span></span><br><span class="line">n_greater_50k = len(data.income[data.income == <span class="string">'&gt;50K'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Number of records where individual's income is at most $50,000</span></span><br><span class="line">n_at_most_50k = len(data.income[data.income == <span class="string">'&lt;=50K'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Percentage of individuals whose income is more than $50,000</span></span><br><span class="line">greater_percent = n_greater_50k/n_records*<span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the results</span></span><br><span class="line">print(<span class="string">"Total number of records: &#123;&#125;"</span>.format(n_records))</span><br><span class="line">print(<span class="string">"Individuals making more than $50,000: &#123;&#125;"</span>.format(n_greater_50k))</span><br><span class="line">print(<span class="string">"Individuals making at most $50,000: &#123;&#125;"</span>.format(n_at_most_50k))</span><br><span class="line">print(<span class="string">"Percentage of individuals making more than $50,000: &#123;&#125;%"</span>.format(greater_percent))</span><br></pre></td></tr></table></figure>

<pre><code>Total number of records: 45222
Individuals making more than $50,000: 11208
Individuals making at most $50,000: 34014
Percentage of individuals making more than $50,000: 24.78439697492371%</code></pre><p>** Featureset Exploration **</p>
<ul>
<li><strong>age</strong>: continuous. </li>
<li><strong>workclass</strong>: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked. </li>
<li><strong>education</strong>: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool. </li>
<li><strong>education-num</strong>: continuous. </li>
<li><strong>marital-status</strong>: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse. </li>
<li><strong>occupation</strong>: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces. </li>
<li><strong>relationship</strong>: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried. </li>
<li><strong>race</strong>: Black, White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other. </li>
<li><strong>sex</strong>: Female, Male. </li>
<li><strong>capital-gain</strong>: continuous. </li>
<li><strong>capital-loss</strong>: continuous. </li>
<li><strong>hours-per-week</strong>: continuous. </li>
<li><strong>native-country</strong>: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&amp;Tobago, Peru, Hong, Holand-Netherlands.</li>
</ul>
<hr>
<h2 id="Preparing-the-Data"><a href="#Preparing-the-Data" class="headerlink" title="Preparing the Data"></a>Preparing the Data</h2><p>Before data can be used as input for machine learning algorithms, it often must be cleaned, formatted, and restructured â€” this is typically known as <strong>preprocessing</strong>. Fortunately, for this dataset, there are no invalid or missing entries we must deal with, however, there are some qualities about certain features that must be adjusted. This preprocessing can help tremendously with the outcome and predictive power of nearly all learning algorithms.</p>
<h3 id="Transforming-Skewed-Continuous-Features"><a href="#Transforming-Skewed-Continuous-Features" class="headerlink" title="Transforming Skewed Continuous Features"></a>Transforming Skewed Continuous Features</h3><p>A dataset may sometimes contain at least one feature whose values tend to lie near a single number, but will also have a non-trivial number of vastly larger or smaller values than that single number.  Algorithms can be sensitive to such distributions of values and can underperform if the range is not properly normalized. With the census dataset two features fit this description: â€˜<code>capital-gain&#39;</code> and <code>&#39;capital-loss&#39;</code>. </p>
<p>Run the code cell below to plot a histogram of these two features. Note the range of the values present and how they are distributed.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Split the data into features and target label</span></span><br><span class="line">income_raw = data[<span class="string">'income'</span>]</span><br><span class="line">features_raw = data.drop(<span class="string">'income'</span>, axis = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize skewed continuous features of original data</span></span><br><span class="line">vs.distribution(data)</span><br></pre></td></tr></table></figure>


<p><img src="/2020/01/22/%E5%AF%BB%E6%89%BE%E6%BD%9C%E5%9C%A8%E7%9A%84%E6%8D%90%E6%AC%BE%E8%80%85/output_10_0.png" alt="png"></p>
<p>For highly-skewed feature distributions such as <code>&#39;capital-gain&#39;</code> and <code>&#39;capital-loss&#39;</code>, it is common practice to apply a <a href="https://en.wikipedia.org/wiki/Data_transformation_(statistics)" target="_blank" rel="noopener">logarithmic transformation</a> on the data so that the very large and very small values do not negatively affect the performance of a learning algorithm. Using a logarithmic transformation significantly reduces the range of values caused by outliers. Care must be taken when applying this transformation however: The logarithm of <code>0</code> is undefined, so we must translate the values by a small amount above <code>0</code> to apply the the logarithm successfully.</p>
<p>Run the code cell below to perform a transformation on the data and visualize the results. Again, note the range of values and how they are distributed. </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Log-transform the skewed features</span></span><br><span class="line">skewed = [<span class="string">'capital-gain'</span>, <span class="string">'capital-loss'</span>]</span><br><span class="line">features_log_transformed = pd.DataFrame(data = features_raw)</span><br><span class="line">features_log_transformed[skewed] = features_raw[skewed].apply(<span class="keyword">lambda</span> x: np.log(x + <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the new log distributions</span></span><br><span class="line">vs.distribution(features_log_transformed, transformed = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>


<p><img src="/2020/01/22/%E5%AF%BB%E6%89%BE%E6%BD%9C%E5%9C%A8%E7%9A%84%E6%8D%90%E6%AC%BE%E8%80%85/output_12_0.png" alt="png"></p>
<h3 id="Normalizing-Numerical-Features"><a href="#Normalizing-Numerical-Features" class="headerlink" title="Normalizing Numerical Features"></a>Normalizing Numerical Features</h3><p>In addition to performing transformations on features that are highly skewed, it is often good practice to perform some type of scaling on numerical features. Applying a scaling to the data does not change the shape of each featureâ€™s distribution (such as <code>&#39;capital-gain&#39;</code> or <code>&#39;capital-loss&#39;</code> above); however, normalization ensures that each feature is treated equally when applying supervised learners. Note that once scaling is applied, observing the data in its raw form will no longer have the same original meaning, as exampled below.</p>
<p>Run the code cell below to normalize each numerical feature. We will use <a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html" target="_blank" rel="noopener"><code>sklearn.preprocessing.MinMaxScaler</code></a> for this.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Import sklearn.preprocessing.StandardScaler</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize a scaler, then apply it to the features</span></span><br><span class="line">scaler = MinMaxScaler() <span class="comment"># default=(0, 1)</span></span><br><span class="line">numerical = [<span class="string">'age'</span>, <span class="string">'education-num'</span>, <span class="string">'capital-gain'</span>, <span class="string">'capital-loss'</span>, <span class="string">'hours-per-week'</span>]</span><br><span class="line"></span><br><span class="line">features_log_minmax_transform = pd.DataFrame(data = features_log_transformed)</span><br><span class="line">features_log_minmax_transform[numerical] = scaler.fit_transform(features_log_transformed[numerical])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show an example of a record with scaling applied</span></span><br><span class="line">display(features_log_minmax_transform.head(n = <span class="number">5</span>))</span><br></pre></td></tr></table></figure>

<pre><code>/home/jason/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.
  return self.partial_fit(X, y)</code></pre><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>workclass</th>
      <th>education_level</th>
      <th>education-num</th>
      <th>marital-status</th>
      <th>occupation</th>
      <th>relationship</th>
      <th>race</th>
      <th>sex</th>
      <th>capital-gain</th>
      <th>capital-loss</th>
      <th>hours-per-week</th>
      <th>native-country</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.301370</td>
      <td>State-gov</td>
      <td>Bachelors</td>
      <td>0.800000</td>
      <td>Never-married</td>
      <td>Adm-clerical</td>
      <td>Not-in-family</td>
      <td>White</td>
      <td>Male</td>
      <td>0.667492</td>
      <td>0.0</td>
      <td>0.397959</td>
      <td>United-States</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.452055</td>
      <td>Self-emp-not-inc</td>
      <td>Bachelors</td>
      <td>0.800000</td>
      <td>Married-civ-spouse</td>
      <td>Exec-managerial</td>
      <td>Husband</td>
      <td>White</td>
      <td>Male</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.122449</td>
      <td>United-States</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.287671</td>
      <td>Private</td>
      <td>HS-grad</td>
      <td>0.533333</td>
      <td>Divorced</td>
      <td>Handlers-cleaners</td>
      <td>Not-in-family</td>
      <td>White</td>
      <td>Male</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.397959</td>
      <td>United-States</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.493151</td>
      <td>Private</td>
      <td>11th</td>
      <td>0.400000</td>
      <td>Married-civ-spouse</td>
      <td>Handlers-cleaners</td>
      <td>Husband</td>
      <td>Black</td>
      <td>Male</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.397959</td>
      <td>United-States</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.150685</td>
      <td>Private</td>
      <td>Bachelors</td>
      <td>0.800000</td>
      <td>Married-civ-spouse</td>
      <td>Prof-specialty</td>
      <td>Wife</td>
      <td>Black</td>
      <td>Female</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.397959</td>
      <td>Cuba</td>
    </tr>
  </tbody>
</table>
</div>


<h3 id="Implementation-Data-Preprocessing"><a href="#Implementation-Data-Preprocessing" class="headerlink" title="Implementation: Data Preprocessing"></a>Implementation: Data Preprocessing</h3><p>From the table in <strong>Exploring the Data</strong> above, we can see there are several features for each record that are non-numeric. Typically, learning algorithms expect input to be numeric, which requires that non-numeric features (called <em>categorical variables</em>) be converted. One popular way to convert categorical variables is by using the <strong>one-hot encoding</strong> scheme. One-hot encoding creates a <em>â€œdummyâ€</em> variable for each possible category of each non-numeric feature. For example, assume <code>someFeature</code> has three possible entries: <code>A</code>, <code>B</code>, or <code>C</code>. We then encode this feature into <code>someFeature_A</code>, <code>someFeature_B</code> and <code>someFeature_C</code>.</p>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">someFeature</th>
<th></th>
<th align="center">someFeature_A</th>
<th align="center">someFeature_B</th>
<th align="center">someFeature_C</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0</td>
<td align="center">B</td>
<td></td>
<td align="center">0</td>
<td align="center">1</td>
<td align="center">0</td>
</tr>
<tr>
<td align="center">1</td>
<td align="center">C</td>
<td>â€”-&gt; one-hot encode â€”-&gt;</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">1</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">A</td>
<td></td>
<td align="center">1</td>
<td align="center">0</td>
<td align="center">0</td>
</tr>
</tbody></table>
<p>Additionally, as with the non-numeric features, we need to convert the non-numeric target label, <code>&#39;income&#39;</code> to numerical values for the learning algorithm to work. Since there are only two possible categories for this label (â€œ&lt;=50Kâ€ and â€œ&gt;50Kâ€), we can avoid using one-hot encoding and simply encode these two categories as <code>0</code> and <code>1</code>, respectively. In code cell below, you will need to implement the following:</p>
<ul>
<li>Use <a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html?highlight=get_dummies#pandas.get_dummies" target="_blank" rel="noopener"><code>pandas.get_dummies()</code></a> to perform one-hot encoding on the <code>&#39;features_log_minmax_transform&#39;</code> data.</li>
<li>Convert the target label <code>&#39;income_raw&#39;</code> to numerical entries.<ul>
<li>Set records with â€œ&lt;=50Kâ€ to <code>0</code> and records with â€œ&gt;50Kâ€ to <code>1</code>.</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> One-hot encode the 'features_log_minmax_transform' data using pandas.get_dummies()</span></span><br><span class="line">features_final = pd.get_dummies(features_log_minmax_transform)</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Encode the 'income_raw' data to numerical values</span></span><br><span class="line">income = pd.get_dummies(income_raw, drop_first=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the number of features after one-hot encoding</span></span><br><span class="line">encoded = list(features_final.columns)</span><br><span class="line">print(<span class="string">"&#123;&#125; total features after one-hot encoding."</span>.format(len(encoded)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Uncomment the following line to see the encoded feature names</span></span><br><span class="line"><span class="comment"># print (encoded)</span></span><br></pre></td></tr></table></figure>

<pre><code>103 total features after one-hot encoding.</code></pre><h3 id="Shuffle-and-Split-Data"><a href="#Shuffle-and-Split-Data" class="headerlink" title="Shuffle and Split Data"></a>Shuffle and Split Data</h3><p>Now all <em>categorical variables</em> have been converted into numerical features, and all numerical features have been normalized. As always, we will now split the data (both features and their labels) into training and test sets. 80% of the data will be used for training and 20% for testing.</p>
<p>Run the code cell below to perform this split.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Import train_test_split</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split the 'features' and 'income' data into training and testing sets</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(features_final, </span><br><span class="line">                                                    income, </span><br><span class="line">                                                    test_size = <span class="number">0.2</span>, </span><br><span class="line">                                                    random_state = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show the results of the split</span></span><br><span class="line">print(<span class="string">"Training set has &#123;&#125; samples."</span>.format(X_train.shape[<span class="number">0</span>]))</span><br><span class="line">print(<span class="string">"Testing set has &#123;&#125; samples."</span>.format(X_test.shape[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>Training set has 36177 samples.
Testing set has 9045 samples.</code></pre><hr>
<h2 id="Evaluating-Model-Performance"><a href="#Evaluating-Model-Performance" class="headerlink" title="Evaluating Model Performance"></a>Evaluating Model Performance</h2><p>In this section, we will investigate four different algorithms, and determine which is best at modeling the data. Three of these algorithms will be supervised learners of your choice, and the fourth algorithm is known as a <em>naive predictor</em>.</p>
<h3 id="Metrics-and-the-Naive-Predictor"><a href="#Metrics-and-the-Naive-Predictor" class="headerlink" title="Metrics and the Naive Predictor"></a>Metrics and the Naive Predictor</h3><p><em>CharityML</em>, equipped with their research, knows individuals that make more than $50,000 are most likely to donate to their charity. Because of this, <em>CharityML</em> is particularly interested in predicting who makes more than $50,000 accurately. It would seem that using <strong>accuracy</strong> as a metric for evaluating a particular modelâ€™s performace would be appropriate. Additionally, identifying someone that <em>does not</em> make more than $50,000 as someone who does would be detrimental to <em>CharityML</em>, since they are looking to find individuals willing to donate. Therefore, a modelâ€™s ability to precisely predict those that make more than $50,000 is <em>more important</em> than the modelâ€™s ability to <strong>recall</strong> those individuals. We can use <strong>F-beta score</strong> as a metric that considers both precision and recall:</p>
<p>$$ F_{\beta} = (1 + \beta^2) \cdot \frac{precision \cdot recall}{\left( \beta^2 \cdot precision \right) + recall} $$</p>
<p>In particular, when $\beta = 0.5$, more emphasis is placed on precision. This is called the <strong>F$_{0.5}$ score</strong> (or F-score for simplicity).</p>
<p>Looking at the distribution of classes (those who make at most $50,000, and those who make more), itâ€™s clear most individuals do not make more than $50,000. This can greatly affect <strong>accuracy</strong>, since we could simply say *â€this person does not make more than $50,000â€* and generally be right, without ever looking at the data! Making such a statement would be called <strong>naive</strong>, since we have not considered any information to substantiate the claim. It is always important to consider the <em>naive prediction</em> for your data, to help establish a benchmark for whether a model is performing well. That been said, using that prediction would be pointless: If we predicted all people made less than $50,000, <em>CharityML</em> would identify no one as donors. </p>
<h4 id="Note-Recap-of-accuracy-precision-recall"><a href="#Note-Recap-of-accuracy-precision-recall" class="headerlink" title="Note: Recap of accuracy, precision, recall"></a>Note: Recap of accuracy, precision, recall</h4><p>** Accuracy ** measures how often the classifier makes the correct prediction. Itâ€™s the ratio of the number of correct predictions to the total number of predictions (the number of test data points).</p>
<p>** Precision ** tells us what proportion of messages we classified as spam, actually were spam.<br>It is a ratio of true positives(words classified as spam, and which are actually spam) to all positives(all words classified as spam, irrespective of whether that was the correct classificatio), in other words it is the ratio of</p>
<p><code>[True Positives/(True Positives + False Positives)]</code></p>
<p>** Recall(sensitivity)** tells us what proportion of messages that actually were spam were classified by us as spam.<br>It is a ratio of true positives(words classified as spam, and which are actually spam) to all the words that were actually spam, in other words it is the ratio of</p>
<p><code>[True Positives/(True Positives + False Negatives)]</code></p>
<p>For classification problems that are skewed in their classification distributions like in our case, for example if we had a 100 text messages and only 2 were spam and the rest 98 werenâ€™t, accuracy by itself is not a very good metric. We could classify 90 messages as not spam(including the 2 that were spam but we classify them as not spam, hence they would be false negatives) and 10 as spam(all 10 false positives) and still get a reasonably good accuracy score. For such cases, precision and recall come in very handy. These two metrics can be combined to get the F1 score, which is weighted average(harmonic mean) of the precision and recall scores. This score can range from 0 to 1, with 1 being the best possible F1 score(we take the harmonic mean as we are dealing with ratios).</p>
<h3 id="Question-1-Naive-Predictor-Performace"><a href="#Question-1-Naive-Predictor-Performace" class="headerlink" title="Question 1 - Naive Predictor Performace"></a>Question 1 - Naive Predictor Performace</h3><ul>
<li>If we chose a model that always predicted an individual made more than $50,000, what would  that modelâ€™s accuracy and F-score be on this dataset? You must use the code cell below and assign your results to <code>&#39;accuracy&#39;</code> and <code>&#39;fscore&#39;</code> to be used later.</li>
</ul>
<p>** Please note ** that the the purpose of generating a naive predictor is simply to show what a base model without any intelligence would look like. In the real world, ideally your base model would be either the results of a previous model or could be based on a research paper upon which you are looking to improve. When there is no benchmark model set, getting a result better than random choice is a place you could start from.</p>
<p>** HINT: ** </p>
<ul>
<li>When we have a model that always predicts â€˜1â€™ (i.e. the individual makes more than 50k) then our model will have no True Negatives(TN) or False Negatives(FN) as we are not making any negative(â€˜0â€™ value) predictions. Therefore our Accuracy in this case becomes the same as our Precision(True Positives/(True Positives + False Positives)) as every prediction that we have made with value â€˜1â€™ that should have â€˜0â€™ becomes a False Positive; therefore our denominator in this case is the total number of records we have in total. </li>
<li>Our Recall score(True Positives/(True Positives + False Negatives)) in this setting becomes 1 as we have no False Negatives.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">TP = np.sum(income)<span class="comment"># Counting the ones as this is the naive case. Note that 'income' is the 'income_raw' data </span></span><br><span class="line"><span class="comment"># encoded to numerical values done in the data preprocessing step.</span></span><br><span class="line">FP = income.count() - TP <span class="comment"># Specific to the naive case</span></span><br><span class="line"></span><br><span class="line">TN = <span class="number">0</span> <span class="comment"># No predicted negatives in the naive case</span></span><br><span class="line">FN = <span class="number">0</span> <span class="comment"># No predicted negatives in the naive case</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Calculate accuracy, precision and recall</span></span><br><span class="line">accuracy = TP/(TP+FP+FN+TN)</span><br><span class="line">recall = TP/(TP+FN)</span><br><span class="line">precision = TP/(TP+FP)</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Calculate F-score using the formula above for beta = 0.5 and correct values for precision and recall.</span></span><br><span class="line">fscore = (<span class="number">1</span>+<span class="number">0.5</span>*<span class="number">0.5</span>)*(precision[<span class="number">0</span>]*recall[<span class="number">0</span>])/(precision[<span class="number">0</span>]*<span class="number">0.5</span>*<span class="number">0.5</span>+recall[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the results </span></span><br><span class="line">print(<span class="string">"Naive Predictor: [Accuracy score: &#123;:.4f&#125;, F-score: &#123;:.4f&#125;]"</span>.format(accuracy[<span class="number">0</span>],fscore))</span><br></pre></td></tr></table></figure>

<pre><code>Naive Predictor: [Accuracy score: 0.2478, F-score: 0.2917]</code></pre><h3 id="Supervised-Learning-Models"><a href="#Supervised-Learning-Models" class="headerlink" title="Supervised Learning Models"></a>Supervised Learning Models</h3><p><strong>The following are some of the supervised learning models that are currently available in</strong> <a href="http://scikit-learn.org/stable/supervised_learning.html" target="_blank" rel="noopener"><code>scikit-learn</code></a> <strong>that you may choose from:</strong></p>
<ul>
<li>Gaussian Naive Bayes (GaussianNB)</li>
<li>Decision Trees</li>
<li>Ensemble Methods (Bagging, AdaBoost, Random Forest, Gradient Boosting)</li>
<li>K-Nearest Neighbors (KNeighbors)</li>
<li>Stochastic Gradient Descent Classifier (SGDC)</li>
<li>Support Vector Machines (SVM)</li>
<li>Logistic Regression</li>
</ul>
<h3 id="Question-2-Model-Application"><a href="#Question-2-Model-Application" class="headerlink" title="Question 2 - Model Application"></a>Question 2 - Model Application</h3><p>List three of the supervised learning models above that are appropriate for this problem that you will test on the census data. For each model chosen</p>
<ul>
<li>Describe one real-world application in industry where the model can be applied. </li>
<li>What are the strengths of the model; when does it perform well?</li>
<li>What are the weaknesses of the model; when does it perform poorly?</li>
<li>What makes this model a good candidate for the problem, given what you know about the data?</li>
</ul>
<p>** HINT: **</p>
<p>Structure your answer in the same format as above^, with 4 parts for each of the three models you pick. Please include references with your answer.</p>
<p>*<em>Answer: *</em></p>
<h3 id="1-Logistic-Regression"><a href="#1-Logistic-Regression" class="headerlink" title="1. Logistic Regression"></a>1. Logistic Regression</h3><p>1) It can be used in banking system to determine whether a customer will default or not.</p>
<p>2) When the targeted is a dummy variable this model can perform well. Itâ€™s highly interpretable and very efficient.</p>
<p>3) It is not suitable for non-linear problems and it is relatively perform worse than other complex methods.</p>
<p>4) In this problem we want to estimate the dummy which are people who have salary over or below 50k. This model is exactly suitable for this situation.</p>
<h3 id="2-Ensemble-Methods-Random-Forest"><a href="#2-Ensemble-Methods-Random-Forest" class="headerlink" title="2. Ensemble Methods (Random Forest)"></a>2. Ensemble Methods (Random Forest)</h3><p>1) It can be used to determine span e-mails or classify userâ€™s profile.</p>
<p>2) It performs better than many other simple models and not likely to be overfitting. It suits non-linear problems.</p>
<p>3) It is not suitable for linear problems, it is hard to interpret, and it requires high computational power.</p>
<p>4) In this problem we have a classification problem and ensemble methods can give us a good model.</p>
<h3 id="3-Support-Vector-Machines-SVM"><a href="#3-Support-Vector-Machines-SVM" class="headerlink" title="3.Support Vector Machines (SVM)"></a>3.Support Vector Machines (SVM)</h3><p>1) It can also be used for classification problems like whether a customer will default or not.</p>
<p>2) SVM is effective in high dimensional spaces and fast.</p>
<p>3) Not suitable for large dataset.</p>
<p>4) In our problem, we have many dimensions, SVM might perform good.</p>
<h3 id="Implementation-Creating-a-Training-and-Predicting-Pipeline"><a href="#Implementation-Creating-a-Training-and-Predicting-Pipeline" class="headerlink" title="Implementation - Creating a Training and Predicting Pipeline"></a>Implementation - Creating a Training and Predicting Pipeline</h3><p>To properly evaluate the performance of each model youâ€™ve chosen, itâ€™s important that you create a training and predicting pipeline that allows you to quickly and effectively train models using various sizes of training data and perform predictions on the testing data. Your implementation here will be used in the following section.<br>In the code block below, you will need to implement the following:</p>
<ul>
<li>Import <code>fbeta_score</code> and <code>accuracy_score</code> from <a href="http://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics" target="_blank" rel="noopener"><code>sklearn.metrics</code></a>.</li>
<li>Fit the learner to the sampled training data and record the training time.</li>
<li>Perform predictions on the test data <code>X_test</code>, and also on the first 300 training points <code>X_train[:300]</code>.<ul>
<li>Record the total prediction time.</li>
</ul>
</li>
<li>Calculate the accuracy score for both the training subset and testing set.</li>
<li>Calculate the F-score for both the training subset and testing set.<ul>
<li>Make sure that you set the <code>beta</code> parameter!</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Import two metrics from sklearn - fbeta_score and accuracy_score</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> fbeta_score</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_predict</span><span class="params">(learner, sample_size, X_train, y_train, X_test, y_test)</span>:</span> </span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    inputs:</span></span><br><span class="line"><span class="string">       - learner: the learning algorithm to be trained and predicted on</span></span><br><span class="line"><span class="string">       - sample_size: the size of samples (number) to be drawn from training set</span></span><br><span class="line"><span class="string">       - X_train: features training set</span></span><br><span class="line"><span class="string">       - y_train: income training set</span></span><br><span class="line"><span class="string">       - X_test: features testing set</span></span><br><span class="line"><span class="string">       - y_test: income testing set</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    results = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Fit the learner to the training data using slicing with 'sample_size' using .fit(training_features[:], training_labels[:])</span></span><br><span class="line">    start = time() <span class="comment"># Get start time</span></span><br><span class="line">    learner = learner.fit(X_train[:sample_size:], y_train[:sample_size].values.ravel())</span><br><span class="line">    end = time() <span class="comment"># Get end time</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Calculate the training time</span></span><br><span class="line">    results[<span class="string">'train_time'</span>] = end - start</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Get the predictions on the test set(X_test),</span></span><br><span class="line">    <span class="comment">#       then get predictions on the first 300 training samples(X_train) using .predict()</span></span><br><span class="line">    start = time() <span class="comment"># Get start time</span></span><br><span class="line">    predictions_test = learner.predict(X_test)</span><br><span class="line">    predictions_train = learner.predict(X_train[:<span class="number">300</span>])</span><br><span class="line">    end = time() <span class="comment"># Get end time</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Calculate the total prediction time</span></span><br><span class="line">    results[<span class="string">'pred_time'</span>] = end - start</span><br><span class="line">            </span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Compute accuracy on the first 300 training samples which is y_train[:300]</span></span><br><span class="line">    results[<span class="string">'acc_train'</span>] = accuracy_score(y_train[:<span class="number">300</span>], predictions_train)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Compute accuracy on test set using accuracy_score()</span></span><br><span class="line">    results[<span class="string">'acc_test'</span>] = accuracy_score(y_test, predictions_test)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Compute F-score on the the first 300 training samples using fbeta_score()</span></span><br><span class="line">    results[<span class="string">'f_train'</span>] = fbeta_score(y_train[:<span class="number">300</span>], predictions_train, beta = <span class="number">0.5</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Compute F-score on the test set which is y_test</span></span><br><span class="line">    results[<span class="string">'f_test'</span>] = fbeta_score(y_test, predictions_test, beta = <span class="number">0.5</span>)</span><br><span class="line">       </span><br><span class="line">    <span class="comment"># Success</span></span><br><span class="line">    print(<span class="string">"&#123;&#125; trained on &#123;&#125; samples."</span>.format(learner.__class__.__name__, sample_size))</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Return the results</span></span><br><span class="line">    <span class="keyword">return</span> results</span><br></pre></td></tr></table></figure>

<h3 id="Implementation-Initial-Model-Evaluation"><a href="#Implementation-Initial-Model-Evaluation" class="headerlink" title="Implementation: Initial Model Evaluation"></a>Implementation: Initial Model Evaluation</h3><p>In the code cell, you will need to implement the following:</p>
<ul>
<li>Import the three supervised learning models youâ€™ve discussed in the previous section.</li>
<li>Initialize the three models and store them in <code>&#39;clf_A&#39;</code>, <code>&#39;clf_B&#39;</code>, and <code>&#39;clf_C&#39;</code>.<ul>
<li>Use a <code>&#39;random_state&#39;</code> for each model you use, if provided.</li>
<li><strong>Note:</strong> Use the default settings for each model â€” you will tune one specific model in a later section.</li>
</ul>
</li>
<li>Calculate the number of records equal to 1%, 10%, and 100% of the training data.<ul>
<li>Store those values in <code>&#39;samples_1&#39;</code>, <code>&#39;samples_10&#39;</code>, and <code>&#39;samples_100&#39;</code> respectively.</li>
</ul>
</li>
</ul>
<p><strong>Note:</strong> Depending on which algorithms you chose, the following implementation may take some time to run!</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Import the three supervised learning models from sklearn</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line">random.seed(<span class="number">42</span>)</span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Initialize the three models</span></span><br><span class="line">clf_A = LogisticRegression(solver = <span class="string">'liblinear'</span>) <span class="comment"># set the default value manually in order to get rid of warnings</span></span><br><span class="line">clf_B = RandomForestClassifier(n_estimators = <span class="number">100</span>) <span class="comment"># set the default value manually in order to get rid of warnings</span></span><br><span class="line">clf_C = SVC(gamma = <span class="string">'scale'</span>) <span class="comment"># set the default value manually in order to get rid of warnings</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Calculate the number of samples for 1%, 10%, and 100% of the training data</span></span><br><span class="line"><span class="comment"># HINT: samples_100 is the entire training set i.e. len(y_train)</span></span><br><span class="line"><span class="comment"># HINT: samples_10 is 10% of samples_100 (ensure to set the count of the values to be `int` and not `float`)</span></span><br><span class="line"><span class="comment"># HINT: samples_1 is 1% of samples_100 (ensure to set the count of the values to be `int` and not `float`)</span></span><br><span class="line">samples_100 = len(y_train)</span><br><span class="line">samples_10 = int(len(y_train)*<span class="number">0.1</span>)</span><br><span class="line">samples_1 = int(len(y_train)*<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Collect results on the learners</span></span><br><span class="line">results = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> clf <span class="keyword">in</span> [clf_A, clf_B, clf_C]:</span><br><span class="line">    clf_name = clf.__class__.__name__</span><br><span class="line">    results[clf_name] = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i, samples <span class="keyword">in</span> enumerate([samples_1, samples_10, samples_100]):</span><br><span class="line">        results[clf_name][i] = \</span><br><span class="line">        train_predict(clf, samples, X_train, y_train, X_test, y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run metrics visualization for the three supervised learning models chosen</span></span><br><span class="line">vs.evaluate(results, accuracy[<span class="number">0</span>], fscore)</span><br></pre></td></tr></table></figure>

<pre><code>LogisticRegression trained on 361 samples.
LogisticRegression trained on 3617 samples.
LogisticRegression trained on 36177 samples.
RandomForestClassifier trained on 361 samples.
RandomForestClassifier trained on 3617 samples.
RandomForestClassifier trained on 36177 samples.
SVC trained on 361 samples.
SVC trained on 3617 samples.
SVC trained on 36177 samples.</code></pre><p><img src="/2020/01/22/%E5%AF%BB%E6%89%BE%E6%BD%9C%E5%9C%A8%E7%9A%84%E6%8D%90%E6%AC%BE%E8%80%85/output_29_1.png" alt="png"></p>
<hr>
<h2 id="Improving-Results"><a href="#Improving-Results" class="headerlink" title="Improving Results"></a>Improving Results</h2><p>In this final section, you will choose from the three supervised learning models the <em>best</em> model to use on the student data. You will then perform a grid search optimization for the model over the entire training set (<code>X_train</code> and <code>y_train</code>) by tuning at least one parameter to improve upon the untuned modelâ€™s F-score. </p>
<h3 id="Question-3-Choosing-the-Best-Model"><a href="#Question-3-Choosing-the-Best-Model" class="headerlink" title="Question 3 - Choosing the Best Model"></a>Question 3 - Choosing the Best Model</h3><ul>
<li>Based on the evaluation you performed earlier, in one to two paragraphs, explain to <em>CharityML</em> which of the three models you believe to be most appropriate for the task of identifying individuals that make more than $50,000. </li>
</ul>
<p>** HINT: **<br>Look at the graph at the bottom left from the cell above(the visualization created by <code>vs.evaluate(results, accuracy, fscore)</code>) and check the F score for the testing set when 100% of the training set is used. Which model has the highest score? Your answer should include discussion of the:</p>
<ul>
<li>metrics - F score on the testing when 100% of the training data is used, </li>
<li>prediction/training time</li>
<li>the algorithmâ€™s suitability for the data.</li>
</ul>
<p>*<em>Answer: *</em></p>
<p>The Logistic Regression model is the best model in this situation. </p>
<p>When 100% of the training data is used, the Logistic Regression model has the second highest F-score on testing set with only very slightly difference compared with the best model (SVM). However, it has the lowest training and prediction times, which is faster than the third(best) model quite a lot. Therefore it is wise to choose Logistic Regression model. Actually, Logistic Regression model is very suitable for this data because the targeted variable is a dummy.</p>
<h3 id="Question-4-Describing-the-Model-in-Laymanâ€™s-Terms"><a href="#Question-4-Describing-the-Model-in-Laymanâ€™s-Terms" class="headerlink" title="Question 4 - Describing the Model in Laymanâ€™s Terms"></a>Question 4 - Describing the Model in Laymanâ€™s Terms</h3><ul>
<li>In one to two paragraphs, explain to <em>CharityML</em>, in laymanâ€™s terms, how the final model chosen is supposed to work. Be sure that you are describing the major qualities of the model, such as how the model is trained and how the model makes a prediction. Avoid using advanced mathematical jargon, such as describing equations.</li>
</ul>
<p>** HINT: **</p>
<p>When explaining your model, if using external resources please include all citations.</p>
<p>*<em>Answer: *</em> </p>
<p>The logistic regression model will create a linear boundary cutting the data points, which can classify whether people have high income or not. It estimates the probability of a person belongs to high income group or low income group based on the change of explaining variables. Logistic regression uses the natural logarithm function to find the relationship between the variables and uses test data to find the coefficients. The function can then predict the future results using these coefficients in the logistic equation.[1] Therefore, this model can split our dataset successfully.</p>
<p><img src="/2020/01/22/%E5%AF%BB%E6%89%BE%E6%BD%9C%E5%9C%A8%E7%9A%84%E6%8D%90%E6%AC%BE%E8%80%85/LogReg_1__1_.png" alt="image_name"></p>
<p>It can be seen in th graph, different from linear model, logistic model is a non-linear function with probability between 0 to 1.</p>
<p>[1] Wikipedia. (2020). Logistic Regression. [online] Available at: <a href="https://simple.wikipedia.org/wiki/Logistic_Regression" target="_blank" rel="noopener">https://simple.wikipedia.org/wiki/Logistic_Regression</a> [Accessed 13 Jan. 2020].</p>
<h3 id="Implementation-Model-Tuning"><a href="#Implementation-Model-Tuning" class="headerlink" title="Implementation: Model Tuning"></a>Implementation: Model Tuning</h3><p>Fine tune the chosen model. Use grid search (<code>GridSearchCV</code>) with at least one important parameter tuned with at least 3 different values. You will need to use the entire training set for this. In the code cell below, you will need to implement the following:</p>
<ul>
<li>Import <a href="http://scikit-learn.org/0.17/modules/generated/sklearn.grid_search.GridSearchCV.html" target="_blank" rel="noopener"><code>sklearn.grid_search.GridSearchCV</code></a> and <a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html" target="_blank" rel="noopener"><code>sklearn.metrics.make_scorer</code></a>.</li>
<li>Initialize the classifier youâ€™ve chosen and store it in <code>clf</code>.<ul>
<li>Set a <code>random_state</code> if one is available to the same state you set before.</li>
</ul>
</li>
<li>Create a dictionary of parameters you wish to tune for the chosen model.<ul>
<li>Example: <code>parameters = {&#39;parameter&#39; : [list of values]}</code>.</li>
<li><strong>Note:</strong> Avoid tuning the <code>max_features</code> parameter of your learner if that parameter is available!</li>
</ul>
</li>
<li>Use <code>make_scorer</code> to create an <code>fbeta_score</code> scoring object (with $\beta = 0.5$).</li>
<li>Perform grid search on the classifier <code>clf</code> using the <code>&#39;scorer&#39;</code>, and store it in <code>grid_obj</code>.</li>
<li>Fit the grid search object to the training data (<code>X_train</code>, <code>y_train</code>), and store it in <code>grid_fit</code>.</li>
</ul>
<p><strong>Note:</strong> Depending on the algorithm chosen and the parameter list, the following implementation may take some time to run!</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Import 'GridSearchCV', 'make_scorer', and any other necessary libraries</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> make_scorer</span><br><span class="line">random.seed(<span class="number">42</span>)</span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Initialize the classifier</span></span><br><span class="line">clf = LogisticRegression(solver = <span class="string">'liblinear'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Create the parameters list you wish to tune, using a dictionary if needed.</span></span><br><span class="line"><span class="comment"># HINT: parameters = &#123;'parameter_1': [value1, value2], 'parameter_2': [value1, value2]&#125;</span></span><br><span class="line">parameters = &#123;<span class="string">'solver'</span>:[<span class="string">'lbfgs'</span>, <span class="string">'newton-cg'</span>, <span class="string">'sag'</span>, <span class="string">'saga'</span>], <span class="string">'max_iter'</span>:[<span class="number">700</span>, <span class="number">1000</span>, <span class="number">1200</span>]&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Make an fbeta_score scoring object using make_scorer()</span></span><br><span class="line">scorer = make_scorer(fbeta_score, beta = <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()</span></span><br><span class="line">grid_obj = GridSearchCV(clf, parameters, scoring=scorer, cv = <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Fit the grid search object to the training data and find the optimal parameters using fit()</span></span><br><span class="line">grid_fit = grid_obj.fit(X_train, y_train.values.ravel())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the estimator</span></span><br><span class="line">best_clf = grid_fit.best_estimator_</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make predictions using the unoptimized and model</span></span><br><span class="line">predictions = (clf.fit(X_train, y_train.values.ravel())).predict(X_test)</span><br><span class="line">best_predictions = best_clf.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Report the before-and-afterscores</span></span><br><span class="line">print(<span class="string">"Unoptimized model\n------"</span>)</span><br><span class="line">print(<span class="string">"Accuracy score on testing data: &#123;:.4f&#125;"</span>.format(accuracy_score(y_test, predictions)))</span><br><span class="line">print(<span class="string">"F-score on testing data: &#123;:.4f&#125;"</span>.format(fbeta_score(y_test, predictions, beta = <span class="number">0.5</span>)))</span><br><span class="line">print(<span class="string">"\nOptimized Model\n------"</span>)</span><br><span class="line">print(<span class="string">"Final accuracy score on the testing data: &#123;:.4f&#125;"</span>.format(accuracy_score(y_test, best_predictions)))</span><br><span class="line">print(<span class="string">"Final F-score on the testing data: &#123;:.4f&#125;"</span>.format(fbeta_score(y_test, best_predictions, beta = <span class="number">0.5</span>)))</span><br></pre></td></tr></table></figure>

<pre><code>Unoptimized model
------
Accuracy score on testing data: 0.8419
F-score on testing data: 0.6832

Optimized Model
------
Final accuracy score on the testing data: 0.8418
Final F-score on the testing data: 0.6829</code></pre><h3 id="Question-5-Final-Model-Evaluation"><a href="#Question-5-Final-Model-Evaluation" class="headerlink" title="Question 5 - Final Model Evaluation"></a>Question 5 - Final Model Evaluation</h3><ul>
<li>What is your optimized modelâ€™s accuracy and F-score on the testing data? </li>
<li>Are these scores better or worse than the unoptimized model? </li>
<li>How do the results from your optimized model compare to the naive predictor benchmarks you found earlier in <strong>Question 1</strong>?_  </li>
</ul>
<p><strong>Note:</strong> Fill in the table below with your results, and then provide discussion in the <strong>Answer</strong> box.</p>
<h4 id="Results"><a href="#Results" class="headerlink" title="Results:"></a>Results:</h4><table>
<thead>
<tr>
<th align="center">Metric</th>
<th align="center">Unoptimized Model</th>
<th align="center">Optimized Model</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Accuracy Score</td>
<td align="center">0.8419</td>
<td align="center">0.8418</td>
</tr>
<tr>
<td align="center">F-score</td>
<td align="center">0.6832</td>
<td align="center">0.6829</td>
</tr>
</tbody></table>
<p>*<em>Answer: *</em></p>
<p>The scores for the optimized model is worse than the unoptimized model. The scores for the optimized model improves a lot compared with the naive model.</p>
<hr>
<h2 id="Feature-Importance"><a href="#Feature-Importance" class="headerlink" title="Feature Importance"></a>Feature Importance</h2><p>An important task when performing supervised learning on a dataset like the census data we study here is determining which features provide the most predictive power. By focusing on the relationship between only a few crucial features and the target label we simplify our understanding of the phenomenon, which is most always a useful thing to do. In the case of this project, that means we wish to identify a small number of features that most strongly predict whether an individual makes at most or more than $50,000.</p>
<p>Choose a scikit-learn classifier (e.g., adaboost, random forests) that has a <code>feature_importance_</code> attribute, which is a function that ranks the importance of features according to the chosen classifier.  In the next python cell fit this classifier to training set and use this attribute to determine the top 5 most important features for the census dataset.</p>
<h3 id="Question-6-Feature-Relevance-Observation"><a href="#Question-6-Feature-Relevance-Observation" class="headerlink" title="Question 6 - Feature Relevance Observation"></a>Question 6 - Feature Relevance Observation</h3><p>When <strong>Exploring the Data</strong>, it was shown there are thirteen available features for each individual on record in the census data. Of these thirteen records, which five features do you believe to be most important for prediction, and in what order would you rank them and why?</p>
<p><strong>Answer:</strong></p>
<ol>
<li><p>education_num: more education might get better jobs</p>
</li>
<li><p>hours-per-week: more working hours more money</p>
</li>
<li><p>age: higher age implies more experience thus more income</p>
</li>
<li><p>marital status: married people might have more incentive to earn more money</p>
</li>
<li><p>sex: it might exists sex discrimination.</p>
</li>
</ol>
<h3 id="Implementation-Extracting-Feature-Importance"><a href="#Implementation-Extracting-Feature-Importance" class="headerlink" title="Implementation - Extracting Feature Importance"></a>Implementation - Extracting Feature Importance</h3><p>Choose a <code>scikit-learn</code> supervised learning algorithm that has a <code>feature_importance_</code> attribute availble for it. This attribute is a function that ranks the importance of each feature when making predictions based on the chosen algorithm.</p>
<p>In the code cell below, you will need to implement the following:</p>
<ul>
<li>Import a supervised learning model from sklearn if it is different from the three used earlier.</li>
<li>Train the supervised model on the entire training set.</li>
<li>Extract the feature importances using <code>&#39;.feature_importances_&#39;</code>.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Import a supervised learning model that has 'feature_importances_'</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> RandomizedSearchCV</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Train the supervised model on the training set using .fit(X_train, y_train)</span></span><br><span class="line">param_dist = &#123;<span class="string">"max_depth"</span>: [<span class="number">3</span>, <span class="literal">None</span>],</span><br><span class="line">              <span class="string">"n_estimators"</span>: list(range(<span class="number">10</span>, <span class="number">200</span>)),</span><br><span class="line">              <span class="string">"max_features"</span>: list(range(<span class="number">1</span>, X_test.shape[<span class="number">1</span>]+<span class="number">1</span>)),</span><br><span class="line">              <span class="string">"min_samples_split"</span>: list(range(<span class="number">2</span>, <span class="number">11</span>)),</span><br><span class="line">              <span class="string">"min_samples_leaf"</span>: list(range(<span class="number">1</span>, <span class="number">11</span>)),</span><br><span class="line">              <span class="string">"bootstrap"</span>: [<span class="literal">True</span>, <span class="literal">False</span>],</span><br><span class="line">              <span class="string">"criterion"</span>: [<span class="string">"gini"</span>, <span class="string">"entropy"</span>]&#125;</span><br><span class="line">model = RandomizedSearchCV(clf_B, param_distributions=param_dist)</span><br><span class="line">model.fit(X_train, y_train.values.ravel())</span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Extract the feature importances using .feature_importances_ </span></span><br><span class="line">importances = model.best_estimator_.feature_importances_</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot</span></span><br><span class="line">vs.feature_plot(importances, X_train, y_train)</span><br></pre></td></tr></table></figure>

<pre><code>/home/jason/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for &apos;cv&apos; instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.
  warnings.warn(CV_WARNING, FutureWarning)</code></pre><p><img src="/2020/01/22/%E5%AF%BB%E6%89%BE%E6%BD%9C%E5%9C%A8%E7%9A%84%E6%8D%90%E6%AC%BE%E8%80%85/output_47_1.png" alt="png"></p>
<h3 id="Question-7-Extracting-Feature-Importance"><a href="#Question-7-Extracting-Feature-Importance" class="headerlink" title="Question 7 - Extracting Feature Importance"></a>Question 7 - Extracting Feature Importance</h3><p>Observe the visualization created above which displays the five most relevant features for predicting if an individual makes at most or above $50,000.  </p>
<ul>
<li>How do these five features compare to the five features you discussed in <strong>Question 6</strong>?</li>
<li>If you were close to the same answer, how does this visualization confirm your thoughts? </li>
<li>If you were not close, why do you think these features are more relevant?</li>
</ul>
<p><strong>Answer:</strong></p>
<p>Compared with what I thought before, the importance of marital status, ages and years of education are comfirmed by the graph because their weights are high. However, capital-gain and relationship_Husband is not what I predicted. </p>
<p>For capital-gain it might be only whealthy people can have more gain in capital since they have more free money to invest. For relationship, it is kind of overlap with the marital status so the reason is similar as what I explained before that married people are more likely try to earn more money to support the family especially for males.</p>
<h3 id="Feature-Selection"><a href="#Feature-Selection" class="headerlink" title="Feature Selection"></a>Feature Selection</h3><p>How does a model perform if we only use a subset of all the available features in the data? With less features required to train, the expectation is that training and prediction time is much lower â€” at the cost of performance metrics. From the visualization above, we see that the top five most important features contribute more than half of the importance of <strong>all</strong> features present in the data. This hints that we can attempt to <em>reduce the feature space</em> and simplify the information required for the model to learn. The code cell below will use the same optimized model you found earlier, and train it on the same training set <em>with only the top five important features</em>. </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Import functionality for cloning a model</span></span><br><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> clone</span><br><span class="line"></span><br><span class="line"><span class="comment"># Reduce the feature space</span></span><br><span class="line">X_train_reduced = X_train[X_train.columns.values[(np.argsort(importances)[::<span class="number">-1</span>])[:<span class="number">5</span>]]]</span><br><span class="line">X_test_reduced = X_test[X_test.columns.values[(np.argsort(importances)[::<span class="number">-1</span>])[:<span class="number">5</span>]]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train on the "best" model found from grid search earlier</span></span><br><span class="line">clf = (clone(best_clf)).fit(X_train_reduced, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make new predictions</span></span><br><span class="line">reduced_predictions = clf.predict(X_test_reduced)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Report scores from the final model using both versions of data</span></span><br><span class="line">print(<span class="string">"Final Model trained on full data\n------"</span>)</span><br><span class="line">print(<span class="string">"Accuracy on testing data: &#123;:.4f&#125;"</span>.format(accuracy_score(y_test, best_predictions)))</span><br><span class="line">print(<span class="string">"F-score on testing data: &#123;:.4f&#125;"</span>.format(fbeta_score(y_test, best_predictions, beta = <span class="number">0.5</span>)))</span><br><span class="line">print(<span class="string">"\nFinal Model trained on reduced data\n------"</span>)</span><br><span class="line">print(<span class="string">"Accuracy on testing data: &#123;:.4f&#125;"</span>.format(accuracy_score(y_test, reduced_predictions)))</span><br><span class="line">print(<span class="string">"F-score on testing data: &#123;:.4f&#125;"</span>.format(fbeta_score(y_test, reduced_predictions, beta = <span class="number">0.5</span>)))</span><br></pre></td></tr></table></figure>

<pre><code>Final Model trained on full data
------
Accuracy on testing data: 0.8418
F-score on testing data: 0.6829

Final Model trained on reduced data
------
Accuracy on testing data: 0.8258
F-score on testing data: 0.6462


/home/jason/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)</code></pre><h3 id="Question-8-Effects-of-Feature-Selection"><a href="#Question-8-Effects-of-Feature-Selection" class="headerlink" title="Question 8 - Effects of Feature Selection"></a>Question 8 - Effects of Feature Selection</h3><ul>
<li>How does the final modelâ€™s F-score and accuracy score on the reduced data using only five features compare to those same scores when all features are used?</li>
<li>If training time was a factor, would you consider using the reduced data as your training set?</li>
</ul>
<p><strong>Answer:</strong></p>
<p>It is actually very close to the full features with slightly lower scores. However since I am using Logistic Regression model, I will not use the reduced data. Because the training time is alreay very short with the full features. The algorithm is very efficient so the room for improvement is quite limited. Therefore it is not wise to sacrifice the model accuracy to save only a few seconds.</p>
<blockquote>
<p><strong>Note</strong>: Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to<br><strong>File -&gt; Download as -&gt; HTML (.html)</strong>. Include the finished document along with this notebook as your submission.</p>
</blockquote>
]]></content>
      <categories>
        <category>Pythonå­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title>Ré€ŸæŸ¥è¡¨</title>
    <url>/2020/01/22/R%E9%80%9F%E6%9F%A5%E8%A1%A8/</url>
    <content><![CDATA[<font size="3">
R å„ç§é€ŸæŸ¥è¡¨, Cheat Sheetã€‚ä¸æ”¯æŒæ‰‹æœºæµè§ˆ...

<a id="more"></a>
<hr>
<h1 id="1-R-åŸºç¡€"><a href="#1-R-åŸºç¡€" class="headerlink" title="1. R åŸºç¡€"></a>1. R åŸºç¡€</h1><h2 id="R-basic"><a href="#R-basic" class="headerlink" title="R basic"></a>R basic</h2><div class="pdf" target="./base-r.pdf" height></div>

<h2 id="R-advanced"><a href="#R-advanced" class="headerlink" title="R advanced"></a>R advanced</h2><div class="pdf" target="./advancedR.pdf" height></div>  

<h2 id="Regular-expressions"><a href="#Regular-expressions" class="headerlink" title="Regular expressions"></a>Regular expressions</h2><div class="pdf" target="./regex.pdf" height></div>

<h2 id="R-Markdown"><a href="#R-Markdown" class="headerlink" title="R Markdown"></a>R Markdown</h2><div class="pdf" target="./rmarkdown-2.0.pdf" height></div> 

<h2 id="Use-Python-in-R"><a href="#Use-Python-in-R" class="headerlink" title="Use Python in R"></a>Use Python in R</h2><div class="pdf" target="./reticulate.pdf" height></div> 

<h1 id="2-æ•°æ®å¤„ç†"><a href="#2-æ•°æ®å¤„ç†" class="headerlink" title="2. æ•°æ®å¤„ç†"></a>2. æ•°æ®å¤„ç†</h1><h2 id="Data-import"><a href="#Data-import" class="headerlink" title="Data import"></a>Data import</h2><div class="pdf" target="./data-import.pdf" height></div>

<h2 id="dplyr"><a href="#dplyr" class="headerlink" title="dplyr"></a>dplyr</h2><div class="pdf" target="./dplyr.pdf" height></div> 

<h2 id="Data-transformation"><a href="#Data-transformation" class="headerlink" title="Data transformation"></a>Data transformation</h2><div class="pdf" target="./data-transformation.pdf" height></div> 

<h2 id="Strings"><a href="#Strings" class="headerlink" title="Strings"></a>Strings</h2><div class="pdf" target="./strings.pdf" height></div>

<h2 id="Factors"><a href="#Factors" class="headerlink" title="Factors"></a>Factors</h2><div class="pdf" target="./factors.pdf" height></div>  

<h2 id="Dates-and-times-with-lubridate"><a href="#Dates-and-times-with-lubridate" class="headerlink" title="Dates and times with lubridate"></a>Dates and times with lubridate</h2><div class="pdf" target="./lubridate.pdf" height></div>

<h2 id="Apply-functions-with-purrr"><a href="#Apply-functions-with-purrr" class="headerlink" title="Apply functions with purrr"></a>Apply functions with purrr</h2><div class="pdf" target="./purrr.pdf" height></div>

<h2 id="Data-Science-in-Spark-with-sparklyr"><a href="#Data-Science-in-Spark-with-sparklyr" class="headerlink" title="Data Science in Spark with sparklyr"></a>Data Science in Spark with sparklyr</h2><div class="pdf" target="./sparklyr.pdf" height></div>


<h1 id="3-æ•°æ®å¯è§†åŒ–"><a href="#3-æ•°æ®å¯è§†åŒ–" class="headerlink" title="3. æ•°æ®å¯è§†åŒ–"></a>3. æ•°æ®å¯è§†åŒ–</h1><h2 id="ggplot2"><a href="#ggplot2" class="headerlink" title="ggplot2"></a>ggplot2</h2><div class="pdf" target="./ggplot2.pdf" height></div>

<h2 id="Thematic-maps-with-cartography"><a href="#Thematic-maps-with-cartography" class="headerlink" title="Thematic maps with cartography"></a>Thematic maps with cartography</h2><div class="pdf" target="./cartography.pdf" height></div>

<h1 id="4-æœºå™¨å­¦ä¹ "><a href="#4-æœºå™¨å­¦ä¹ " class="headerlink" title="4. æœºå™¨å­¦ä¹ "></a>4. æœºå™¨å­¦ä¹ </h1><h2 id="R-è‡ªå¸¦"><a href="#R-è‡ªå¸¦" class="headerlink" title="R è‡ªå¸¦"></a>R è‡ªå¸¦</h2><div class="pdf" target="./Rmachinelearning.pdf" height></div>

<h2 id="caret-Classification-And-REgression-Training"><a href="#caret-Classification-And-REgression-Training" class="headerlink" title="caret(Classification And REgression Training)"></a>caret(Classification And REgression Training)</h2><div class="pdf" target="./caret.pdf" height></div>

<h2 id="Deep-Learning-with-Keras"><a href="#Deep-Learning-with-Keras" class="headerlink" title="Deep Learning with Keras"></a>Deep Learning with Keras</h2><div class="pdf" target="./keras.pdf" height></div>

<h2 id="Time-series"><a href="#Time-series" class="headerlink" title="Time series"></a>Time series</h2><div class="pdf" target="./time-series.pdf" height></div>  
<h1 id="æ¥æºä»¥åŠæœ€åŽ"><a href="#æ¥æºä»¥åŠæœ€åŽ" class="headerlink" title="æ¥æºä»¥åŠæœ€åŽ"></a>æ¥æºä»¥åŠæœ€åŽ</h1><ol>
<li><a href="https://rstudio.com/resources/cheatsheets/" target="_blank" rel="noopener">https://rstudio.com/resources/cheatsheets/</a></li>
<li>å¤§å®¶æœ‰å¥½çš„Cheat sheetæ¬¢è¿Žç•™è¨€è¡¥å……</li>
</ol>
</font>]]></content>
      <categories>
        <category>Rå­¦ä¹ </category>
      </categories>
      <tags>
        <tag>é€ŸæŸ¥è¡¨</tag>
      </tags>
  </entry>
  <entry>
    <title>Pythoné€ŸæŸ¥è¡¨</title>
    <url>/2020/01/22/Python%E9%80%9F%E6%9F%A5%E8%A1%A8/</url>
    <content><![CDATA[<font size="3">
Python å„ç§é€ŸæŸ¥è¡¨,Cheat Sheetã€‚ä¸æ”¯æŒæ‰‹æœºæµè§ˆ...

<a id="more"></a>
<hr>
<h1 id="1-Python-åŸºç¡€"><a href="#1-Python-åŸºç¡€" class="headerlink" title="1. Python åŸºç¡€"></a>1. Python åŸºç¡€</h1><h2 id="Python-basic"><a href="#Python-basic" class="headerlink" title="Python basic"></a>Python basic</h2><div class="pdf" target="./Python_basic.pdf" height></div>

<h2 id="Python-classes"><a href="#Python-classes" class="headerlink" title="Python classes"></a>Python classes</h2><div class="pdf" target="./Python_classes.pdf" height></div>  

<h2 id="Python-dictionaries"><a href="#Python-dictionaries" class="headerlink" title="Python dictionaries"></a>Python dictionaries</h2><div class="pdf" target="./Python_dictionaries.pdf" height></div>  

<h2 id="Python-lists"><a href="#Python-lists" class="headerlink" title="Python lists"></a>Python lists</h2><div class="pdf" target="./Python_lists.pdf" height></div>  

<h2 id="Python-functions"><a href="#Python-functions" class="headerlink" title="Python functions"></a>Python functions</h2><div class="pdf" target="./Python_functions.pdf" height></div>

<h2 id="Python-if-and-while"><a href="#Python-if-and-while" class="headerlink" title="Python if and while"></a>Python if and while</h2><div class="pdf" target="./Python_if_while.pdf" height></div> 

<h2 id="Python-regular-expressions"><a href="#Python-regular-expressions" class="headerlink" title="Python regular expressions"></a>Python regular expressions</h2><div class="pdf" target="./python_regular_expressions.pdf" height></div> 

<h2 id="Python-read-files"><a href="#Python-read-files" class="headerlink" title="Python read files"></a>Python read files</h2><div class="pdf" target="./Python_files_exceptions.pdf" height></div>  

<h1 id="2-Python-å„ç§åŒ…"><a href="#2-Python-å„ç§åŒ…" class="headerlink" title="2. Python å„ç§åŒ…"></a>2. Python å„ç§åŒ…</h1><h2 id="1-Numpy"><a href="#1-Numpy" class="headerlink" title="1. Numpy"></a>1. Numpy</h2><div class="pdf" target="./Numpy.pdf" height></div>  

<h2 id="2-Pandas"><a href="#2-Pandas" class="headerlink" title="2. Pandas"></a>2. Pandas</h2><div class="pdf" target="./Pandas.pdf" height></div>  

<h2 id="3-Matplotlib"><a href="#3-Matplotlib" class="headerlink" title="3. Matplotlib"></a>3. Matplotlib</h2><div class="pdf" target="./Matplotlib.pdf" height></div>  

<h2 id="4-Seaborn"><a href="#4-Seaborn" class="headerlink" title="4. Seaborn"></a>4. Seaborn</h2><div class="pdf" target="./Seaborn.pdf" height></div>  

<h2 id="5-Scikit-learn"><a href="#5-Scikit-learn" class="headerlink" title="5. Scikit-learn"></a>5. Scikit-learn</h2><div class="pdf" target="./Sklearn.pdf" height></div> 

<p><img src="/2020/01/22/Python%E9%80%9F%E6%9F%A5%E8%A1%A8/1.png" alt="sklearn"></p>
<h2 id="6-Pytorch"><a href="#6-Pytorch" class="headerlink" title="6. Pytorch"></a>6. Pytorch</h2><p>å®˜æ–¹æ–‡æ¡£: <a href="https://pytorch.org/tutorials/beginner/ptcheat.html" target="_blank" rel="noopener">https://pytorch.org/tutorials/beginner/ptcheat.html</a></p>
<h1 id="æ¥æºä»¥åŠæœ€åŽ"><a href="#æ¥æºä»¥åŠæœ€åŽ" class="headerlink" title="æ¥æºä»¥åŠæœ€åŽ"></a>æ¥æºä»¥åŠæœ€åŽ</h1><ol>
<li><a href="https://ehmatthes.github.io/pcc_2e/cheat_sheets/cheat_sheets/" target="_blank" rel="noopener">https://ehmatthes.github.io/pcc_2e/cheat_sheets/cheat_sheets/</a></li>
<li><a href="https://www.datacamp.com/community/data-science-cheatsheets" target="_blank" rel="noopener">https://www.datacamp.com/community/data-science-cheatsheets</a></li>
<li><a href="https://www.dataquest.io/blog/regex-cheatsheet/" target="_blank" rel="noopener">https://www.dataquest.io/blog/regex-cheatsheet/</a></li>
<li>ä¸€ä¸ªå¾ˆå¥½çš„pythonåŸºç¡€é€ŸæŸ¥è¡¨: <a href="https://github.com/crazyguitar/pysheeet" target="_blank" rel="noopener">https://github.com/crazyguitar/pysheeet</a> </li>
<li>å¤§å®¶æœ‰å¥½çš„Cheat sheetæ¬¢è¿Žç•™è¨€è¡¥å……</li>
</ol>
</font>]]></content>
      <categories>
        <category>Pythonå­¦ä¹ </category>
      </categories>
      <tags>
        <tag>é€ŸæŸ¥è¡¨</tag>
      </tags>
  </entry>
  <entry>
    <title>åŸºäºŽpythonçš„åŸºé‡‘å®šæŠ•åˆ†æž</title>
    <url>/2020/01/21/%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E5%9F%BA%E9%87%91%E5%AE%9A%E6%8A%95%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<font size="3">
ä½¿ç”¨pythonå¯è§†åŒ–åˆ†æžåŸºé‡‘å®šæŠ•å’Œæ™®é€šæŠ•èµ„ä¹‹é—´çš„åŒºåˆ«å’Œä¼˜ç¼ºç‚¹ã€‚æŽ¢è®¨åŸºé‡‘å®šæŠ•çš„é™ä½Žé£Žé™©ä½œç”¨ã€‚

<a id="more"></a>
<hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<h2 id="çˆ¬å–ä¸œæ–¹è´¢å¯Œç½‘å…³äºŽåŸºé‡‘çš„æ•°æ®å¹¶å¤„ç†æ•°æ®"><a href="#çˆ¬å–ä¸œæ–¹è´¢å¯Œç½‘å…³äºŽåŸºé‡‘çš„æ•°æ®å¹¶å¤„ç†æ•°æ®" class="headerlink" title="çˆ¬å–ä¸œæ–¹è´¢å¯Œç½‘å…³äºŽåŸºé‡‘çš„æ•°æ®å¹¶å¤„ç†æ•°æ®"></a>çˆ¬å–ä¸œæ–¹è´¢å¯Œç½‘å…³äºŽåŸºé‡‘çš„æ•°æ®å¹¶å¤„ç†æ•°æ®</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">link=<span class="string">"http://fund.eastmoney.com/f10/F10DataApi.aspx?type=lsjz&amp;code=000311&amp;page=1&amp;sdate=2018-01-02&amp;edate=2020-01-10&amp;per=20"</span></span><br><span class="line">headers=&#123;</span><br><span class="line">    <span class="string">'user-agent'</span>: <span class="string">"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36"</span>,</span><br><span class="line">&#125;</span><br><span class="line">r=requests.get(link,headers=headers)</span><br><span class="line"></span><br><span class="line">s1 = requests.get(link)</span><br><span class="line">s1.encoding = <span class="string">'utf-8'</span></span><br><span class="line">soup1=BeautifulSoup(s1.text,<span class="string">'lxml'</span>)</span><br><span class="line">fund_detail=soup1.find_all(<span class="string">'tr'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">###èŽ·å–æ€»é¡µæ•°</span></span><br><span class="line">pattern=re.compile(<span class="string">r'pages:(.*),'</span>)</span><br><span class="line">html=r.text</span><br><span class="line">result=re.search(pattern,html).group(<span class="number">1</span>)</span><br><span class="line">pages=int(result)</span><br><span class="line">pages</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data_list=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, pages+<span class="number">1</span>):</span><br><span class="line">    link=<span class="string">'http://fund.eastmoney.com/f10/F10DataApi.aspx?type=lsjz&amp;code=000311&amp;page='</span>+str(i)+<span class="string">'&amp;sdate=2018-01-02&amp;edate=2020-01-10&amp;per=20'</span></span><br><span class="line">    r=requests.get(link,headers=headers)</span><br><span class="line">    s1 = requests.get(link)</span><br><span class="line">    s1.encoding = <span class="string">'utf-8'</span></span><br><span class="line">    soup1=BeautifulSoup(s1.text,<span class="string">'lxml'</span>)</span><br><span class="line">    fund_detail=soup1.find_all(<span class="string">'tr'</span>)</span><br><span class="line">    <span class="keyword">for</span> each <span class="keyword">in</span> fund_detail:</span><br><span class="line">       <span class="keyword">if</span> each.td <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">       <span class="keyword">else</span>:</span><br><span class="line">        date=each.td.text.strip()<span class="comment">##only first td</span></span><br><span class="line">        unit_value=each.find(<span class="string">'td'</span>,class_=<span class="string">'tor bold'</span>).text.strip()<span class="comment">##first tor bold</span></span><br><span class="line">        cum_value=each.find(<span class="string">'td'</span>,class_=<span class="string">'tor bold'</span>).next_sibling.text.strip()<span class="comment">##second td</span></span><br><span class="line">        percent=each.find(<span class="string">'td'</span>).next_sibling.next_sibling.next_sibling.text.strip()</span><br><span class="line">        purchase_condition=each.find(<span class="string">'td'</span>).next_sibling.next_sibling.next_sibling.next_sibling.text.strip()</span><br><span class="line">        sale_condition=each.find(<span class="string">'td'</span>).next_sibling.next_sibling.next_sibling.next_sibling.next_sibling.text.strip()</span><br><span class="line">        dividend=each.find(<span class="string">'td'</span>,class_=<span class="string">'red unbold'</span>).text.strip()</span><br><span class="line">        code= <span class="string">'000311'</span></span><br><span class="line">        name=  <span class="string">'æ™¯é¡ºé•¿åŸŽæ²ªæ·±300å¢žå¼º'</span></span><br><span class="line">        data_list.append([code, name, date,unit_value, cum_value, percent,purchase_condition, sale_condition, dividend])</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">len(data_list)</span><br></pre></td></tr></table></figure>




<pre><code>497</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fund_000311 = pd.DataFrame(data_list)<span class="comment"># è®¾ç½®åˆ—å</span></span><br><span class="line">fund_000311.columns=[<span class="string">'ä»£ç '</span>,<span class="string">'åå­—'</span>,<span class="string">'æ—¶é—´'</span>,<span class="string">'å•ä½å‡€å€¼'</span>,<span class="string">'ç´¯è®¡å‡€å€¼'</span>,<span class="string">'æ¶¨è·Œå¹…'</span>,<span class="string">'ç”³è´­'</span>,<span class="string">'èµŽå›ž'</span>,<span class="string">'åˆ†çº¢'</span>]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fund_000311.to_csv(<span class="string">"data0003111"</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fund_000311.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ä»£ç </th>
      <th>åå­—</th>
      <th>æ—¶é—´</th>
      <th>å•ä½å‡€å€¼</th>
      <th>ç´¯è®¡å‡€å€¼</th>
      <th>æ¶¨è·Œå¹…</th>
      <th>ç”³è´­</th>
      <th>èµŽå›ž</th>
      <th>åˆ†çº¢</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>000311</td>
      <td>æ™¯é¡ºé•¿åŸŽæ²ªæ·±300å¢žå¼º</td>
      <td>2020-01-10</td>
      <td>2.2260</td>
      <td>2.5660</td>
      <td>0.32%</td>
      <td>å¼€æ”¾ç”³è´­</td>
      <td>å¼€æ”¾èµŽå›ž</td>
      <td></td>
    </tr>
    <tr>
      <th>1</th>
      <td>000311</td>
      <td>æ™¯é¡ºé•¿åŸŽæ²ªæ·±300å¢žå¼º</td>
      <td>2020-01-09</td>
      <td>2.2190</td>
      <td>2.5590</td>
      <td>1.46%</td>
      <td>å¼€æ”¾ç”³è´­</td>
      <td>å¼€æ”¾èµŽå›ž</td>
      <td></td>
    </tr>
    <tr>
      <th>2</th>
      <td>000311</td>
      <td>æ™¯é¡ºé•¿åŸŽæ²ªæ·±300å¢žå¼º</td>
      <td>2020-01-08</td>
      <td>2.1870</td>
      <td>2.5270</td>
      <td>-1.44%</td>
      <td>å¼€æ”¾ç”³è´­</td>
      <td>å¼€æ”¾èµŽå›ž</td>
      <td></td>
    </tr>
    <tr>
      <th>3</th>
      <td>000311</td>
      <td>æ™¯é¡ºé•¿åŸŽæ²ªæ·±300å¢žå¼º</td>
      <td>2020-01-07</td>
      <td>2.2190</td>
      <td>2.5590</td>
      <td>0.91%</td>
      <td>å¼€æ”¾ç”³è´­</td>
      <td>å¼€æ”¾èµŽå›ž</td>
      <td></td>
    </tr>
    <tr>
      <th>4</th>
      <td>000311</td>
      <td>æ™¯é¡ºé•¿åŸŽæ²ªæ·±300å¢žå¼º</td>
      <td>2020-01-06</td>
      <td>2.1990</td>
      <td>2.5390</td>
      <td>-0.36%</td>
      <td>å¼€æ”¾ç”³è´­</td>
      <td>å¼€æ”¾èµŽå›ž</td>
      <td></td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">##ç”»å‡ºèµ°åŠ¿å›¾(ç´¯è®¡å‡€å€¼),å…±1328ä¸ªäº¤æ˜“æ—¥,ç´¯è®¡å‡€å€¼=å•ä½å‡€å€¼+ç´¯è®¡åˆ†çº¢</span></span><br><span class="line">cum_price=fund_000311[[<span class="string">'å•ä½å‡€å€¼'</span>]]</span><br><span class="line">cum_price=np.array(cum_price)</span><br><span class="line">cum_price=cum_price.tolist()</span><br><span class="line">cum_value=list()</span><br><span class="line">t = np.arange(<span class="number">0.0</span>, <span class="number">497.0</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> each <span class="keyword">in</span> reversed(cum_price):<span class="comment">##å€’åº ä»Žå¤´å¼€å§‹</span></span><br><span class="line"> cum_price=float(*each)</span><br><span class="line"> cum_value.append(cum_price)</span><br><span class="line">plt.plot(t,cum_value,<span class="string">'r-'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'value'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'time'</span>) </span><br><span class="line">plt.title(<span class="string">'Fund 000311 HuSheng 300 '</span>)</span><br></pre></td></tr></table></figure>




<pre><code>Text(0.5, 1.0, &apos;Fund 000311 HuSheng 300 &apos;)</code></pre><p><img src="/2020/01/21/%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E5%9F%BA%E9%87%91%E5%AE%9A%E6%8A%95%E5%88%86%E6%9E%90/output_6_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">##æ¶¨è·Œæƒ…å†µ</span></span><br><span class="line">percent=fund_000311[[<span class="string">'æ¶¨è·Œå¹…'</span>]]</span><br><span class="line">percent=np.array(percent)</span><br><span class="line">percent=percent.tolist()</span><br><span class="line"><span class="keyword">while</span> [<span class="string">''</span>] <span class="keyword">in</span> percent:</span><br><span class="line"> percent.remove([<span class="string">''</span>])<span class="comment">#åŽ»é™¤ä¸‰ä¸ªç©ºç™½å€¼</span></span><br><span class="line">percent_val=[]</span><br><span class="line">t = np.arange(<span class="number">0.0</span>, <span class="number">494.0</span>, <span class="number">1</span>)</span><br><span class="line">i=<span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> each <span class="keyword">in</span> reversed(percent):<span class="comment">##å€’åº ä»Žå¤´å¼€å§‹</span></span><br><span class="line">   each=float(str(*each).strip(<span class="string">'%'</span>))</span><br><span class="line">   percent_val.append(each)</span><br><span class="line">   i=i+<span class="number">1</span>  </span><br><span class="line">plt.plot(t,percent_val,<span class="string">'r-'</span>,linewidth=<span class="number">0.5</span>)</span><br><span class="line">plt.ylabel(<span class="string">'ratio(%)'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'time'</span>) </span><br><span class="line">plt.title(<span class="string">'Fund 000311 HuSheng 300 '</span>)</span><br><span class="line"><span class="comment">##å¯ä»¥çœ‹åˆ°åœ¨ç¬¬äºŒç™¾åˆ°ç¬¬ä¸‰ç™¾ä¸ªäº¤æ˜“æ—¥æ³¢åŠ¨çŽ‡æ˜Žæ˜¾å˜å¤§,400åˆ°500æ˜Žæ˜¾æ³¢åŠ¨çŽ‡å°ã€‚</span></span><br></pre></td></tr></table></figure>




<pre><code>Text(0.5, 1.0, &apos;Fund 000311 HuSheng 300 &apos;)</code></pre><p><img src="/2020/01/21/%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E5%9F%BA%E9%87%91%E5%AE%9A%E6%8A%95%E5%88%86%E6%9E%90/output_7_1.png" alt="png"></p>
<h2 id="åŸºé‡‘å®šæŠ•ç ”ç©¶"><a href="#åŸºé‡‘å®šæŠ•ç ”ç©¶" class="headerlink" title="åŸºé‡‘å®šæŠ•ç ”ç©¶"></a>åŸºé‡‘å®šæŠ•ç ”ç©¶</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">unit_price=fund_000311[[<span class="string">'å•ä½å‡€å€¼'</span>]]</span><br><span class="line">unit_price=np.array(unit_price)</span><br><span class="line">unit_price=unit_price.tolist()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># æŸ¥å‡ºåˆ†çº¢æƒ…å†µ</span></span><br><span class="line">dividend=fund_000311.loc[fund_000311[<span class="string">'åˆ†çº¢'</span>] != <span class="string">''</span>]</span><br><span class="line">dividend <span class="comment"># æœŸé—´æ— åˆ†çº¢</span></span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ä»£ç </th>
      <th>åå­—</th>
      <th>æ—¶é—´</th>
      <th>å•ä½å‡€å€¼</th>
      <th>ç´¯è®¡å‡€å€¼</th>
      <th>æ¶¨è·Œå¹…</th>
      <th>ç”³è´­</th>
      <th>èµŽå›ž</th>
      <th>åˆ†çº¢</th>
    </tr>
  </thead>
  <tbody>
  </tbody>
</table>
</div>



<h4 id="1-ç®€å•æƒ…å†µä¸€æ¬¡ä¹°å…¥"><a href="#1-ç®€å•æƒ…å†µä¸€æ¬¡ä¹°å…¥" class="headerlink" title="1. ç®€å•æƒ…å†µä¸€æ¬¡ä¹°å…¥"></a>1. ç®€å•æƒ…å†µä¸€æ¬¡ä¹°å…¥</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># è®¡ç®—å¹´åŒ–æ”¶ç›Š</span></span><br><span class="line"></span><br><span class="line">total_yearly_once = []</span><br><span class="line">unit_price_new = unit_price[<span class="number">50</span>:<span class="number">494</span>] <span class="comment"># é¿å…æŠ•èµ„æ—¶é—´å¤ªçŸ­ä¸è€ƒè™‘è¿‘50ä¸ªäº¤æ˜“æ—¥</span></span><br><span class="line">dayday = <span class="number">497</span> <span class="comment"># æŠ•èµ„æ—¶é•¿</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> reversed(unit_price_new):</span><br><span class="line"> i=float(*i)</span><br><span class="line"> pie=<span class="number">1000</span>/i <span class="comment">##ä»½é¢</span></span><br><span class="line"> temp = pie*<span class="number">2.226</span> <span class="comment"># 2.226 20å¹´1æœˆ10æ—¥</span></span><br><span class="line"> total_return = (temp<span class="number">-1000</span>)/<span class="number">1000</span></span><br><span class="line"> yearly_return = total_return/dayday*<span class="number">250</span> <span class="comment"># 250ä¸ªäº¤æ˜“æ—¥</span></span><br><span class="line"> total_yearly_once.append(yearly_return)</span><br><span class="line"> dayday = dayday - <span class="number">1</span></span><br><span class="line">total_yearly_once_df=pd.DataFrame(total_yearly_once)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç”»å›¾</span></span><br><span class="line"></span><br><span class="line">t = np.arange(<span class="number">0.0</span>, <span class="number">444.0</span>, <span class="number">1</span>)</span><br><span class="line">plt.plot(t,total_yearly_once_df,<span class="string">'r-'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'ratio(%)'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'time'</span>) </span><br><span class="line">plt.title(<span class="string">'Fund 000311 normal investment '</span>)</span><br></pre></td></tr></table></figure>




<pre><code>Text(0.5, 1.0, &apos;Fund 000311 normal investment &apos;)</code></pre><p><img src="/2020/01/21/%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E5%9F%BA%E9%87%91%E5%AE%9A%E6%8A%95%E5%88%86%E6%9E%90/output_13_1.png" alt="png"></p>
<p>å¯ä»¥çœ‹åˆ°æ”¶ç›ŠçŽ‡çš„æ³¢åŠ¨çŽ‡åœ¨è¿™æ®µæœŸé—´éžå¸¸çš„å¤§,å¤§è‡´ä»Ž0åˆ°å°†è¿‘60%,å¹¶ä¸”å­˜åœ¨è´Ÿæ”¶ç›ŠçŽ‡ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.hist(total_yearly_once, bins=<span class="number">5</span>,facecolor=<span class="string">"red"</span>, edgecolor=<span class="string">"black"</span>, alpha=<span class="number">0.7</span>)</span><br></pre></td></tr></table></figure>




<pre><code>(array([101., 110., 124.,  78.,  31.]),
 array([-0.02019805,  0.09804444,  0.21628694,  0.33452944,  0.45277193,
         0.57101443]),
 &lt;a list of 5 Patch objects&gt;)</code></pre><p><img src="/2020/01/21/%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E5%9F%BA%E9%87%91%E5%AE%9A%E6%8A%95%E5%88%86%E6%9E%90/output_15_1.png" alt="png"></p>
<ol start="2">
<li>å®šæŠ•ç®€å•æƒ…å†µ(ä¸è€ƒè™‘æ—¶é—´ä»·å€¼)automatic investment plan</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># è®¡ç®—å¹´åŒ–æ”¶ç›Š</span></span><br><span class="line">total_yearly_auto = []</span><br><span class="line">times= np.arange(<span class="number">494</span>, <span class="number">50</span>, <span class="number">-1</span>)</span><br><span class="line">dayday = <span class="number">497</span> <span class="comment"># æŠ•èµ„æ—¶é•¿</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> times:</span><br><span class="line">    unit_price_new = unit_price[<span class="number">50</span>:i+<span class="number">1</span>] <span class="comment"># é¿å…æŠ•èµ„æ—¶é—´å¤ªçŸ­ä¸è€ƒè™‘è¿‘50ä¸ªäº¤æ˜“æ—¥</span></span><br><span class="line">    each_pie=[]</span><br><span class="line">    each_amount=<span class="number">1000</span>/(len(unit_price_new)) <span class="comment"># æ¯ç¬”æŠ•èµ„å¤šå°‘</span></span><br><span class="line">    <span class="keyword">for</span> each <span class="keyword">in</span> (unit_price_new):</span><br><span class="line">        each=float(*each)</span><br><span class="line">        pie=each_amount/each</span><br><span class="line">        each_pie.append(pie)</span><br><span class="line">    total_return=(sum(each_pie)*<span class="number">2.226</span><span class="number">-1000</span>)/<span class="number">1000</span></span><br><span class="line">    yearly_return = total_return/dayday*<span class="number">250</span></span><br><span class="line">    total_yearly_auto.append(yearly_return)</span><br><span class="line">    dayday = dayday - <span class="number">1</span></span><br><span class="line">total_yearly_auto_df=pd.DataFrame(total_yearly_auto)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = np.arange(<span class="number">0.0</span>, <span class="number">444.0</span>, <span class="number">1</span>)</span><br><span class="line">plt.plot(t,total_yearly_auto_df,<span class="string">'b-'</span>, label=<span class="string">'auto'</span>)</span><br><span class="line">plt.plot(t,total_yearly_once_df,<span class="string">'r-'</span>, label=<span class="string">'normal'</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'upper right'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'ratio(%)'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'time'</span>) </span><br><span class="line">plt.title(<span class="string">'Fund 000311 auto investment'</span>)</span><br></pre></td></tr></table></figure>




<pre><code>Text(0.5, 1.0, &apos;Fund 000311 auto investment&apos;)</code></pre><p><img src="/2020/01/21/%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E5%9F%BA%E9%87%91%E5%AE%9A%E6%8A%95%E5%88%86%E6%9E%90/output_18_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.hist(total_yearly_once, bins=<span class="number">10</span>,facecolor=<span class="string">"red"</span>, edgecolor=<span class="string">"black"</span>, alpha=<span class="number">0.7</span>)</span><br><span class="line">plt.hist(total_yearly_auto, bins=<span class="number">10</span>,facecolor=<span class="string">"blue"</span>, edgecolor=<span class="string">"black"</span>, alpha=<span class="number">0.7</span>)</span><br></pre></td></tr></table></figure>




<pre><code>(array([ 52., 107., 148., 108.,   6.,  12.,   2.,   5.,   2.,   2.]),
 array([0.07401984, 0.09161269, 0.10920555, 0.1267984 , 0.14439126,
        0.16198411, 0.17957697, 0.19716982, 0.21476267, 0.23235553,
        0.24994838]),
 &lt;a list of 10 Patch objects&gt;)</code></pre><p><img src="/2020/01/21/%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E5%9F%BA%E9%87%91%E5%AE%9A%E6%8A%95%E5%88%86%E6%9E%90/output_19_1.png" alt="png"></p>
<p>é€šè¿‡ç”»å›¾å¯ä»¥çœ‹å‡º,å®šæŠ•å…·æœ‰æ›´å°çš„æ”¶ç›ŠçŽ‡æ³¢åŠ¨æ€§,å› æ­¤ä¹Ÿå…·æœ‰æ›´å°çš„é£Žé™©ã€‚æ”¶ç›ŠçŽ‡å¤§è‡´åœ¨10%åˆ°15%ä¹‹é—´,æ²¡æœ‰è´Ÿæ”¶ç›ŠçŽ‡ã€‚</p>
<h2 id="æ€»ç»“"><a href="#æ€»ç»“" class="headerlink" title="æ€»ç»“"></a>æ€»ç»“</h2><p>å¯ä»¥çœ‹å‡ºå®šæŠ•å…·æœ‰æ‰€è°“çš„â€œå‰Šå³°å¡«è°·â€çš„ä½œç”¨, å¯ä»¥è¾¾åˆ°éšæ—¶ä¹°å…¥è€Œå¹´åŒ–æ”¶ç›Šæ³¢åŠ¨ä¸å¤§çš„æ•ˆæžœã€‚è€Œæ™®é€šæŠ•èµ„çš„æ”¶ç›ŠçŽ‡ä¸ç¨³å®š,å¯èƒ½ä¸€ä¸‹å­â€œæš´å¯Œâ€ä¹Ÿå¯èƒ½â€œç™½å¿™æ´»â€,éœ€è¦æŠ•èµ„è€…æŽŒæ¡æŠ•èµ„æ—¶æœºã€‚å®šæŠ•å¯¹äºŽæƒ³è¦ç¨³å®šå¢žå€¼èµ„äº§çš„æŠ•èµ„è€…æ˜¯ä¸€ä¸ªç›¸å¯¹æ¯”è¾ƒå¥½çš„é€‰æ‹©ã€‚</p>
<p> æœ€åŽé¡¹ç›®åœ¨GitHubä¸Šç½‘å€:<a href="https://github.com/JasonVictor17/Stock-market-analysis" target="_blank" rel="noopener">https://github.com/JasonVictor17/Stock-market-analysis</a></p>
</font>]]></content>
      <categories>
        <category>Pythonå­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æ•°æ®åˆ†æž</tag>
        <tag>é‡‘èžè‚¡å¸‚</tag>
      </tags>
  </entry>
  <entry>
    <title>ä¸ªäººåšå®¢å»ºè®¾6-ä¸»é¢˜çš„é…ç½®Next</title>
    <url>/2020/01/21/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE6-%E4%B8%BB%E9%A2%98%E7%9A%84%E9%85%8D%E7%BD%AENext/</url>
    <content><![CDATA[<font size="3">
ä¸ªäººåšå®¢å»ºè®¾ç¬¬å…­æ­¥:æœ¬ç¯‡ä»‹ç»å¦‚ä½•é…ç½®Nextä¸»é¢˜,ä½¿å…¶æ›´åŠ ä¸ªæ€§åŒ–,å…¶ä»–ä¸»é¢˜çš„é…ç½®å…¶å®žä¹Ÿå¤§åŒå°å¼‚å¯ä»¥å‚è€ƒ,è¿™ç¯‡ä¹Ÿæ˜¯æœ¬æ•™ç¨‹çš„æœ€åŽä¸€ç¯‡,ä»¥åŽå¯èƒ½ä¼šæ›´æ–°æ›´å¤šå…³äºŽåšå®¢å»ºè®¾çš„å†…å®¹ã€‚

<a id="more"></a>
<hr>
<p>é¦–å…ˆæˆ‘ä»¬æ‰¾åˆ°ä¸»é¢˜æ–‡ä»¶å¤¹Nexté‡Œçš„é…ç½®æ–‡ä»¶,å¯¹,å°±æ˜¯é‚£ä¸ªå’Œç«™ç‚¹é…ç½®æ–‡ä»¶ä¸€æ ·åå­—çš„_config.ymlã€‚ æ‰“å¼€ä¼šå‘çŽ°é‡Œé¢æœ‰å‡ ç™¾è¡Œã€‚ã€‚ã€‚ä¸è¿‡æ²¡å…³ç³»æˆ‘ä¼šä»‹ç»æœ€ä¸»è¦çš„å‡ ä¸ªé…ç½®ã€‚å…¶ä»–å°±ç•™ç»™å¤§å®¶è‡ªè¡ŒæŽ¢ç´¢äº†ã€‚ </p>
<h3 id="1-èœå•æ "><a href="#1-èœå•æ " class="headerlink" title="1. èœå•æ "></a>1. èœå•æ </h3><p>çœ‹æˆ‘çš„åšå®¢å¯ä»¥çœ‹è§æœ‰å¾ˆå¤šæ ‡ç­¾å’Œåˆ†ç±»,ä½†æ˜¯åˆå§‹çŠ¶æ€æ˜¯æ²¡æœ‰çš„,æˆ‘ä»¬éœ€è¦é…ç½®ã€‚<br>é¦–å…ˆæˆ‘ä»¬æ‰¾åˆ°menu æ ,æŠŠtags(æ ‡ç­¾),Categories(åˆ†ç±»)æˆ–è€…å…¶ä»–éœ€è¦çš„æ ‡ç­¾å‰çš„#å·åˆ é™¤,è¡¨é¢æˆ‘ä»¬ä½¿ç”¨è¿™è¡Œä»£ç </p>
<p><img src="/2020/01/21/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE6-%E4%B8%BB%E9%A2%98%E7%9A%84%E9%85%8D%E7%BD%AENext/1.png" alt="peizhi">  </p>
<p>ä¿å­˜åŽæˆ‘ä»¬åœ¨åšå®¢ä¸»ç›®å½•ä¸‹è¾“å…¥:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo new page tags</span><br></pre></td></tr></table></figure>
<p>æˆ‘ä»¬å°±ä¼šåœ¨sourceæ–‡ä»¶å¤¹å†…å‘çŽ°ç”Ÿæˆäº†ä¸€ä¸ªtagsçš„æ–‡ä»¶å¤¹,æ‰“å¼€é‡Œé¢çš„index.mdæ–‡ä»¶æŒ‰ç…§å¦‚ä¸‹è®¾ç½®</p>
<p><img src="/2020/01/21/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE6-%E4%B8%BB%E9%A2%98%E7%9A%84%E9%85%8D%E7%BD%AENext/2.png" alt="peizhi"> </p>
<p> ç„¶åŽä¿å­˜å°±è¡Œäº†,å…¶ä»–çš„æ ‡ç­¾ä¹Ÿæ˜¯ç±»ä¼¼æ“ä½œå°±ä¸èµ˜è¿°äº†ã€‚ä¹‹åŽæˆ‘ä»¬åœ¨å†™æ–‡ç« æ—¶å€™æŒ‰ç…§å¦‚ä¸‹æ ¼å¼å†™å¼€å¤´:</p>
<p> <img src="/2020/01/21/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE6-%E4%B8%BB%E9%A2%98%E7%9A%84%E9%85%8D%E7%BD%AENext/3.png" alt="peizhi">  </p>
<p>å°±å¯ä»¥è‡ªåŠ¨ç”Ÿæˆæ ‡ç­¾å’Œåˆ†ç±»äº†ã€‚æ¯ä¸€æ¬¡æœ‰æ–°çš„æ ‡ç­¾å’Œåˆ†ç±»éƒ½ä¼šè‡ªåŠ¨åœ¨ç½‘é¡µç”Ÿæˆ,æ–¹ä¾¿å¿«æ·~(è‡ªåŠ¨ä¿å­˜åœ¨publicæ–‡ä»¶å¤¹å†…)</p>
<h3 id="2-è¯­è¨€"><a href="#2-è¯­è¨€" class="headerlink" title="2.è¯­è¨€"></a>2.è¯­è¨€</h3><p> åœ¨ç«™ç‚¹é…ç½®æ–‡ä»¶ä¸­å¡«å†™: language: â€˜zh-CNâ€™å°±å¯ä»¥è®¾ç½®ä¸­æ–‡äº†,å…¶ä»–è¯­è¨€å‚è€ƒå¦‚ä¸‹:</p>
<p> <img src="/2020/01/21/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE6-%E4%B8%BB%E9%A2%98%E7%9A%84%E9%85%8D%E7%BD%AENext/4.png" alt="peizhi"></p>
<h3 id="3-æ·»åŠ æœç´¢åŠŸèƒ½"><a href="#3-æ·»åŠ æœç´¢åŠŸèƒ½" class="headerlink" title="3. æ·»åŠ æœç´¢åŠŸèƒ½"></a>3. æ·»åŠ æœç´¢åŠŸèƒ½</h3><p>é¦–å…ˆåœ¨åšå®¢æ ¹ç›®å½•å®‰è£…æ’ä»¶:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install hexo-generator-searchdb --save</span><br></pre></td></tr></table></figure>
<p>åœ¨ç«™ç‚¹é…ç½®æ–‡ä»¶æœ€åŽåŠ ä¸Š:  </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">search:</span><br><span class="line">  path: search.xml</span><br><span class="line">  field: post</span><br><span class="line">  format: html</span><br><span class="line">  limit: 10000</span><br></pre></td></tr></table></figure>
<p>æœ€åŽåœ¨ä¸»é¢˜é…ç½®æ–‡ä»¶ä¸­:æ‰¾åˆ° local_search æ”¹ä¸ºtrue(åœ¨vs codeä¸­å¯ä»¥ä½¿ç”¨æœç´¢æ‰¾åˆ°è¯¥å‚æ•°)<br>è¿™æ ·éƒ¨ç½²ä¸€ä¸‹åšå®¢å†…æœç´¢åŠŸèƒ½å°±ä¸Šçº¿äº†ã€‚</p>
<h3 id="4-è®¾ç½®ä½œè€…æ˜µç§°å’Œç«™ç‚¹ä»‹ç»ç­‰"><a href="#4-è®¾ç½®ä½œè€…æ˜µç§°å’Œç«™ç‚¹ä»‹ç»ç­‰" class="headerlink" title="4.è®¾ç½®ä½œè€…æ˜µç§°å’Œç«™ç‚¹ä»‹ç»ç­‰"></a>4.è®¾ç½®ä½œè€…æ˜µç§°å’Œç«™ç‚¹ä»‹ç»ç­‰</h3><p>æ‰“å¼€ç«™ç‚¹é…ç½®æ–‡ä»¶,ç¬¬ä¸€é¡¹é‡Œè¿›è¡Œç¼–è¾‘,title,authorå’Œdescriptionç­‰</p>
<h3 id="5-ä¿®æ”¹faviconå›¾æ ‡"><a href="#5-ä¿®æ”¹faviconå›¾æ ‡" class="headerlink" title="5. ä¿®æ”¹faviconå›¾æ ‡"></a>5. ä¿®æ”¹faviconå›¾æ ‡</h3><p>ä¹Ÿå°±æ˜¯ç½‘é¡µçš„å›¾æ ‡,æˆ‘çš„åšå®¢æ˜¯ä¸€åªæ‰‹å·¥ç”»çš„å°è€é¼ ã€‚<br>ä½¿ç”¨å·¥å…·å°†éœ€è¦çš„å›¾ç‰‡è£å‰ªæˆ16<em>16å’Œ32</em>32å¤§å°,ç„¶åŽæ‰“å¼€nextæ–‡ä»¶å¤¹ä¸‹çš„sourceæ–‡ä»¶å¤¹,å†æ‰“å¼€imagesæ–‡ä»¶å¤¹å°†å›¾ç‰‡ä¿å­˜å…¶ä¸­ã€‚<br>æ‰“å¼€ä¸»é¢˜é…ç½®æ–‡ä»¶æ‰¾åˆ°favicon,å¦‚ä¸‹è¾“å…¥è‡ªå·±æ–‡ä»¶åå­—:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">favicon:</span><br><span class="line">  small: &#x2F;images&#x2F;mouse16.ico</span><br><span class="line">  medium: &#x2F;images&#x2F;mouse32.ico</span><br></pre></td></tr></table></figure>
<p>å†éƒ¨ç½²å°±å¯ä»¥çœ‹è§å›¾æ ‡å·²ç»æ›´æ”¹ã€‚å›¾ç‰‡å‰ªè£å·¥å…·:<a href="http://www.bitbug.net/" target="_blank" rel="noopener">æ¯”ç‰¹è™«</a></p>
<h3 id="6-å¼€é€šè¯„è®ºåŠŸèƒ½"><a href="#6-å¼€é€šè¯„è®ºåŠŸèƒ½" class="headerlink" title="6. å¼€é€šè¯„è®ºåŠŸèƒ½"></a>6. å¼€é€šè¯„è®ºåŠŸèƒ½</h3><p>æ³¨å†Œ<a href="https://www.livere.com/" target="_blank" rel="noopener">æ¥å¿…åŠ›</a>,å¡«å†™ç½‘ç«™åŽå¯ä»¥å¾—åˆ°ä¸€ä¸²ä»£ç ,æˆ‘ä»¬åªéœ€è¦å…¶ä¸­ä¸€éƒ¨åˆ†ã€‚<br> <img src="/2020/01/21/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE6-%E4%B8%BB%E9%A2%98%E7%9A%84%E9%85%8D%E7%BD%AENext/5.jpg" alt="peizhi"><br>å¤åˆ¶è¿™ä¸€ä¸²æ•°å­—åŽæˆ‘ä»¬æ‰“å¼€ä¸»é¢˜é…ç½®æ–‡ä»¶,æ‰¾åˆ°LiveRe comments system,åŽ»æŽ‰#æ³¨é‡Š,åœ¨livere_uidåŽç²˜è´´ä¸Šå°±å®Œæˆäº†ã€‚</p>
<h3 id="7-èµžèµ"><a href="#7-èµžèµ" class="headerlink" title="7.èµžèµ"></a>7.èµžèµ</h3><p> æ‰¾åˆ°ä¸»é¢˜é…ç½®æ–‡ä»¶ä¸‹çš„reward,æŠŠè‡ªå·±çš„å¾®ä¿¡æ”¶æ¬¾æˆ–è€…æ”¯ä»˜å®æ”¶æ¬¾äºŒç»´ç æ”¾ä¸ŠåŽ»å°±è¡Œäº†ã€‚</p>
<h3 id="ç»“è¯­"><a href="#ç»“è¯­" class="headerlink" title="ç»“è¯­"></a>ç»“è¯­</h3><p>åˆ°è¿™é‡Œä¸€ä¸ªè‡ªå·±çš„åšå®¢åŸºæœ¬ä¸Šå°±æ­å¥½äº†,æŽ¥ä¸‹æ¥å°±æ˜¯è‡ªæˆ‘æŽ¢ç´¢æŠ˜è…¾çš„è¿‡ç¨‹äº†,ç½‘ä¸Šèµ„æºå¾ˆå¤šå¯ä»¥æ…¢æ…¢æŠ˜è…¾è‡ªå·±çš„åšå®¢,æˆ‘ä¹Ÿä¼šä¸å®šæœŸçš„æ›´æ–°æˆ‘æ‰€èŽ·å¾—çš„æ­åšå®¢å°æŠ€å·§ã€‚</p>
<p>åœ¨å±žäºŽè‡ªå·±çš„ä¸€ç‰‡å°å¤©åœ°å°½æƒ…å‘æŒ¥å§!</p>
<hr>
<p>ä¸Šä¸€æ­¥: <a href="https://www.guojingde.cn/2020/01/21/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE5-Markdown%E8%AF%AD%E6%B3%95/" target="_blank" rel="noopener">ä¸ªäººåšå®¢å»ºè®¾5-Markdownè¯­æ³•</a><br>ç›®å½•: <a href="https://www.guojingde.cn/2020/01/19/%E7%94%A8GitHub%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/" target="_blank" rel="noopener">ç”¨GitHubæ­å»ºä¸ªäººåšå®¢(ç›®å½•)</a></p>
</font>]]></content>
      <categories>
        <category>åšå®¢æ­å»º</category>
      </categories>
      <tags>
        <tag>åšå®¢æ­å»º</tag>
      </tags>
  </entry>
  <entry>
    <title>ä¸ªäººåšå®¢å»ºè®¾5-Markdownè¯­æ³•</title>
    <url>/2020/01/21/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE5-Markdown%E8%AF%AD%E6%B3%95/</url>
    <content><![CDATA[<font size="3">
ä¸ªäººåšå®¢å»ºè®¾ç¬¬äº”æ­¥:ç”¨åŸºæœ¬çš„Markdownè¯­æ³•æ¥å†™åšå®¢...

<a id="more"></a>
<hr>
<h3 id="ä¸ºä»€ä¹ˆmarkdown"><a href="#ä¸ºä»€ä¹ˆmarkdown" class="headerlink" title="ä¸ºä»€ä¹ˆmarkdown"></a>ä¸ºä»€ä¹ˆmarkdown</h3><p>é¦–å…ˆæˆ‘ä»¬å†™çš„æ¯ä¸€ç¯‡åšå®¢éƒ½æ˜¯åŸºäºŽmdä¹Ÿå°±æ˜¯markdownæ–‡ä»¶çš„,å…¶æ¬¡Markdown æ˜¯ä¸€ç§è½»é‡çº§æ ‡è®°è¯­è¨€,å®ƒå…è®¸äººä»¬ä½¿ç”¨æ˜“è¯»æ˜“å†™çš„çº¯æ–‡æœ¬æ ¼å¼ç¼–å†™æ–‡æ¡£,éžå¸¸é€‚åˆå†™åšå®¢ã€‚</p>
<h3 id="ç¼–è¾‘å™¨çš„é€‰æ‹©"><a href="#ç¼–è¾‘å™¨çš„é€‰æ‹©" class="headerlink" title="ç¼–è¾‘å™¨çš„é€‰æ‹©"></a>ç¼–è¾‘å™¨çš„é€‰æ‹©</h3><p>å¯ä»¥ä½¿ç”¨<a href="https://code.visualstudio.com/" target="_blank" rel="noopener">VS code</a>, <a href="http://markdownpad.com/" target="_blank" rel="noopener">MarkdownPad</a>æˆ–è€…<a href="https://sosfos.wordpress.com/" target="_blank" rel="noopener">BookPad</a>(æ³¨æ„è¿™ä¸ªæ”¶è´¹14å—)<br>å…·ä½“çš„é€‰æ‹©ç»“åˆè‡ªèº«å–œå¥½,æ²¡æœ‰ä»€ä¹ˆå¤§çš„åŒºåˆ«,éƒ½å…·æœ‰å®žæ—¶é¢„è§ˆåŠŸèƒ½æˆ‘è§‰å¾—å°±å¤Ÿäº†ã€‚</p>
<h3 id="ç®€å•çš„å‡ ä¸ªè¯­æ³•"><a href="#ç®€å•çš„å‡ ä¸ªè¯­æ³•" class="headerlink" title="ç®€å•çš„å‡ ä¸ªè¯­æ³•"></a>ç®€å•çš„å‡ ä¸ªè¯­æ³•</h3><h4 id="1-æ ‡é¢˜"><a href="#1-æ ‡é¢˜" class="headerlink" title="1. æ ‡é¢˜"></a>1. æ ‡é¢˜</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># hello</span><br><span class="line">## hello</span><br></pre></td></tr></table></figure>
<p>ä¸Šé¢ä¸¤ä¸ªåˆ†åˆ«æ˜¯ä¸»æ ‡é¢˜å’Œæ¬¡çº§æ ‡é¢˜çš„åŒºåˆ«,äº•å·è¶Šå¤šå­—ä½“å°±è¶Šå°,æ ‡é¢˜ä¹Ÿè¶Šæ¬¡çº§ã€‚</p>
<h4 id="2-å­—ä½“å’Œé¢œè‰²"><a href="#2-å­—ä½“å’Œé¢œè‰²" class="headerlink" title="2. å­—ä½“å’Œé¢œè‰²"></a>2. å­—ä½“å’Œé¢œè‰²</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;font face&#x3D;&quot;é»‘ä½“&quot;&gt;</span><br><span class="line">&lt;font color&#x3D;red size&#x3D;4&gt;</span><br><span class="line">&lt;font color&#x3D;red size&#x3D;4 face&#x3D;&quot;é»‘ä½“&quot;&gt;</span><br></pre></td></tr></table></figure>
<p>ä½¿ç”¨å¦‚ä¸Šæ–¹å¼åœ¨ä¹‹åŽåŠ ä¸Šæ–‡å­—çš„æ•ˆæžœåˆ†åˆ«å¦‚ä¸‹:<br><font face="é»‘ä½“" size="4">æˆ‘æ˜¯å¥½äºº<br><font color="red" size="4">æˆ‘æ˜¯å¥½äºº<br><font color="red" size="6" face="é»‘ä½“">æˆ‘æ˜¯å¥½äºº</font></font></font></p>
<p>:kissing:</p>
<p><font size="3" face="å¾®è½¯é›…é»‘">æƒ³è¦æ‰“å‡ºå¦‚ä¸Šè¡¨æƒ…æˆ‘ä»¬åªéœ€è¦è¾“å…¥</font></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">:kissing:</span><br></pre></td></tr></table></figure>
<p>å…·ä½“å¯ä»¥å‚è€ƒGitHubä¸Š: <a href="https://github.com/guodongxiaren/README/blob/master/emoji.md" target="_blank" rel="noopener">https://github.com/guodongxiaren/README/blob/master/emoji.md</a></p>
<h4 id="3-å›¾ç‰‡çš„æ’å…¥"><a href="#3-å›¾ç‰‡çš„æ’å…¥" class="headerlink" title="3. å›¾ç‰‡çš„æ’å…¥"></a>3. å›¾ç‰‡çš„æ’å…¥</h4><p>å½“æˆ‘ä»¬ä½¿ç”¨Hexoå»ºç«‹åšå®¢çš„æ—¶å€™å…ä¸äº†è¦æ’å…¥å›¾ç‰‡,æˆ‘ä»¬å¯ä»¥åœ¨sourceä¸‹åˆ›å»ºä¸€ä¸ªimageæ–‡ä»¶å¤¹ç„¶åŽé€šè¿‡:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">![](&#x2F;image&#x2F;image.png)</span><br></pre></td></tr></table></figure>
<p>æ¥è®¿é—®ã€‚ä½†æ˜¯è¿™é‡Œæˆ‘æŽ¨èä»¥ä¸‹ä¸€ç§ç”¨æ³•è™½ç„¶ç•¥æœ‰ç‚¹éº»çƒ¦,ä½†æ˜¯ä»¥åŽå¤„ç†å›¾ç‰‡ä¼šå¾ˆæ–¹ä¾¿ã€‚<br>é¦–å…ˆåœ¨åšå®¢çš„ç›®å½•blogä¸‹ç”¨å‘½ä»¤è¡Œè¿è¡Œå¦‚ä¸‹ä»£ç å®‰è£…ä¸€ä¸ªå°æ’ä»¶, å‚è€ƒ:<a href="https://github.com/xcodebuild/hexo-asset-image" target="_blank" rel="noopener">https://github.com/xcodebuild/hexo-asset-image</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install hexo-asset-image --save</span><br></pre></td></tr></table></figure>
<p>ç„¶åŽæˆ‘ä»¬åœ¨ç«™ç‚¹é…ç½®æ–‡ä»¶_congif.ymlä¸­è®¾ç½®:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">post_asset_folder: true</span><br></pre></td></tr></table></figure>
<p>ä¹‹åŽæˆ‘ä»¬æˆ‘ä»¬åªè¦åœ¨æ’°å†™åšå®¢çš„ç›®å½•ä¸‹,ä¹Ÿå°±æ˜¯sourceç›®å½•ä¸‹çš„_postsæ–‡ä»¶å¤¹å†…åˆ›å»ºä¸€ä¸ªå’Œè¿™ç¯‡åšå®¢åå­—ä¸€æ ·çš„æ–‡ä»¶å¤¹,æŠŠå›¾ç‰‡æ”¾å…¥å…¶ä¸­,å°±å¯ä»¥æŒ‰ç…§å¦‚ä¸‹æ¥æ’å…¥å›¾ç‰‡äº†:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">![å›¾ç‰‡å‘½å](logo.jpg)</span><br></pre></td></tr></table></figure>
<p>ä¸éœ€è¦æŒ‡å®šå›¾ç‰‡çš„è·¯å¾„,å¦å¤–å¦‚æžœæ˜¯åœ¨å‘½ä»¤è¡Œä½¿ç”¨ hexo new å‘½ä»¤æ¥åˆ›ç«‹æ–°çš„mdæ–‡ä»¶æ—¶ä¼šè‡ªåŠ¨ç”ŸæˆåŒåæ–‡ä»¶å¤¹ä¸éœ€è¦æ‰‹åŠ¨å¦å¤–åˆ›å»ºã€‚</p>
<h4 id="4-ç½‘ç«™é“¾æŽ¥"><a href="#4-ç½‘ç«™é“¾æŽ¥" class="headerlink" title="4. ç½‘ç«™é“¾æŽ¥"></a>4. ç½‘ç«™é“¾æŽ¥</h4><p>æ¯”èµ·ç›´æŽ¥å¤åˆ¶é“¾æŽ¥æ›´å¥½çš„æ–¹å¼æ˜¯:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[VS code](https:&#x2F;&#x2F;code.visualstudio.com&#x2F;)</span><br></pre></td></tr></table></figure>
<p>å‰é¢å†™åœ¨æ–‡ç« ä¸­å‘ˆçŽ°çš„åå­—,åŽé¢å†™é“¾æŽ¥,è¿™æ ·ç›´æŽ¥ç‚¹æ–‡å­—å°±å¯ä»¥è¿›å…¥é“¾æŽ¥ã€‚</p>
<h4 id="5-å¼ºè°ƒ"><a href="#5-å¼ºè°ƒ" class="headerlink" title="5.å¼ºè°ƒ"></a>5.å¼ºè°ƒ</h4><p>ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤å°±å¯ä»¥è¾¾åˆ°å¯¹æŸäº›æ–‡å­—å¼ºè°ƒçš„ä½œç”¨  </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">*single asterisks*</span><br><span class="line">_single underscores_  </span><br><span class="line">**double asterisks**</span><br><span class="line">__double underscores__</span><br></pre></td></tr></table></figure>
<p><em>single asterisks</em><br><em>single underscores</em><br><strong>double asterisks</strong><br><strong>double underscores</strong> </p>
<h3 id="æ€»ç»“"><a href="#æ€»ç»“" class="headerlink" title="æ€»ç»“"></a>æ€»ç»“</h3><p>å­¦ä¼šè¿™å‡ ä¸ªåŸºæœ¬å‘½ä»¤å°±å·®ä¸å¤šå¯ä»¥å¼€å§‹å†™åšå®¢äº†,è‡³äºŽå…¶ä»–æ›´å¤šçš„å‘½ä»¤è¯·å‚è€ƒ<a href="https://www.appinn.com/markdown/" target="_blank" rel="noopener">Markdownä¸­æ–‡ç‰ˆè¯´æ˜Ž</a>ã€‚<br>å¸Œæœ›å¤§å®¶å†™åšå®¢æ„‰å¿«!</p>
<hr>
<p>ä¸Šä¸€æ­¥: <a href="https://www.guojingde.cn/2020/01/21/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE4-%E9%80%89%E6%8B%A9%E5%8D%9A%E5%AE%A2%E4%B8%BB%E9%A2%98/" target="_blank" rel="noopener">ä¸ªäººåšå®¢å»ºè®¾4-é€‰æ‹©åšå®¢ä¸»é¢˜</a><br>ä¸‹ä¸€æ­¥: <a href="https://www.guojingde.cn/2020/01/21/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE6-%E4%B8%BB%E9%A2%98%E7%9A%84%E9%85%8D%E7%BD%AENext/" target="_blank" rel="noopener">ä¸ªäººåšå®¢å»ºè®¾6-ä¸»é¢˜çš„é…ç½®Next</a><br>ç›®å½•: <a href="https://www.guojingde.cn/2020/01/19/%E7%94%A8GitHub%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/" target="_blank" rel="noopener">ç”¨GitHubæ­å»ºä¸ªäººåšå®¢(ç›®å½•)</a></p>
</font>]]></content>
      <categories>
        <category>åšå®¢æ­å»º</category>
      </categories>
      <tags>
        <tag>åšå®¢æ­å»º</tag>
      </tags>
  </entry>
  <entry>
    <title>ä¸ªäººåšå®¢å»ºè®¾4-é€‰æ‹©åšå®¢ä¸»é¢˜</title>
    <url>/2020/01/21/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE4-%E9%80%89%E6%8B%A9%E5%8D%9A%E5%AE%A2%E4%B8%BB%E9%A2%98/</url>
    <content><![CDATA[<font size="3">
ä¸ªäººåšå®¢å»ºè®¾ç¬¬å››æ­¥:é€‰æ‹©ä¸€ä¸ªè‡ªå·±å–œæ¬¢çš„ä¸»é¢˜æ¨¡æ¿...

<a id="more"></a>
<hr>
<h3 id="ä¸ºä»€ä¹ˆä½¿ç”¨æ¨¡æ¿"><a href="#ä¸ºä»€ä¹ˆä½¿ç”¨æ¨¡æ¿" class="headerlink" title="ä¸ºä»€ä¹ˆä½¿ç”¨æ¨¡æ¿"></a>ä¸ºä»€ä¹ˆä½¿ç”¨æ¨¡æ¿</h3><p>å› ä¸ºç®€å•å¿«é€Ÿ,è‡ªå·±å¼€å‘çš„è¯ã€‚ã€‚ã€‚å°±è¦ç³»ç»Ÿå­¦ä¹ äº†ã€‚æ€»ä¹‹å…³äºŽæ¨¡æ¿çš„é€‰æ‹©æˆ‘ä»¬å¯ä»¥å‚è€ƒçŸ¥ä¹Žçš„ä¹‹ä¸€ç¯‡å›žç­”:<a href="https://www.zhihu.com/question/24422335" target="_blank" rel="noopener">æœ‰å“ªäº›å¥½çœ‹çš„ Hexo ä¸»é¢˜</a>, ä»¥åŠå®˜ç½‘<a href="https://hexo.io/themes/ã€‚" target="_blank" rel="noopener">https://hexo.io/themes/ã€‚</a><br>æˆ‘é€‰ç”¨çš„æ˜¯æˆ‘ä¸ªäººè§‰å¾—è¿˜è›®å¥½çš„Nextä¸»é¢˜,å¤§è‡´æ ·å­å¯ä»¥å‚ç…§æˆ‘çŽ°åœ¨çš„åšå®¢ã€‚</p>
<h3 id="éƒ¨ç½²æ¨¡æ¿"><a href="#éƒ¨ç½²æ¨¡æ¿" class="headerlink" title="éƒ¨ç½²æ¨¡æ¿"></a>éƒ¨ç½²æ¨¡æ¿</h3><p>æ‰¾åˆ°æˆ‘ä»¬å–œæ¬¢çš„æ¨¡æ¿åŽ,åœ¨ç»ˆç«¯çª—å£ä¸‹,å®šä½åˆ° Hexo ç«™ç‚¹ç›®å½•ä¸‹,ä¹Ÿå°±æ˜¯blogæ–‡ä»¶å¤¹å†…ã€‚ä½¿ç”¨ Git checkout ä»£ç :</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ git clone https:&#x2F;&#x2F;github.com&#x2F;iissnan&#x2F;hexo-theme-next themes&#x2F;next</span><br></pre></td></tr></table></figure>
<p>ç­‰å¾…ä¸‹è½½å®ŒæˆåŽæˆ‘ä»¬å¯ä»¥åœ¨ç«™ç‚¹ç›®å½•ä¸‹çš„themesçœ‹åˆ°å¤šå‡ºæ¥ä¸€ä¸ªnextæ–‡ä»¶å¤¹,è¿™å°±æ˜¯æˆ‘ä»¬çš„ä¸»é¢˜æ‰€åœ¨ä½ç½®äº†ã€‚æŽ¥ç€æˆ‘ä»¬éœ€è¦åœ¨ç«™ç‚¹é…ç½®æ–‡ä»¶é‡Œä¿®æ”¹æˆ‘ä»¬çš„ä¸»é¢˜(æ³¨æ„åœ¨æˆ‘ä»¬çš„ä¸»é¢˜,ä¹Ÿå°±æ˜¯nextæ–‡ä»¶å¤¹é‡Œé¢ä¹Ÿæœ‰ä¸€ä¸ªåŒæ ·åå­—çš„é…ç½®æ–‡ä»¶,é‚£æ˜¯ä¸»é¢˜é…ç½®æ–‡ä»¶,ä¸è¦æžé”™äº†ã€‚)</p>
<p><img src="/2020/01/21/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE4-%E9%80%89%E6%8B%A9%E5%8D%9A%E5%AE%A2%E4%B8%BB%E9%A2%98/1.png" alt="moban"></p>
<p>æŠŠä¸»é¢˜å¦‚å›¾ä¿®æ”¹å®ŒæˆåŽ,æˆ‘ä»¬è¾“å…¥:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure>
<p>å°±å¯ä»¥éƒ¨ç½²æˆ‘ä»¬æ–°çš„ä¸»é¢˜ä¸Šçº¿äº†,å½“ç„¶ä¹Ÿå¯ä»¥ä½¿ç”¨hexo serveråœ¨æœ¬åœ°æŸ¥çœ‹ä¸»é¢˜çš„å˜åŒ–,ä¸æŽ¨é€ä¸Šçº¿ã€‚<br>æˆ‘ä»¬å¯ä»¥ä¸‹è½½ä¸æ­¢ä¸€ä¸ªæ¨¡æ¿æ¥æŸ¥çœ‹ä»–ä»¬æ˜¯å¦é€‚åˆæˆ‘ä»¬,åªéœ€è¦é‡å¤å¦‚ä¸Šæ­¥éª¤ä¸‹è½½ä¸»é¢˜åˆ°themesæ–‡ä»¶å¤¹å†…,ç„¶åŽä¿®æ”¹ç«™ç‚¹é…ç½®æ–‡ä»¶å°±è¡Œäº†ã€‚<br>å¸Œæœ›å¤§å®¶å¯ä»¥æ‰¾åˆ°è‡ªå·±å¿ƒä»ªçš„ç½‘ç«™ä¸»é¢˜!!!</p>
<hr>
<p>ä¸Šä¸€æ­¥: <a href="https://www.guojingde.cn/2020/01/21/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE3-%E7%94%B3%E8%AF%B7%E5%B9%B6%E7%BB%91%E5%AE%9A%E5%9F%9F%E5%90%8D/#more" target="_blank" rel="noopener">ä¸ªäººåšå®¢å»ºè®¾3-ç”³è¯·å¹¶ç»‘å®šåŸŸå</a><br>ä¸‹ä¸€æ­¥: <a href="https://www.guojingde.cn/2020/01/21/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE5-Markdown%E8%AF%AD%E6%B3%95/" target="_blank" rel="noopener">ä¸ªäººåšå®¢å»ºè®¾5-Markdownè¯­æ³•</a><br>ç›®å½•: <a href="https://www.guojingde.cn/2020/01/19/%E7%94%A8GitHub%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/" target="_blank" rel="noopener">ç”¨GitHubæ­å»ºä¸ªäººåšå®¢(ç›®å½•)</a></p>
</font>]]></content>
      <categories>
        <category>åšå®¢æ­å»º</category>
      </categories>
      <tags>
        <tag>åšå®¢æ­å»º</tag>
      </tags>
  </entry>
  <entry>
    <title>ä¸ªäººåšå®¢å»ºè®¾3-ç”³è¯·å¹¶ç»‘å®šåŸŸå</title>
    <url>/2020/01/21/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE3-%E7%94%B3%E8%AF%B7%E5%B9%B6%E7%BB%91%E5%AE%9A%E5%9F%9F%E5%90%8D/</url>
    <content><![CDATA[<font size="3">
ä¸ªäººåšå®¢å»ºè®¾ç¬¬ä¸‰æ­¥:ç”³è¯·å¹¶ç»‘å®šåŸŸå...

<a id="more"></a>
<hr>
<h3 id="é˜¿é‡Œäº‘ç”³è¯·åŸŸå"><a href="#é˜¿é‡Œäº‘ç”³è¯·åŸŸå" class="headerlink" title="é˜¿é‡Œäº‘ç”³è¯·åŸŸå"></a>é˜¿é‡Œäº‘ç”³è¯·åŸŸå</h3><p>ä¸ºä»€ä¹ˆè¦ç”³è¯·ä¸€ä¸ªåŸŸå?å› ä¸ºé…·ç‚«å•Š!è¿™é‡Œå¯ä»¥åœ¨ä»»ä½•åœ°æ–¹ç”³è¯·åŸŸå,çŽ°åœ¨å¤§å¤šæ•°å¹³å°éƒ½æœ‰äº‘è§£æžæœåŠ¡æ‰€ä»¥æ— æ‰€è°“ã€‚è¿™é‡Œä»¥<a href="https://wanwang.aliyun.com/" target="_blank" rel="noopener">é˜¿é‡Œäº‘</a>ä¸¾ä¾‹å­ã€‚</p>
<p><img src="/2020/01/21/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE3-%E7%94%B3%E8%AF%B7%E5%B9%B6%E7%BB%91%E5%AE%9A%E5%9F%9F%E5%90%8D/1.png" alt="github"></p>
<p>è¾“å…¥æƒ³è¦çš„åŸŸåç„¶åŽæœç´¢,å¯ä»¥é€‰æ‹©è¿˜æœªè¢«æ³¨å†Œçš„åŸŸåã€‚.comæ˜¯å›½é™…åŸŸå.cnæ˜¯å›½å†…åŸŸå,äºŒè€…æ²¡æœ‰å®žè´¨åŒºåˆ«,å½“ç„¶ä¹Ÿæœ‰äº›ç‰¹åˆ«çš„é€‰é¡¹â€¦</p>
<p>ä¹°å¥½åŸŸååŽæˆ‘ä»¬å°±è¦è¿›è¡Œè§£æž,ç™»å½•é˜¿é‡Œäº‘åŽæˆ‘ä»¬å°±å¯ä»¥å¼€å§‹è§£æžäº†ã€‚</p>
<p><img src="/2020/01/21/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE3-%E7%94%B3%E8%AF%B7%E5%B9%B6%E7%BB%91%E5%AE%9A%E5%9F%9F%E5%90%8D/3.jpg" alt="github"></p>
<p>ç‚¹è¿›æˆ‘ä»¬çš„åŸŸååŽæ¥åˆ°è§£æžè®¾ç½®,æ·»åŠ ä¸¤æ¡Aè®°å½•å’Œä¸€æ¡CNAMEè®°å½•(çº¢åœˆ),Aè®°å½•çš„è®°å½•å€¼æ˜¯å›ºå®šçš„192.30.252.153å’Œ190.30.252.154,ä¸ºGitHubåœ°å€,CNAMEè®°å½•ä¸º: ç”¨æˆ·å.github.io çš„å½¢å¼ã€‚</p>
<p>è¿™é‡ŒAè®°å½•å¯å®žçŽ°å°†åŸŸåæŒ‡å‘ IP åœ°å€,å½“éœ€è¦å°†åŸŸåæŒ‡å‘å¦ä¸€ä¸ªåŸŸå,å†ç”±å¦ä¸€ä¸ªåŸŸåæä¾› IP åœ°å€,å°±éœ€è¦æ·»åŠ  CNAME è®°å½•ã€‚</p>
<p>æ·»åŠ å®Œè®°å½•åŽæ‰“å¼€GitHubè¿›å…¥æˆ‘ä»¬ä¹‹å‰è®¾ç½®åšå®¢çš„ä»“åº“,ç‚¹å‡»settings(è®¾ç½®),æˆ‘ä»¬æ‹‰åˆ°é¡µé¢åä¸‹æ–¹ã€‚</p>
<p><img src="/2020/01/21/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE3-%E7%94%B3%E8%AF%B7%E5%B9%B6%E7%BB%91%E5%AE%9A%E5%9F%9F%E5%90%8D/4.png" alt="github"></p>
<p>è¾“å…¥ä½ ç”³è¯·çš„åŸŸåç„¶åŽä¿å­˜ã€‚</p>
<p>ç»ˆäºŽæœ€åŽä¸€æ­¥äº†,å®Œæˆè¿™æ­¥ä¹‹åŽä½ çš„åšå®¢å°±å¯ä»¥é€šè¿‡åŸŸåè®¿é—®äº†ã€‚å†·é™ä¸€ä¸‹,æˆ‘ä»¬æ‰“å¼€æœ¬åœ°ä¿å­˜åšå®¢çš„æ–‡ä»¶å¤¹,è¿›å…¥sourceæ–‡ä»¶å¤¹åˆ›å»ºä¸€ä¸ªç©ºç™½è®°äº‹æœ¬åœ¨é‡Œé¢è¾“å…¥ä½ ç”³è¯·çš„åŸŸå,ç„¶åŽæŠŠå®ƒä¿å­˜ä¸ºæ‰€æœ‰æ–‡ä»¶æ ¼å¼å¹¶ä¸”å‘½åä¸ºCNAMEã€‚</p>
<p><img src="/2020/01/21/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE3-%E7%94%B3%E8%AF%B7%E5%B9%B6%E7%BB%91%E5%AE%9A%E5%9F%9F%E5%90%8D/5.png" alt="github"></p>
<p>ç„¶åŽå¸¸è§„æ“ä½œg+då°±å¯ä»¥ä½¿ç”¨è‡ªå·±çš„åŸŸåç™»å½•ç½‘ç«™å•¦ã€‚</p>
<hr>
<p>ä¸Šä¸€æ­¥: <a href="https://www.guojingde.cn/2020/01/20/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE2-%E5%AE%89%E8%A3%85%E5%B9%B6%E9%85%8D%E7%BD%AENode.js%E4%BB%A5%E5%8F%8AHexo/" target="_blank" rel="noopener">ä¸ªäººåšå®¢å»ºè®¾2-å®‰è£…å¹¶é…ç½®Node.jsä»¥åŠHexo</a><br>ä¸‹ä¸€æ­¥: <a href="https://www.guojingde.cn/2020/01/21/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE4-%E9%80%89%E6%8B%A9%E5%8D%9A%E5%AE%A2%E4%B8%BB%E9%A2%98/" target="_blank" rel="noopener">ä¸ªäººåšå®¢å»ºè®¾4-é€‰æ‹©åšå®¢ä¸»é¢˜</a><br>ç›®å½•: <a href="https://www.guojingde.cn/2020/01/19/%E7%94%A8GitHub%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/" target="_blank" rel="noopener">ç”¨GitHubæ­å»ºä¸ªäººåšå®¢(ç›®å½•)</a></p>
</font>]]></content>
      <categories>
        <category>åšå®¢æ­å»º</category>
      </categories>
      <tags>
        <tag>åšå®¢æ­å»º</tag>
      </tags>
  </entry>
  <entry>
    <title>ä¸ªäººåšå®¢å»ºè®¾2:å®‰è£…å¹¶é…ç½®Node.jsä»¥åŠHexo</title>
    <url>/2020/01/20/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE2-%E5%AE%89%E8%A3%85%E5%B9%B6%E9%85%8D%E7%BD%AENode.js%E4%BB%A5%E5%8F%8AHexo/</url>
    <content><![CDATA[<font size="3">
ä¸ªäººåšå®¢å»ºè®¾ç¬¬äºŒæ­¥:å®‰è£…å¹¶é…ç½®æ‰€éœ€æ¡†æž¶å’ŒçŽ¯å¢ƒ...

<a id="more"></a>
<hr>
<h3 id="å®‰è£…-Node-js"><a href="#å®‰è£…-Node-js" class="headerlink" title="å®‰è£… Node.js"></a>å®‰è£… Node.js</h3><p>Node.jså…è®¸é€šè¿‡JavaScriptå’Œä¸€ç³»åˆ—æ¨¡å—æ¥ç¼–å†™æœåŠ¡å™¨ç«¯åº”ç”¨å’Œç½‘ç»œç›¸å…³çš„råº”ç”¨ã€‚</p>
<p>æˆ‘ä»¬é€šè¿‡å®˜ç½‘<a href="https://nodejs.org/en/download/æ¥é€‰æ‹©åˆé€‚çš„ç‰ˆæœ¬è¿›è¡Œä¸‹è½½,å¹¶ä¸”å®‰è£…ã€‚å®‰è£…å®ŒæˆåŽå†windowå‘½ä»¤è¡Œæˆ–è€…windowsè‡ªå¸¦çš„powershellè¾“å…¥" target="_blank" rel="noopener">https://nodejs.org/en/download/æ¥é€‰æ‹©åˆé€‚çš„ç‰ˆæœ¬è¿›è¡Œä¸‹è½½,å¹¶ä¸”å®‰è£…ã€‚å®‰è£…å®ŒæˆåŽå†windowå‘½ä»¤è¡Œæˆ–è€…windowsè‡ªå¸¦çš„powershellè¾“å…¥</a>:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">node -v</span><br></pre></td></tr></table></figure>
<p>å‡ºçŽ°å¯¹åº”ç‰ˆæœ¬å·è¯´æ˜Žå®‰è£…æˆåŠŸ,npmä¼šéšç€å®‰è£…åŒ…è‡ªåŠ¨å®‰è£…,è¾“å…¥:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm -v</span><br></pre></td></tr></table></figure>
<p>åŒæ ·å‡ºçŽ°å¯¹åº”ç‰ˆæœ¬å·è¯´æ˜Žå®‰è£…æˆåŠŸã€‚åˆ°è¿™é‡Œå°±å®Œæˆæ¥äº†çŽ¯å¢ƒå®‰è£…ã€‚</p>
<h3 id="å®‰è£…Hexo"><a href="#å®‰è£…Hexo" class="headerlink" title="å®‰è£…Hexo"></a>å®‰è£…Hexo</h3><p>Hexoæ˜¯ä¸€ä¸ªå¿«é€Ÿã€ç®€æ´ä¸”é«˜æ•ˆçš„åšå®¢æ¡†æž¶ã€‚ Hexoä½¿ç”¨Markdownè§£æžæ–‡ç« ,åœ¨å‡ ç§’å†…,å³å¯åˆ©ç”¨é“ä¸½çš„ä¸»é¢˜ç”Ÿæˆé™æ€ç½‘é¡µã€‚</p>
<p>ç¬¬äºŒæ­¥å°±æ˜¯å®‰è£…è¿™ä¸ªåšå®¢çš„æ¡†æž¶,åŒæ ·æ‰“å¼€å‘½ä»¤è¡Œè¾“å…¥:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br></pre></td></tr></table></figure>
<p>æˆ‘ä»¬å°±å¼€å§‹å®‰è£…Hexoäº†,ç­‰å¾…ä»–å®‰è£…å®ŒåŽæˆ‘ä»¬ä½¿ç”¨å‘½ä»¤è¡Œç§»åŠ¨åˆ°æˆ‘ä»¬å¸Œæœ›ä¿å­˜æˆ‘ä»¬åšå®¢æ‰€æœ‰å†…å®¹çš„æ–‡ä»¶å¤¹å†…ã€‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &lt;å½“å‰ç›®å½•ä¸‹æ–‡ä»¶å¤¹åå­—&gt;</span><br></pre></td></tr></table></figure>
<p>ä½¿ç”¨å¦‚ä¸Šå‘½ä»¤å°±å¯ä»¥ç§»åŠ¨è‡³æˆ‘ä»¬å¸Œæœ›åšå®¢é¡¹ç›®ä¿å­˜çš„åœ°æ–¹ã€‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo init blog</span><br></pre></td></tr></table></figure>
<p>æŽ¥ç€ä½¿ç”¨å¦‚ä¸Šå‘½ä»¤å°±å¯ä»¥åˆå§‹åŒ–æˆ‘ä»¬çš„åšå®¢é¡¹ç›®äº†ã€‚è‡³æ­¤æˆ‘ä»¬å°±å¯ä»¥åœ¨æœ¬åœ°çœ‹æˆ‘ä»¬çš„ç½‘ç«™é›å½¢äº†ã€‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo new stest_my_site</span><br><span class="line">hexo g </span><br><span class="line">hexo s</span><br></pre></td></tr></table></figure>
<p>æŒ‰ç…§é¡ºåºè¾“å…¥å¦‚ä¸Šå‘½ä»¤,å°±ä¼šå¼¹å‡ºæç¤º,æ­¤æ—¶æˆ‘ä»¬åœ¨æµè§ˆå™¨è®¿é—®localhost:4000å°±å¯ä»¥çœ‹è§æˆ‘ä»¬åšå®¢çš„é›å½¢äº†ã€‚<br>hexo s = hexo server å¯åŠ¨æœåŠ¡é¢„è§ˆ<br>hexo g = hexo generate ç”Ÿæˆ<br>å…¶ä»–hexoå‘½ä»¤:<br>hexo n â€œåšå®¢åç§°â€  =&gt; hexo new â€œåšå®¢åç§°â€   #è¿™ä¸¤ä¸ªéƒ½æ˜¯åˆ›å»ºæ–°æ–‡ç« ,å‰è€…æ˜¯ç®€å†™æ¨¡å¼<br>hexo clean   #æ¸…é™¤ç¼“å­˜,ç½‘é¡µæ­£å¸¸æƒ…å†µä¸‹å¯ä»¥å¿½ç•¥æ­¤æ¡å‘½ä»¤<br>hexo d  =&gt; hexo deploy  #éƒ¨ç½²ç½‘ç«™ä¸Šçº¿ (æŽ¨é€åˆ°GitHub)</p>
<p><img src="/2020/01/20/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE2-%E5%AE%89%E8%A3%85%E5%B9%B6%E9%85%8D%E7%BD%AENode.js%E4%BB%A5%E5%8F%8AHexo/1.jpg" alt="github"></p>
<p>è¿™æ—¶å€™æˆ‘ä»¬å¯ä»¥çœ‹åˆ°blogæ–‡ä»¶å¤¹å†…å¤§è‡´æ˜¯å›¾ä¸­çš„æ ·å­,è¿™æ—¶å€™æˆ‘ä»¬æ‰“å¼€ç«™ç‚¹é…ç½®æ–‡ä»¶,å¯ä»¥ä½¿ç”¨<a href="https://code.visualstudio.com/" target="_blank" rel="noopener">Visual Studio Code</a>æ¥ç¼–è¾‘ã€‚</p>
<p><img src="/2020/01/20/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE2-%E5%AE%89%E8%A3%85%E5%B9%B6%E9%85%8D%E7%BD%AENode.js%E4%BB%A5%E5%8F%8AHexo/2.png" alt="github"></p>
<p>æˆ‘ä»¬ç¿»åˆ°å¦‚å›¾ä¸­çš„éƒ¨ç½²é€‰é¡¹ä¸‹,æŠŠä¹‹å‰åœ¨GitHubåˆ›å»ºçš„åº“å®Œæ•´è¿žæŽ¥å¤åˆ¶ç²˜è´´ä¸Šå¹¶åœ¨ç»“å°¾åŠ ä¸Š.gitã€‚(xxx.github.io.git )<br>ç„¶åŽåœ¨å‘½ä»¤è¡Œè¾“å…¥å¦‚ä¸‹å‘½ä»¤æ¥å®‰è£…éƒ¨ç½²æ¨¡å—,</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure>
<p>ç„¶åŽå¸¸è§„æ“ä½œg+s,å°±å¯ä»¥åœ¨æµè§ˆå™¨é€šè¿‡è®¿é—®xxx.github.ioæ¥è¿›å…¥ä¸ªäººä¸»é¡µäº†ã€‚<br>ä½†æ˜¯è¿™ä¸ªä¼¼ä¹Žæœ‰ç‚¹å‚»,è¿™æ—¶å€™æˆ‘ä»¬éœ€è¦ä¸€ä¸ªå¸…æ°”çš„åŸŸå!<br>äºŽæ˜¯ä¸‹ä¸€ç¯‡å°±æ˜¯:ç”³è¯·å¹¶ç»‘å®šåŸŸå</p>
<hr>
<p>ä¸Šä¸€æ­¥: <a href="https://www.guojingde.cn/2020/01/20/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE1-%E5%9C%A8GitHub%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E4%BB%93%E5%BA%93%E5%B9%B6%E9%85%8D%E7%BD%AEGit/" target="_blank" rel="noopener">ä¸ªäººåšå®¢å»ºè®¾1-åœ¨GitHubåˆ›å»ºä¸€ä¸ªä»“åº“å¹¶é…ç½®Git</a></p>
<p>ä¸‹ä¸€æ­¥: <a href="https://www.guojingde.cn/2020/01/21/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE3-%E7%94%B3%E8%AF%B7%E5%B9%B6%E7%BB%91%E5%AE%9A%E5%9F%9F%E5%90%8D/#more" target="_blank" rel="noopener">ä¸ªäººåšå®¢å»ºè®¾3-ç”³è¯·å¹¶ç»‘å®šåŸŸå</a></p>
<p>ç›®å½•: <a href="https://www.guojingde.cn/2020/01/19/%E7%94%A8GitHub%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/" target="_blank" rel="noopener">ç”¨GitHubæ­å»ºä¸ªäººåšå®¢(ç›®å½•)</a></p>
</font>]]></content>
      <categories>
        <category>åšå®¢æ­å»º</category>
      </categories>
      <tags>
        <tag>åšå®¢æ­å»º</tag>
      </tags>
  </entry>
  <entry>
    <title>Gitçš„å‡ ä¸ªåŸºæœ¬æŒ‡ä»¤</title>
    <url>/2020/01/20/Git%E7%9A%84%E5%87%A0%E4%B8%AA%E5%9F%BA%E6%9C%AC%E6%8C%87%E4%BB%A4/</url>
    <content><![CDATA[<p>Gitä»¥åŠGitçš„å‡ ä¸ªæœ€å¸¸ç”¨çš„æŒ‡ä»¤,åˆæ­¥äº†è§£ç‰ˆæœ¬ç®¡ç†ã€‚</p>
<a id="more"></a>
<hr>
<h1 id="ä»€ä¹ˆæ˜¯Gitä»¥åŠä¸ºä½•ä½¿ç”¨Git"><a href="#ä»€ä¹ˆæ˜¯Gitä»¥åŠä¸ºä½•ä½¿ç”¨Git" class="headerlink" title="ä»€ä¹ˆæ˜¯Gitä»¥åŠä¸ºä½•ä½¿ç”¨Git?"></a>ä»€ä¹ˆæ˜¯Gitä»¥åŠä¸ºä½•ä½¿ç”¨Git?</h1><p>Git æ˜¯ç›®å‰ä½¿ç”¨äººæ•°æœ€å¤šçš„ç‰ˆæœ¬ç®¡ç†å·¥å…·ã€‚ä¸ºä»€ä¹ˆä¼šæœ‰Gitå‘¢?å› ä¸ºè¦ç‰ˆæœ¬æŽ§åˆ¶ã€‚ ä¸¾ä¸€ä¸ªä¾‹å­,å°å¾·è¦å†™è®ºæ–‡é‚£ä¹ˆä»–å°±å¯èƒ½è¦ä¿®æ”¹å¥½å‡ æ¬¡,è¿™æ—¶å€™æ¯”èµ·åœ¨ä¸€ä¸ªæ–‡ä»¶é‡Œä¿®æ”¹å°å¾·å¯èƒ½ä¼šå¤åˆ¶ä¸€ä¸ªæ–°çš„è®ºæ–‡æ–‡ä»¶åœ¨æ–°æ–‡ä»¶é‡Œä¿®æ”¹ã€‚è¿™æ—¶å€™å°±å¯èƒ½ä¼šæœ‰å¥½å‡ ä¸ªç‰ˆæœ¬çš„è®ºæ–‡ã€‚å…¶å®žå¹³æ—¶ä½¿ç”¨çš„å¿«æ·é”®ctrl+zçš„æ’¤å›žå°±æ˜¯æœ€ç®€å•çš„ç‰ˆæœ¬ç®¡ç†ã€‚</p>
<p>æƒ…å†µå†å¤æ‚äº›,å°å¾·å¯èƒ½åšçš„æ˜¯ä¸€ä¸ªå°ç»„ä½œä¸š,å¤§å®¶éƒ½æœ‰å„è‡ªè¦åšçš„éƒ¨åˆ†,é‚£ä¹ˆå¥½çŽ©çš„æƒ…å†µå°±æ˜¯å¤§å®¶åšå®Œä¸€éƒ¨åˆ†åŽå°±è¦åœ¨å¾®ä¿¡æˆ–è€…é‚®ç®±é‡Œä¼ æ¥ä¼ åŽ»,è€Œæœ‰äº†Git ä¸€ä¸ªå›¢é˜Ÿå°±å¯ä»¥è½»æ¾åˆä½œã€‚æƒ³è¦æ›´åŠ æ·±å…¥äº†è§£ä»€ä¹ˆæ˜¯Git çš„å¯ä»¥è®¿é—® <a href="http://blog.a0z.me/2014/05/21/GitBeginning/" target="_blank" rel="noopener">Ghosty Core çš„åšå®¢</a>ã€‚</p>
<p>Git å®˜ç½‘: <a href="https://git-scm.com/" target="_blank" rel="noopener">https://git-scm.com/</a></p>
<h1 id="Git-çš„åŸºæœ¬æ“ä½œ"><a href="#Git-çš„åŸºæœ¬æ“ä½œ" class="headerlink" title="Git çš„åŸºæœ¬æ“ä½œ"></a>Git çš„åŸºæœ¬æ“ä½œ</h1><h3 id="1-å‘½ä»¤è¡ŒåŸºæœ¬æ“ä½œ-å’ŒLinuxä¸€æ ·"><a href="#1-å‘½ä»¤è¡ŒåŸºæœ¬æ“ä½œ-å’ŒLinuxä¸€æ ·" class="headerlink" title="1. å‘½ä»¤è¡ŒåŸºæœ¬æ“ä½œ(å’ŒLinuxä¸€æ ·)"></a>1. å‘½ä»¤è¡ŒåŸºæœ¬æ“ä½œ(å’ŒLinuxä¸€æ ·)</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd blog # åˆ‡æ¢åˆ°å½“å‰ç›®å½•ä¸‹å«blogçš„æ–‡ä»¶å¤¹</span><br><span class="line">cd .. # é€€å›žä¸Šä¸€çº§ç›®å½•</span><br><span class="line">ls # æŸ¥çœ‹å½“å‰ç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶å’Œæ–‡ä»¶å¤¹</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd # è¿”å›žhomeç›®å½•~</span><br><span class="line">ls -ah # æŸ¥çœ‹å½“å‰ç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶,æ–‡ä»¶å¤¹ä»¥åŠéšè—æ–‡ä»¶</span><br><span class="line">pwd # æŸ¥çœ‹å½“å‰è·¯å¾„</span><br><span class="line">touch balabala.py # åœ¨å½“å‰ç›®å½•åˆ›å»ºä¸€ä¸ªåå­—å«åšbalabalaçš„pythonæ–‡ä»¶</span><br><span class="line">mkdir blog # åœ¨å½“å‰ç›®å½•åˆ›å»ºä¸€ä¸ªå«blogçš„æ–‡ä»¶å¤¹</span><br><span class="line">rm balabala.py # åˆ é™¤è¯¥æ–‡ä»¶</span><br><span class="line">rm -r blog # åˆ é™¤è¯¥æ–‡ä»¶å¤¹</span><br><span class="line">mv &#123;src&#125; &#123;dest&#125; # ç§»åŠ¨æ–‡ä»¶,å¦‚æžœ dest æ˜¯ç›®å½•,åˆ™ç§»åŠ¨,æ˜¯æ–‡ä»¶ååˆ™è¦†ç›–</span><br></pre></td></tr></table></figure>
<p>è¦ä½¿ç”¨gitæŽŒæ¡å‰ä¸‰ä¸ªå¯ä»¥æŠŠç›®å½•ç§»åˆ°å·¥ä½œç›®å½•initå°±è¡Œäº†â€¦ ä¸‹é¢çš„ä½œä¸ºè¡¥å……, ä¸‡ä¸€å“ªå¤©æ²¡æœ‰å›¾å½¢åŒ–ç•Œé¢åªæœ‰ä¸ªå‘½ä»¤è¡Œå‘¢ã€‚</p>
<h3 id="2-åˆ›å»ºä¸€ä¸ªä»“åº“-åˆå§‹åŒ–"><a href="#2-åˆ›å»ºä¸€ä¸ªä»“åº“-åˆå§‹åŒ–" class="headerlink" title="2. åˆ›å»ºä¸€ä¸ªä»“åº“,åˆå§‹åŒ–"></a>2. åˆ›å»ºä¸€ä¸ªä»“åº“,åˆå§‹åŒ–</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git init</span><br></pre></td></tr></table></figure>
<p>åœ¨ä¸€ä¸ªå·¥ä½œç›®å½•ä¸‹è¾“å…¥åŽ,å°±å®Œæˆäº†åˆå§‹åŒ–,å¯ä»¥çœ‹åˆ°éšè—æ–‡ä»¶æœ‰gitæ–‡ä»¶å¤¹ã€‚æˆ‘ä»¬å°±æœ‰äº†ä¸€ä¸ªä»“åº“ã€‚</p>
<h3 id="3-æŸ¥çœ‹ç›®å‰çŠ¶æ€"><a href="#3-æŸ¥çœ‹ç›®å‰çŠ¶æ€" class="headerlink" title="3. æŸ¥çœ‹ç›®å‰çŠ¶æ€"></a>3. æŸ¥çœ‹ç›®å‰çŠ¶æ€</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git status</span><br></pre></td></tr></table></figure>
<p>ä¼šæç¤ºä½ å½“å‰åº“çš„çŠ¶æ€,è¿½è¸ªçš„æ–‡ä»¶,æœªè¿½è¸ªçš„æ–‡ä»¶,åšè¿‡ä¿®æ”¹æœªcommit(æ³¨é‡Š)çš„æ–‡ä»¶ã€‚</p>
<h3 id="4-æäº¤ä¿®æ”¹"><a href="#4-æäº¤ä¿®æ”¹" class="headerlink" title="4. æäº¤ä¿®æ”¹"></a>4. æäº¤ä¿®æ”¹</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git add &lt;file&gt; # æŒ‡å®šä¸€ä¸ªæ–‡ä»¶æäº¤</span><br><span class="line">git add . # æäº¤æ‰€æœ‰å½“å‰ç›®å½•ä¸‹æ–‡ä»¶</span><br></pre></td></tr></table></figure>
<p>ä¿®æ”¹æ–‡ä»¶å®Œé¦–å…ˆè¦æŠŠæ–‡ä»¶æŽ¨é€åˆ°staging index,é€‰æ‹©è¦æŽ¨é€çš„æ–‡ä»¶æˆ–å…¨éƒ¨</p>
<h3 id="5-commit-æ³¨é‡Š"><a href="#5-commit-æ³¨é‡Š" class="headerlink" title="5. commit(æ³¨é‡Š)"></a>5. commit(æ³¨é‡Š)</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git commit -m &quot;descriptions&quot; # ç›´æŽ¥æ³¨é‡Š</span><br><span class="line">git commit æ‰“å¼€ç¼–è¾‘å™¨è¿›è¡Œæ³¨é‡Š</span><br><span class="line">git commit --amend å¯¹æœ€è¿‘ä¸€æ¬¡çš„æäº¤åšå†…å®¹ä¿®æ”¹</span><br></pre></td></tr></table></figure>
<p>æŽ¨é€åˆ°staging indexåŽéœ€è¦æ·»åŠ æ³¨é‡Š,å¯ä»¥ç›´æŽ¥æ³¨é‡Šä¹Ÿå¯ä»¥é€‰æ‹©è‡ªå·±å–œæ¬¢çš„æ–‡æœ¬ç¼–è¾‘å™¨è¿›è¡Œæ³¨é‡Š,Linuxä¸‹çš„è¯å¯ä»¥æ˜¯vimå’Œnanoç­‰,Windowsä¸‹å¯ä»¥æ˜¯Notepad++ç­‰ã€‚æ³¨é‡Šå®ŒåŽæ–‡ä»¶å°±ä»Žstaging index æŽ¨é€åˆ° repoä»“åº“äº†ã€‚</p>
<h3 id="6-å…‹éš†GitHubè¿œç¨‹ä»“åº“"><a href="#6-å…‹éš†GitHubè¿œç¨‹ä»“åº“" class="headerlink" title="6. å…‹éš†GitHubè¿œç¨‹ä»“åº“"></a>6. å…‹éš†GitHubè¿œç¨‹ä»“åº“</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone &lt;remote address&gt;</span><br></pre></td></tr></table></figure>
<p>å¯ä»¥ä¸‹è½½è¿œç¨‹Githubçš„ä»“åº“åˆ°æœ¬åœ°,ä¾‹å¦‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;JasonVictor17&#x2F;Housing.git</span><br></pre></td></tr></table></figure>
<p>å°±å¯ä»¥å¤åˆ¶æˆ‘çš„ä¸€ä¸ªrepoåˆ°æœ¬åœ°äº†ã€‚</p>
<h3 id="7-å…³è”å¹¶æŽ¨é€åˆ°Githubè¿œç¨‹ä»“åº“"><a href="#7-å…³è”å¹¶æŽ¨é€åˆ°Githubè¿œç¨‹ä»“åº“" class="headerlink" title="7. å…³è”å¹¶æŽ¨é€åˆ°Githubè¿œç¨‹ä»“åº“"></a>7. å…³è”å¹¶æŽ¨é€åˆ°Githubè¿œç¨‹ä»“åº“</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git remote add origin &lt;remote address&gt;	åœ¨æœ¬åœ°å·¥ä½œåŒºç›®å½•ä¸‹æŒ‰ç…§ GitHub æç¤ºè¿›è¡Œå…³è”</span><br><span class="line">git remote rm origin			è§£é™¤é”™è¯¯å…³è”</span><br><span class="line">git push -u origin master		ç¬¬ä¸€æ¬¡å°†æœ¬åœ°ä»“åº“æŽ¨é€è‡³è¿œç¨‹ä»“åº“(æ¯æ¬¡åœ¨æœ¬åœ°æäº¤åŽè¿›è¡Œæ“ä½œ)</span><br><span class="line">git push origin master			ä»¥åŽæ¯æ¬¡å°†æœ¬åœ°ä»“åº“æŽ¨é€è‡³è¿œç¨‹ä»“åº“(æ¯æ¬¡åœ¨æœ¬åœ°æäº¤åŽè¿›è¡Œæ“ä½œ)</span><br></pre></td></tr></table></figure>

<p>ç¬¬ä¸€æ­¥è¦æŠŠæœ¬åœ°åº“å…³è”åˆ°ä¸€ä¸ªGitHubåº“,åœ¨GitHubåˆ›å»ºæ–°åº“æ—¶ä¼šæœ‰æç¤ºå¦‚ä½•æ“ä½œã€‚ç¬¬äºŒæ­¥å°±æ˜¯æŽ¨é€æœ¬åœ°åº“åˆ°è¿œç¨‹åº“,ç¬¬ä¸€æ¬¡æ“ä½œæœ‰æ‰€ä¸åŒä»¥åŽéƒ½ä¸€æ ·ã€‚</p>
<h3 id="8-æµç¨‹å›¾"><a href="#8-æµç¨‹å›¾" class="headerlink" title="8.æµç¨‹å›¾"></a>8.æµç¨‹å›¾</h3><p>è‡³æ­¤,ä¸€ä¸ªå¤§è‡´çš„ç»“æž„å°±å¯ä»¥å½¢æˆäº†ã€‚</p>
<p><img src="/2020/01/20/Git%E7%9A%84%E5%87%A0%E4%B8%AA%E5%9F%BA%E6%9C%AC%E6%8C%87%E4%BB%A4/1.png" alt="github"></p>
]]></content>
      <categories>
        <category>Git &amp; GitHub</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title>ä¸ªäººåšå®¢å»ºè®¾1:åœ¨GitHubåˆ›å»ºä¸€ä¸ªä»“åº“å¹¶é…ç½®Git</title>
    <url>/2020/01/20/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE1-%E5%9C%A8GitHub%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E4%BB%93%E5%BA%93%E5%B9%B6%E9%85%8D%E7%BD%AEGit/</url>
    <content><![CDATA[<font size="3">
ä¸ªäººåšå®¢å»ºè®¾ç¬¬ä¸€æ­¥:é…ç½®GitHub,å®‰è£…Git Bashå¹¶é…ç½®Git...

<a id="more"></a>
<hr>
<h3 id="é¦–å…ˆä»€ä¹ˆæ˜¯GitHub-ä»¥ä¸‹æ˜¯ç»´åŸºç™¾ç§‘çš„å®šä¹‰"><a href="#é¦–å…ˆä»€ä¹ˆæ˜¯GitHub-ä»¥ä¸‹æ˜¯ç»´åŸºç™¾ç§‘çš„å®šä¹‰" class="headerlink" title="é¦–å…ˆä»€ä¹ˆæ˜¯GitHub,ä»¥ä¸‹æ˜¯ç»´åŸºç™¾ç§‘çš„å®šä¹‰:"></a>é¦–å…ˆä»€ä¹ˆæ˜¯GitHub,ä»¥ä¸‹æ˜¯ç»´åŸºç™¾ç§‘çš„å®šä¹‰:</h3><p>GitHubæ˜¯é€šè¿‡Gitè¿›è¡Œç‰ˆæœ¬æŽ§åˆ¶çš„è½¯ä»¶æºä»£ç æ‰˜ç®¡æœåŠ¡å¹³å°ã€‚</p>
<h3 id="é‚£ä»€ä¹ˆæ˜¯Git"><a href="#é‚£ä»€ä¹ˆæ˜¯Git" class="headerlink" title="é‚£ä»€ä¹ˆæ˜¯Git?"></a>é‚£ä»€ä¹ˆæ˜¯Git?</h3><p>Gitæ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼ç‰ˆæœ¬æŽ§åˆ¶è½¯ä»¶,æœ€åˆç”±æž—çº³æ–¯Â·æ‰˜ç“¦å…¹åˆ›ä½œ,äºŽ2005å¹´ä»¥GPLå‘å¸ƒã€‚æœ€åˆç›®çš„æ˜¯ä¸ºæ›´å¥½åœ°ç®¡ç†Linuxå†…æ ¸å¼€å‘è€Œè®¾è®¡ã€‚</p>
<h3 id="çœ‹ä¸æ‡‚æ²¡å…³ç³»-åªè¦ä¼šä½¿ç”¨ä¸€æ ·å¯ä»¥æ­å»ºä¸ªäººåšå®¢ã€‚"><a href="#çœ‹ä¸æ‡‚æ²¡å…³ç³»-åªè¦ä¼šä½¿ç”¨ä¸€æ ·å¯ä»¥æ­å»ºä¸ªäººåšå®¢ã€‚" class="headerlink" title="çœ‹ä¸æ‡‚æ²¡å…³ç³»,åªè¦ä¼šä½¿ç”¨ä¸€æ ·å¯ä»¥æ­å»ºä¸ªäººåšå®¢ã€‚"></a>çœ‹ä¸æ‡‚æ²¡å…³ç³»,åªè¦ä¼šä½¿ç”¨ä¸€æ ·å¯ä»¥æ­å»ºä¸ªäººåšå®¢ã€‚</h3><p>é¦–å…ˆæˆ‘ä»¬è¦åˆ›å»ºä¸€ä¸ªGitHubçš„è´¦å·,ç½‘å€:<a href="https://github.com/" target="_blank" rel="noopener">https://github.com/</a><br>åˆ›å»ºå®ŒåŽæˆ‘ä»¬å°±éœ€è¦åˆ›å»ºä¸€ä¸ªæ–°çš„åº“,å…·ä½“å¦‚å›¾:</p>
<p><img src="/2020/01/20/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE1-%E5%9C%A8GitHub%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E4%BB%93%E5%BA%93%E5%B9%B6%E9%85%8D%E7%BD%AEGit/1.png" alt="github"></p>
<p>ç‚¹å‡»Repositories å†ç‚¹å‡» New,ç„¶åŽè¿›å…¥åˆ°ä¸‹ä¸ªç•Œé¢:</p>
<p><img src="/2020/01/20/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE1-%E5%9C%A8GitHub%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E4%BB%93%E5%BA%93%E5%B9%B6%E9%85%8D%E7%BD%AEGit/2.png" alt="github"></p>
<p>Repositories name æŒ‰ç…§å›ºå®šçš„æ ¼å¼å†™ username.github.io<br>è¿™é‡Œçš„usernameå°±æ˜¯ä½ æ³¨å†Œæ—¶å€™çš„ç”¨æˆ·å,åƒæˆ‘å°±æ˜¯JasonVictor17</p>
<p>åˆ°è¿™é‡ŒGithubå°±é…ç½®å¥½äº†,æˆ‘ä»¬æŽ¥ç€ä¸‹ä¸€ä¸ªGit Bashåˆ°Windowsç³»ç»Ÿä»¥ä¾¿ä½¿ç”¨Git,<br>ç½‘å€:<a href="https://git-scm.com/download/win" target="_blank" rel="noopener">https://git-scm.com/download/win</a> ä¼šè‡ªåŠ¨ä¸‹è½½</p>
<p>å®‰è£…æˆåŠŸåŽéœ€è¦é…ç½®Git Bash,æ‰“å¼€Git bashæˆ‘ä»¬çœ‹åˆ°å‘½ä»¤è¡Œ,è¾“å…¥:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git config --global user.name &quot;ä½ çš„GitHubç”¨æˆ·å&quot;</span><br><span class="line">git config --global user.email &quot;ä½ çš„GitHubæ³¨å†Œé‚®ç®±&quot;</span><br></pre></td></tr></table></figure>

<p>ç„¶åŽç”Ÿæˆä¸€ä¸ªSSHå¯†é’¥</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa -C &quot;ä½ çš„GitHubæ³¨å†Œé‚®ç®±&quot;</span><br></pre></td></tr></table></figure>

<p><img src="/2020/01/20/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE1-%E5%9C%A8GitHub%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E4%BB%93%E5%BA%93%E5%B9%B6%E9%85%8D%E7%BD%AEGit/4.jpg" alt="github"></p>
<p>é»˜è®¤å›žè½¦,ç„¶åŽç³»ç»Ÿä¼šè‡ªåŠ¨ç”Ÿæˆä¸€ä¸ªæ–‡ä»¶,ç„¶åŽåœ¨Cç›˜userä¸­æ‰¾åˆ°.sshæ–‡ä»¶å¤¹(è·¯å¾„å¦‚å›¾), å¹¶å¤åˆ¶å…¶ä¸­id_rsa.pubçš„å†…å®¹ã€‚ç„¶åŽæ‰“å¼€ <a href="https://github.com/settings/keys" target="_blank" rel="noopener">Githubä¸Šçš„ssh keyè®¾ç½®</a>ã€‚</p>
<p><img src="/2020/01/20/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE1-%E5%9C%A8GitHub%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E4%BB%93%E5%BA%93%E5%B9%B6%E9%85%8D%E7%BD%AEGit/3.png" alt="github"></p>
<p>ç‚¹å‡»new ssh keyåŽè¾“å…¥åç§°å’Œå¤åˆ¶çš„å†…å®¹ç¡®è®¤å°±å¯ä»¥äº†ã€‚</p>
<p>(å¦‚æžœæ‰¾ä¸åˆ°SSHå¯†åŒ™æ–‡ä»¶æœ‰ä¸¤ä¸ªåŠžæ³•:1. <a href="https://desktop.github.com/" target="_blank" rel="noopener">ä¸‹è½½GitHubæ¡Œé¢ç‰ˆ</a>å°±å¯ä»¥æ— éœ€å¯†åŒ™   2. ä¸‹è½½ä¸€ä¸ªå°ç¨‹åº: <a href="https://www.voidtools.com/zh-cn/" target="_blank" rel="noopener">everything</a> è¾“å…¥æ–‡ä»¶åå­—å°±å¯ä»¥æŸ¥æ‰¾)<br>æŽ¥ç€è¾“å…¥å¦‚å›¾å°±æˆåŠŸé…ç½®å®Œæˆäº†ã€‚</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ssh git@github.com</span><br></pre></td></tr></table></figure>
<p><img src="/2020/01/20/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE1-%E5%9C%A8GitHub%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E4%BB%93%E5%BA%93%E5%B9%B6%E9%85%8D%E7%BD%AEGit/5.png" alt="github"></p>
<p>ç»‘å®šSSH Keyçš„ç›®çš„æ˜¯ä¸ºäº†ä¹‹åŽæ¯æ¬¡pullå’Œpushçš„æ—¶å€™ä¸éœ€è¦è¾“å…¥å¯†ç å’Œè´¦å·,æ€ªéº»çƒ¦çš„ã€‚åŒæ—¶ä¹Ÿé¿å…ä¸€äº›å¯èƒ½äº§ç”Ÿçš„é”™è¯¯ã€‚</p>
<p>å…³äºŽGitçš„åŸºæœ¬ä½¿ç”¨è¯·å‚è€ƒ: <a href="https://www.guojingde.cn/2020/01/19/Git%E7%9A%84%E5%87%A0%E4%B8%AA%E5%9F%BA%E6%9C%AC%E6%8C%87%E4%BB%A4/#more" target="_blank" rel="noopener">Gitçš„å‡ ä¸ªåŸºæœ¬æŒ‡ä»¤</a></p>
<hr>
<p>ä¸‹ä¸€æ­¥: <a href="https://www.guojingde.cn/2020/01/20/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE2-%E5%AE%89%E8%A3%85%E5%B9%B6%E9%85%8D%E7%BD%AENode.js%E4%BB%A5%E5%8F%8AHexo/" target="_blank" rel="noopener">ä¸ªäººåšå®¢å»ºè®¾2-å®‰è£…å¹¶é…ç½®Node.jsä»¥åŠHexo</a><br>ç›®å½•: <a href="https://www.guojingde.cn/2020/01/19/%E7%94%A8GitHub%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/" target="_blank" rel="noopener">ç”¨GitHubæ­å»ºä¸ªäººåšå®¢(ç›®å½•)</a></p>
</font>]]></content>
      <categories>
        <category>åšå®¢æ­å»º</category>
      </categories>
      <tags>
        <tag>åšå®¢æ­å»º</tag>
      </tags>
  </entry>
  <entry>
    <title>Image Classifier with Deep learning</title>
    <url>/2020/01/19/Image%20Classifier%20with%20Deep%20learning/</url>
    <content><![CDATA[<h1 id="Image-Classifier-with-Deep-learning"><a href="#Image-Classifier-with-Deep-learning" class="headerlink" title="Image Classifier with Deep learning"></a>Image Classifier with Deep learning</h1><p>Going forward, AI algorithms will be incorporated into more and more everyday applications. For example, you might want to include an image classifier in a smart phone app. To do this, youâ€™d use a deep learning model trained on hundreds of thousands of images as part of the overall application architecture. A large part of software development in the future will be using these types of models as common parts of applications. </p>
<a id="more"></a>

<p>In this project, youâ€™ll train an image classifier to recognize different species of flowers. You can imagine using something like this in a phone app that tells you the name of the flower your camera is looking at. In practice youâ€™d train this classifier, then export it for use in your application. Weâ€™ll be using <a href="http://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html" target="_blank" rel="noopener">this dataset</a> of 102 flower categories, you can see a few examples below. </p>
<p><img src="/2020/01/19/Image%20Classifier%20with%20Deep%20learning/Flowers.png" alt="flowers"></p>
<p>The project is broken down into multiple steps:</p>
<ul>
<li>Load and preprocess the image dataset</li>
<li>Train the image classifier on your dataset</li>
<li>Use the trained classifier to predict image content</li>
</ul>
<p>Weâ€™ll lead you through each part which youâ€™ll implement in Python.</p>
<p>When youâ€™ve completed this project, youâ€™ll have an application that can be trained on any set of labeled images. Here your network will be learning about flowers and end up as a command line application. But, what you do with your new skills depends on your imagination and effort in building a dataset. For example, imagine an app where you take a picture of a car, it tells you what the make and model is, then looks up information about it. Go build your own dataset and make something new.</p>
<p>First up is importing the packages youâ€™ll need. Itâ€™s good practice to keep all the imports at the beginning of your code. As you work through this notebook and find you need to import a package, make sure to add the import up here.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Imports here</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms, models</span><br><span class="line"><span class="keyword">import</span> helper</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br></pre></td></tr></table></figure>

<h2 id="Load-the-data"><a href="#Load-the-data" class="headerlink" title="Load the data"></a>Load the data</h2><p>Here youâ€™ll use <code>torchvision</code> to load the data (<a href="http://pytorch.org/docs/0.3.0/torchvision/index.html" target="_blank" rel="noopener">documentation</a>). The data should be included alongside this notebook, otherwise you can <a href="https://s3.amazonaws.com/content.udacity-data.com/nd089/flower_data.tar.gz" target="_blank" rel="noopener">download it here</a>. The dataset is split into three parts, training, validation, and testing. For the training, youâ€™ll want to apply transformations such as random scaling, cropping, and flipping. This will help the network generalize leading to better performance. Youâ€™ll also need to make sure the input data is resized to 224x224 pixels as required by the pre-trained networks.</p>
<p>The validation and testing sets are used to measure the modelâ€™s performance on data it hasnâ€™t seen yet. For this you donâ€™t want any scaling or rotation transformations, but youâ€™ll need to resize then crop the images to the appropriate size.</p>
<p>The pre-trained networks youâ€™ll use were trained on the ImageNet dataset where each color channel was normalized separately. For all three sets youâ€™ll need to normalize the means and standard deviations of the images to what the network expects. For the means, itâ€™s <code>[0.485, 0.456, 0.406]</code> and for the standard deviations <code>[0.229, 0.224, 0.225]</code>, calculated from the ImageNet images.  These values will shift each color channel to be centered at 0 and range from -1 to 1.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_dir = <span class="string">'flowers'</span></span><br><span class="line">train_dir = data_dir + <span class="string">'/train'</span></span><br><span class="line">valid_dir = data_dir + <span class="string">'/valid'</span></span><br><span class="line">test_dir = data_dir + <span class="string">'/test'</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Define your transforms for the training, validation, and testing sets</span></span><br><span class="line">train_transforms = transforms.Compose([transforms.RandomRotation(<span class="number">30</span>),</span><br><span class="line">                                       transforms.RandomResizedCrop(<span class="number">224</span>),</span><br><span class="line">                                       transforms.RandomHorizontalFlip(),</span><br><span class="line">                                       transforms.ToTensor(),</span><br><span class="line">                                       transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], </span><br><span class="line">                                                            [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])]) </span><br><span class="line">valid_transforms = transforms.Compose([transforms.Resize(<span class="number">255</span>),</span><br><span class="line">                                      transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">                                      transforms.ToTensor(),</span><br><span class="line">                                      transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], </span><br><span class="line">                                                            [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])])</span><br><span class="line"></span><br><span class="line">test_transforms = transforms.Compose([transforms.Resize(<span class="number">255</span>),</span><br><span class="line">                                      transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">                                      transforms.ToTensor(),</span><br><span class="line">                                      transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], </span><br><span class="line">                                                            [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])])</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Load the datasets with ImageFolder</span></span><br><span class="line">train_data = datasets.ImageFolder(train_dir, transform=train_transforms)</span><br><span class="line">valid_data = datasets.ImageFolder(valid_dir, transform=valid_transforms)</span><br><span class="line">test_data = datasets.ImageFolder(test_dir, transform=test_transforms)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Using the image datasets and the trainforms, define the dataloaders</span></span><br><span class="line">trainloader = torch.utils.data.DataLoader(train_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">validloader = torch.utils.data.DataLoader(valid_data, batch_size=<span class="number">64</span>)</span><br><span class="line">testloader = torch.utils.data.DataLoader(test_data, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line">image_datasets = [train_data, valid_data, test_data]</span><br><span class="line">dataloaders = [trainloader, validloader, testloader]</span><br></pre></td></tr></table></figure>

<h3 id="Label-mapping"><a href="#Label-mapping" class="headerlink" title="Label mapping"></a>Label mapping</h3><p>Youâ€™ll also need to load in a mapping from category label to category name. You can find this in the file <code>cat_to_name.json</code>. Itâ€™s a JSON object which you can read in with the <a href="https://docs.python.org/2/library/json.html" target="_blank" rel="noopener"><code>json</code> module</a>. This will give you a dictionary mapping the integer encoded categories to the actual names of the flowers.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'cat_to_name.json'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    cat_to_name = json.load(f)</span><br></pre></td></tr></table></figure>

<h1 id="Building-and-training-the-classifier"><a href="#Building-and-training-the-classifier" class="headerlink" title="Building and training the classifier"></a>Building and training the classifier</h1><p>Now that the data is ready, itâ€™s time to build and train the classifier. As usual, you should use one of the pretrained models from <code>torchvision.models</code> to get the image features. Build and train a new feed-forward classifier using those features.</p>
<p>Weâ€™re going to leave this part up to you. Refer to <a href="https://review.udacity.com/#!/rubrics/1663/view" target="_blank" rel="noopener">the rubric</a> for guidance on successfully completing this section. Things youâ€™ll need to do:</p>
<ul>
<li>Load a <a href="http://pytorch.org/docs/master/torchvision/models.html" target="_blank" rel="noopener">pre-trained network</a> (If you need a starting point, the VGG networks work great and are straightforward to use)</li>
<li>Define a new, untrained feed-forward network as a classifier, using ReLU activations and dropout</li>
<li>Train the classifier layers using backpropagation using the pre-trained network to get the features</li>
<li>Track the loss and accuracy on the validation set to determine the best hyperparameters</li>
</ul>
<p>Weâ€™ve left a cell open for you below, but use as many as you need. Our advice is to break the problem up into smaller parts you can run separately. Check that each part is doing what you expect, then move on to the next. Youâ€™ll likely find that as you work through each part, youâ€™ll need to go back and modify your previous code. This is totally normal!</p>
<p>When training make sure youâ€™re updating only the weights of the feed-forward network. You should be able to get the validation accuracy above 70% if you build everything right. Make sure to try different hyperparameters (learning rate, units in the classifier, epochs, etc) to find the best model. Save those hyperparameters to use as default values in the next part of the project.</p>
<p>One last important tip if youâ€™re using the workspace to run your code: To avoid having your workspace disconnect during the long-running tasks in this notebook, please read in the earlier page in this lesson called Intro to<br>GPU Workspaces about Keeping Your Session Active. Youâ€™ll want to include code from the workspace_utils.py module.</p>
<p><strong>Note for Workspace users:</strong> If your network is over 1 GB when saved as a checkpoint, there might be issues with saving backups in your workspace. Typically this happens with wide dense layers after the convolutional layers. If your saved checkpoint is larger than 1 GB (you can open a terminal and check with <code>ls -lh</code>), you should reduce the size of your hidden layers and train again.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Build and train your network</span></span><br><span class="line">model = models.vgg19(pretrained=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Freeze parameters so we don't backprop through them</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br><span class="line">    </span><br><span class="line">model.classifier = nn.Sequential(nn.Linear(<span class="number">25088</span>, <span class="number">2048</span>),</span><br><span class="line">                                 nn.ReLU(),</span><br><span class="line">                                 nn.Dropout(<span class="number">0.25</span>),</span><br><span class="line">                                 nn.Linear(<span class="number">2048</span>, <span class="number">102</span>),</span><br><span class="line">                                 nn.LogSoftmax(dim=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">criterion = nn.NLLLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Only train the classifier parameters, feature parameters are frozen</span></span><br><span class="line">optimizer = optim.Adam(model.classifier.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">model</span><br></pre></td></tr></table></figure>




<pre><code>VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU(inplace=True)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU(inplace=True)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (17): ReLU(inplace=True)
    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU(inplace=True)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU(inplace=True)
    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (24): ReLU(inplace=True)
    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (26): ReLU(inplace=True)
    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU(inplace=True)
    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (31): ReLU(inplace=True)
    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (33): ReLU(inplace=True)
    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (35): ReLU(inplace=True)
    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=2048, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Linear(in_features=2048, out_features=102, bias=True)
    (4): LogSoftmax()
  )
)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.to(device)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">epochs = <span class="number">8</span></span><br><span class="line">steps = <span class="number">0</span></span><br><span class="line">running_loss = <span class="number">0</span></span><br><span class="line">print_every = <span class="number">50</span></span><br><span class="line">start = time.time()</span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> range(epochs):</span><br><span class="line">    <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> trainloader:</span><br><span class="line">        steps += <span class="number">1</span></span><br><span class="line">        inputs, labels = inputs.to(device), labels.to(device)</span><br><span class="line">        </span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">        logps = model.forward(inputs)</span><br><span class="line">        loss = criterion(logps, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> steps % print_every == <span class="number">0</span>:</span><br><span class="line">            test_loss = <span class="number">0</span></span><br><span class="line">            accuracy = <span class="number">0</span></span><br><span class="line">            model.eval()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> validloader:</span><br><span class="line">                    inputs, labels = inputs.to(device), labels.to(device)</span><br><span class="line">                    logps = model.forward(inputs)</span><br><span class="line">                    batch_loss = criterion(logps, labels)</span><br><span class="line">                    </span><br><span class="line">                    test_loss += batch_loss.item()</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Calculate accuracy</span></span><br><span class="line">                    ps = torch.exp(logps)</span><br><span class="line">                    top_p, top_class = ps.topk(<span class="number">1</span>, dim=<span class="number">1</span>)</span><br><span class="line">                    equals = top_class == labels.view(*top_class.shape)</span><br><span class="line">                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()</span><br><span class="line">                    </span><br><span class="line">            print(<span class="string">"Epoch: &#123;&#125;/&#123;&#125;.. "</span>.format(e+<span class="number">1</span>, epochs),</span><br><span class="line">                  <span class="string">f"Train loss: <span class="subst">&#123;running_loss/print_every:<span class="number">.3</span>f&#125;</span>.. "</span></span><br><span class="line">                  <span class="string">f"Valid loss: <span class="subst">&#123;test_loss/len(validloader):<span class="number">.3</span>f&#125;</span>.. "</span></span><br><span class="line">                  <span class="string">f"Valid accuracy: <span class="subst">&#123;accuracy/len(validloader):<span class="number">.3</span>f&#125;</span>"</span>)</span><br><span class="line">            running_loss = <span class="number">0</span></span><br><span class="line">            model.train()</span><br><span class="line">time_end = time.time() - start</span><br><span class="line">print(<span class="string">"\nTotal time: &#123;:.0f&#125;m &#123;:.0f&#125;s"</span>.format(time_end//<span class="number">60</span>, time_end % <span class="number">60</span>))</span><br></pre></td></tr></table></figure>

<pre><code>Epoch: 1/8..  Train loss: 3.569.. Valid loss: 1.455.. Valid accuracy: 0.612
Epoch: 1/8..  Train loss: 1.689.. Valid loss: 0.859.. Valid accuracy: 0.768
Epoch: 2/8..  Train loss: 1.296.. Valid loss: 0.759.. Valid accuracy: 0.784
Epoch: 2/8..  Train loss: 1.181.. Valid loss: 0.720.. Valid accuracy: 0.800
Epoch: 3/8..  Train loss: 1.054.. Valid loss: 0.655.. Valid accuracy: 0.809
Epoch: 3/8..  Train loss: 1.020.. Valid loss: 0.517.. Valid accuracy: 0.839
Epoch: 4/8..  Train loss: 0.988.. Valid loss: 0.487.. Valid accuracy: 0.850
Epoch: 4/8..  Train loss: 0.932.. Valid loss: 0.596.. Valid accuracy: 0.832
Epoch: 5/8..  Train loss: 0.893.. Valid loss: 0.517.. Valid accuracy: 0.857
Epoch: 5/8..  Train loss: 0.854.. Valid loss: 0.493.. Valid accuracy: 0.870
Epoch: 6/8..  Train loss: 0.790.. Valid loss: 0.508.. Valid accuracy: 0.861
Epoch: 6/8..  Train loss: 0.806.. Valid loss: 0.491.. Valid accuracy: 0.873
Epoch: 7/8..  Train loss: 0.806.. Valid loss: 0.565.. Valid accuracy: 0.861
Epoch: 7/8..  Train loss: 0.825.. Valid loss: 0.449.. Valid accuracy: 0.887
Epoch: 8/8..  Train loss: 0.734.. Valid loss: 0.453.. Valid accuracy: 0.882
Epoch: 8/8..  Train loss: 0.773.. Valid loss: 0.539.. Valid accuracy: 0.860

Total time: 14m 39s</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Testing-your-network"><a href="#Testing-your-network" class="headerlink" title="Testing your network"></a>Testing your network</h2><p>Itâ€™s good practice to test your trained network on test data, images the network has never seen either in training or validation. This will give you a good estimate for the modelâ€™s performance on completely new images. Run the test images through the network and measure the accuracy, the same way you did validation. You should be able to reach around 70% accuracy on the test set if the model has been trained well.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Do validation on the test set</span></span><br><span class="line">model.eval()</span><br><span class="line">accuracy = <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> testloader:</span><br><span class="line">        inputs, labels = inputs.to(device), labels.to(device)</span><br><span class="line">        logps = model.forward(inputs)</span><br><span class="line">        batch_loss = criterion(logps, labels)</span><br><span class="line">                    </span><br><span class="line">        test_loss += batch_loss.item()</span><br><span class="line">                    </span><br><span class="line">        <span class="comment"># Calculate accuracy</span></span><br><span class="line">        ps = torch.exp(logps)</span><br><span class="line">        top_p, top_class = ps.topk(<span class="number">1</span>, dim=<span class="number">1</span>)</span><br><span class="line">        equals = top_class == labels.view(*top_class.shape)</span><br><span class="line">        accuracy += torch.mean(equals.type(torch.FloatTensor)).item()</span><br><span class="line">print(<span class="string">f"Valid accuracy: <span class="subst">&#123;accuracy/len(testloader):<span class="number">.3</span>f&#125;</span>"</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Valid accuracy: 0.854</code></pre><h2 id="Save-the-checkpoint"><a href="#Save-the-checkpoint" class="headerlink" title="Save the checkpoint"></a>Save the checkpoint</h2><p>Now that your network is trained, save the model so you can load it later for making predictions. You probably want to save other things such as the mapping of classes to indices which you get from one of the image datasets: <code>image_datasets[&#39;train&#39;].class_to_idx</code>. You can attach this to the model as an attribute which makes inference easier later on.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">Remember that you&#39;ll want to completely rebuild the model later so you can use it for inference. Make sure to include any information you need in the checkpoint. If you want to load the model and keep training, you&#39;ll want to save the number of epochs as well as the optimizer state, &#96;optimizer.state_dict&#96;. You&#39;ll likely want to use this trained model in the next part of the project, so best to save it now.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;python</span><br><span class="line"># TODO: Save the checkpoint     </span><br><span class="line">classifier &#x3D; nn.Sequential(nn.Linear(25088, 2048),</span><br><span class="line">                           nn.ReLU(),</span><br><span class="line">                           nn.Dropout(0.25),</span><br><span class="line">                           nn.Linear(2048, 102),</span><br><span class="line">                           nn.LogSoftmax(dim&#x3D;1))</span><br><span class="line"></span><br><span class="line">model.class_to_idx &#x3D; image_datasets[0].class_to_idx</span><br><span class="line">checkpoint &#x3D; &#123;&#39;input_size&#39;: 25088,</span><br><span class="line">              &#39;output_size&#39;: 102,</span><br><span class="line">              &#39;classifier&#39; : classifier,</span><br><span class="line">              &#39;arch&#39;: &#39;vgg19&#39;,</span><br><span class="line">              &#39;optimizer&#39;: optimizer.state_dict(),</span><br><span class="line">              &#39;state_dict&#39;: model.state_dict(),</span><br><span class="line">              &#39;class_to_idx&#39;: model.class_to_idx&#125;</span><br><span class="line">torch.save(checkpoint, &#39;checkpoint.pth&#39;)</span><br></pre></td></tr></table></figure>

<h2 id="Loading-the-checkpoint"><a href="#Loading-the-checkpoint" class="headerlink" title="Loading the checkpoint"></a>Loading the checkpoint</h2><p>At this point itâ€™s good to write a function that can load a checkpoint and rebuild the model. That way you can come back to this project and keep working on it without having to retrain the network.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Write a function that loads a checkpoint and rebuilds the model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_checkpoint</span><span class="params">(filepath)</span>:</span></span><br><span class="line">    checkpoints = torch.load(filepath)</span><br><span class="line">    model = models.vgg19(pretrained=<span class="literal">True</span>)</span><br><span class="line">    model.class_to_idx = checkpoints[<span class="string">'class_to_idx'</span>]</span><br><span class="line">    </span><br><span class="line">    model.classifier = checkpoints[<span class="string">'classifier'</span>]</span><br><span class="line">    </span><br><span class="line">    model.load_state_dict(checkpoints[<span class="string">'state_dict'</span>])</span><br><span class="line">    optimizer.load_state_dict(checkpoints[<span class="string">'optimizer'</span>])                       </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = load_checkpoint(<span class="string">'checkpoint.pth'</span>)</span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure>

<pre><code>VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU(inplace=True)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU(inplace=True)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (17): ReLU(inplace=True)
    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU(inplace=True)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU(inplace=True)
    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (24): ReLU(inplace=True)
    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (26): ReLU(inplace=True)
    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU(inplace=True)
    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (31): ReLU(inplace=True)
    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (33): ReLU(inplace=True)
    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (35): ReLU(inplace=True)
    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=2048, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Linear(in_features=2048, out_features=102, bias=True)
    (4): LogSoftmax()
  )
)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="Inference-for-classification"><a href="#Inference-for-classification" class="headerlink" title="Inference for classification"></a>Inference for classification</h1><p>Now youâ€™ll write a function to use a trained network for inference. That is, youâ€™ll pass an image into the network and predict the class of the flower in the image. Write a function called <code>predict</code> that takes an image and a model, then returns the top $K$ most likely classes along with the probabilities. It should look like </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">probs, classes = predict(image_path, model)</span><br><span class="line">print(probs)</span><br><span class="line">print(classes)</span><br><span class="line">&gt; [ <span class="number">0.01558163</span>  <span class="number">0.01541934</span>  <span class="number">0.01452626</span>  <span class="number">0.01443549</span>  <span class="number">0.01407339</span>]</span><br><span class="line">&gt; [<span class="string">'70'</span>, <span class="string">'3'</span>, <span class="string">'45'</span>, <span class="string">'62'</span>, <span class="string">'55'</span>]</span><br></pre></td></tr></table></figure>

<p>First youâ€™ll need to handle processing the input image such that it can be used in your network. </p>
<h2 id="Image-Preprocessing"><a href="#Image-Preprocessing" class="headerlink" title="Image Preprocessing"></a>Image Preprocessing</h2><p>Youâ€™ll want to use <code>PIL</code> to load the image (<a href="https://pillow.readthedocs.io/en/latest/reference/Image.html" target="_blank" rel="noopener">documentation</a>). Itâ€™s best to write a function that preprocesses the image so it can be used as input for the model. This function should process the images in the same manner used for training. </p>
<p>First, resize the images where the shortest side is 256 pixels, keeping the aspect ratio. This can be done with the <a href="http://pillow.readthedocs.io/en/3.1.x/reference/Image.html#PIL.Image.Image.thumbnail" target="_blank" rel="noopener"><code>thumbnail</code></a> or <a href="http://pillow.readthedocs.io/en/3.1.x/reference/Image.html#PIL.Image.Image.thumbnail" target="_blank" rel="noopener"><code>resize</code></a> methods. Then youâ€™ll need to crop out the center 224x224 portion of the image.</p>
<p>Color channels of images are typically encoded as integers 0-255, but the model expected floats 0-1. Youâ€™ll need to convert the values. Itâ€™s easiest with a Numpy array, which you can get from a PIL image like so <code>np_image = np.array(pil_image)</code>.</p>
<p>As before, the network expects the images to be normalized in a specific way. For the means, itâ€™s <code>[0.485, 0.456, 0.406]</code> and for the standard deviations <code>[0.229, 0.224, 0.225]</code>. Youâ€™ll want to subtract the means from each color channel, then divide by the standard deviation. </p>
<p>And finally, PyTorch expects the color channel to be the first dimension but itâ€™s the third dimension in the PIL image and Numpy array. You can reorder dimensions using <a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.ndarray.transpose.html" target="_blank" rel="noopener"><code>ndarray.transpose</code></a>. The color channel needs to be first and retain the order of the other two dimensions.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Process a PIL image for use in a PyTorch model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_image</span><span class="params">(image)</span>:</span></span><br><span class="line">    <span class="string">''' Scales, crops, and normalizes a PIL image for a PyTorch model,</span></span><br><span class="line"><span class="string">        returns an Numpy array</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    im = Image.open(image)</span><br><span class="line">    im = im.resize((<span class="number">256</span>,<span class="number">256</span>))</span><br><span class="line">    transform = transforms.Compose([transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">                                    transforms.ToTensor(),</span><br><span class="line">                                    transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], </span><br><span class="line">                                                         [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])])</span><br><span class="line">    im = transform(im)</span><br><span class="line">    <span class="keyword">return</span> im</span><br></pre></td></tr></table></figure>

<p>To check your work, the function below converts a PyTorch tensor and displays it in the notebook. If your <code>process_image</code> function works, running the output through this function should return the original image (except for the cropped out portions).</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow</span><span class="params">(image, ax=None, title=None)</span>:</span></span><br><span class="line">    <span class="string">"""Imshow for Tensor."""</span></span><br><span class="line">    <span class="keyword">if</span> ax <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        fig, ax = plt.subplots()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># PyTorch tensors assume the color channel is the first dimension</span></span><br><span class="line">    <span class="comment"># but matplotlib assumes is the third dimension</span></span><br><span class="line">    image = image.numpy().transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Undo preprocessing</span></span><br><span class="line">    mean = np.array([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>])</span><br><span class="line">    std = np.array([<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    image = std * image + mean</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Image needs to be clipped between 0 and 1 or it looks like noise when displayed</span></span><br><span class="line">    image = np.clip(image, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    ax.imshow(image)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> ax</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Show original pics</span></span><br><span class="line">image_path = test_dir + <span class="string">'/17/image_03911.jpg'</span></span><br><span class="line">pic = Image.open(image_path)</span><br><span class="line">pic</span><br></pre></td></tr></table></figure>




<p><img src="/2020/01/19/Image%20Classifier%20with%20Deep%20learning/output_24_0.png" alt="èŠ±"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Show processed pics</span></span><br><span class="line">pic_process = process_image(image_path)</span><br><span class="line">imshow(pic_process)</span><br></pre></td></tr></table></figure>




<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x2ae83fbbf08&gt;</code></pre><p><img src="/2020/01/19/Image%20Classifier%20with%20Deep%20learning/output_25_1.png" alt="èŠ±"></p>
<h2 id="Class-Prediction"><a href="#Class-Prediction" class="headerlink" title="Class Prediction"></a>Class Prediction</h2><p>Once you can get images in the correct format, itâ€™s time to write a function for making predictions with your model. A common practice is to predict the top 5 or so (usually called top-$K$) most probable classes. Youâ€™ll want to calculate the class probabilities then find the $K$ largest values.</p>
<p>To get the top $K$ largest values in a tensor use <a href="http://pytorch.org/docs/master/torch.html#torch.topk" target="_blank" rel="noopener"><code>x.topk(k)</code></a>. This method returns both the highest <code>k</code> probabilities and the indices of those probabilities corresponding to the classes. You need to convert from these indices to the actual class labels using <code>class_to_idx</code> which hopefully you added to the model or from an <code>ImageFolder</code> you used to load the data (<a href="#Save-the-checkpoint">see here</a>). Make sure to invert the dictionary so you get a mapping from index to class as well.</p>
<p>Again, this method should take a path to an image and a model checkpoint, then return the probabilities and classes.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">probs, classes = predict(image_path, model)</span><br><span class="line">print(probs)</span><br><span class="line">print(classes)</span><br><span class="line">&gt; [ <span class="number">0.01558163</span>  <span class="number">0.01541934</span>  <span class="number">0.01452626</span>  <span class="number">0.01443549</span>  <span class="number">0.01407339</span>]</span><br><span class="line">&gt; [<span class="string">'70'</span>, <span class="string">'3'</span>, <span class="string">'45'</span>, <span class="string">'62'</span>, <span class="string">'55'</span>]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(image_path, model, topk=<span class="number">5</span>)</span>:</span></span><br><span class="line">    <span class="string">''' Predict the class (or classes) of an image using a trained deep learning model.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    processed_image = process_image(image_path)</span><br><span class="line">    processed_image.unsqueeze_(<span class="number">0</span>)</span><br><span class="line">    probs = torch.exp(model.forward(processed_image))</span><br><span class="line">    top_probs, top_index = probs.topk(topk)</span><br><span class="line">    top_index = top_index[<span class="number">0</span>].numpy()</span><br><span class="line">    index = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(model.class_to_idx.items())):</span><br><span class="line">        index.append(list(model.class_to_idx.items())[i][<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    label = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        label.append(index[top_index[i]])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> top_probs, label</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img_path = test_dir  + <span class="string">'/17/image_03911.jpg'</span></span><br><span class="line">predict(img_path, model, topk=<span class="number">5</span>)</span><br></pre></td></tr></table></figure>




<pre><code>(tensor([[9.8274e-01, 1.6505e-02, 3.9908e-04, 1.0707e-04, 9.1983e-05]],
        grad_fn=&lt;TopkBackward&gt;), [&apos;17&apos;, &apos;100&apos;, &apos;18&apos;, &apos;34&apos;, &apos;92&apos;])</code></pre><h2 id="Sanity-Checking"><a href="#Sanity-Checking" class="headerlink" title="Sanity Checking"></a>Sanity Checking</h2><p>Now that you can use a trained model for predictions, check to make sure it makes sense. Even if the testing accuracy is high, itâ€™s always good to check that there arenâ€™t obvious bugs. Use <code>matplotlib</code> to plot the probabilities for the top 5 classes as a bar graph, along with the input image. It should look like this:</p>
<p><img src="/2020/01/19/Image%20Classifier%20with%20Deep%20learning/inference_example.png" alt="flowers"></p>
<p>You can convert from the class integer encoding to actual flower names with the <code>cat_to_name.json</code> file (should have been loaded earlier in the notebook). To show a PyTorch tensor as an image, use the <code>imshow</code> function defined above.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Display an image along with the top 5 classes</span></span><br><span class="line">prob, classes = predict(img_path, model)</span><br><span class="line">prob = prob[<span class="number">0</span>].detach().numpy()</span><br><span class="line">labels = []</span><br><span class="line"><span class="keyword">for</span> each <span class="keyword">in</span> classes:</span><br><span class="line">    labels.append(cat_to_name[each])</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize = (<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">ax = plt.subplot(<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">flower_num = img_path.split(<span class="string">'/'</span>)[<span class="number">2</span>] <span class="comment"># find the index of the flower</span></span><br><span class="line">title= cat_to_name[flower_num] </span><br><span class="line">img = process_image(img_path)</span><br><span class="line">plt.title(title)</span><br><span class="line">imshow(img, ax)</span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">sns.barplot(prob, y=labels)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="/2020/01/19/Image%20Classifier%20with%20Deep%20learning/output_31_0.png" alt="èŠ±"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># test a flower pic outside the dataset</span></span><br><span class="line">img_path = <span class="string">'C:\\Users\\jasonguo\\Desktop\\flowers_zwy\\11.JPG'</span></span><br><span class="line">prob, classes = predict(img_path, model)</span><br><span class="line">prob = prob[<span class="number">0</span>].detach().numpy()</span><br><span class="line">labels = []</span><br><span class="line"><span class="keyword">for</span> each <span class="keyword">in</span> classes:</span><br><span class="line">    labels.append(cat_to_name[each])</span><br><span class="line">plt.figure(figsize = (<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">ax = plt.subplot(<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">img = process_image(img_path)</span><br><span class="line">imshow(img, ax)</span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">sns.barplot(prob, y=labels)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># wihch is exactly true</span></span><br></pre></td></tr></table></figure>


<p><img src="/2020/01/19/Image%20Classifier%20with%20Deep%20learning/output_32_0.png" alt="èŠ±"></p>
]]></content>
      <categories>
        <category>Pythonå­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
        <tag>æ·±åº¦å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title>ç”¨GitHubæ­å»ºä¸ªäººåšå®¢(ç›®å½•)</title>
    <url>/2020/01/19/%E7%94%A8GitHub%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</url>
    <content><![CDATA[<font size="3">
å¿«é€Ÿä¸Šæ‰‹ä½¿ç”¨GitHub+Hexoå»ºç«‹åšå®¢,é€‚åˆæ–°æ‰‹ã€‚
ç³»ç»ŸçŽ¯å¢ƒ:Windows
æœ¬æ–‡æ˜¯ç›®å½•ç¯‡,åŒ…å«äº†æ‰€æœ‰å…³äºŽä½¿ç”¨GitHub+Hexoå»ºç«‹åšå®¢çš„ç›¸å…³æ•™ç¨‹ã€‚ä¼šé€æ¸æ›´æ–°ã€‚ã€‚ã€‚

<a id="more"></a>
<hr>
<h1 id="é¦–å…ˆä¸ºä»€ä¹ˆä½¿ç”¨GitHub-Hexoå»ºç«‹åšå®¢"><a href="#é¦–å…ˆä¸ºä»€ä¹ˆä½¿ç”¨GitHub-Hexoå»ºç«‹åšå®¢" class="headerlink" title="é¦–å…ˆä¸ºä»€ä¹ˆä½¿ç”¨GitHub+Hexoå»ºç«‹åšå®¢"></a>é¦–å…ˆä¸ºä»€ä¹ˆä½¿ç”¨GitHub+Hexoå»ºç«‹åšå®¢</h1><h2 id="1-ä¼˜ç‚¹"><a href="#1-ä¼˜ç‚¹" class="headerlink" title="1. ä¼˜ç‚¹"></a>1. ä¼˜ç‚¹</h2><ol>
<li>ä¸éœ€è¦æœåŠ¡å™¨çœé’±çœæ—¶é—´å‘€(ä¸»è¦åŽŸå› )</li>
<li>é€‚åˆæ–°æ‰‹ä»¥åŠå¯¹å»ºç«™ä¸ç†Ÿæ‚‰çš„å°ç™½(æœ¬äºº)</li>
<li>æœ‰å¾ˆå¤šçŽ°æˆç²¾ç¾Žæ¨¡æ¿</li>
<li>æ— æµé‡é™åˆ¶</li>
</ol>
<h2 id="2-ç¼ºç‚¹"><a href="#2-ç¼ºç‚¹" class="headerlink" title="2. ç¼ºç‚¹"></a>2. ç¼ºç‚¹</h2><ol>
<li><p>ä¸ªæ€§åŒ–ç©ºé—´å°</p>
</li>
<li><p>é™æ€é¡µé¢(è™½ç„¶åšå®¢ä»¥åŠè¶³çŸ£)</p>
</li>
<li><p>æ²¡æœ‰æ•°æ®åº“,ä¸é€‚åˆå¤§åž‹ç½‘ç«™,è¿è¡Œä¸€æ¬¡å°±è¦éåŽ†æ‰€æœ‰ç½‘ç«™å†…å®¹</p>
<p><font color="red" size="4"> <strong>æ€»ä¹‹å°±æ˜¯éžå¸¸é€‚åˆåšä¸ªäººåšå®¢å°±å¯¹äº†ã€‚</strong></font></p>
</li>
</ol>
<p>å®Œæˆè¿™ä¸ªåšå®¢é¡¹ç›®ä½ éœ€è¦: </p>
<ol>
<li>éžå¸¸åŸºç¡€çš„GitçŸ¥è¯†,ç†Ÿæ‚‰å‘½ä»¤è¡Œ(å‚ç…§æ–‡æœ«è¾…åŠ©èµ„æ–™)</li>
<li>å–œæ¬¢æŠ˜è…¾</li>
<li>è€å¿ƒ</li>
</ol>
<h1 id="ç›®å½•"><a href="#ç›®å½•" class="headerlink" title="ç›®å½•"></a>ç›®å½•</h1><ol>
<li><a href="https://www.guojingde.cn/2020/01/20/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE1-%E5%9C%A8GitHub%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E4%BB%93%E5%BA%93%E5%B9%B6%E9%85%8D%E7%BD%AEGit/" target="_blank" rel="noopener">ä¸ªäººåšå®¢å»ºè®¾1-åœ¨GitHubåˆ›å»ºä¸€ä¸ªä»“åº“å¹¶é…ç½®Git</a></li>
<li><a href="https://www.guojingde.cn/2020/01/20/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE2-%E5%AE%89%E8%A3%85%E5%B9%B6%E9%85%8D%E7%BD%AENode.js%E4%BB%A5%E5%8F%8AHexo/" target="_blank" rel="noopener">ä¸ªäººåšå®¢å»ºè®¾2-å®‰è£…å¹¶é…ç½®Node.jsä»¥åŠHexo</a></li>
<li><a href="https://www.guojingde.cn/2020/01/21/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE3-%E7%94%B3%E8%AF%B7%E5%B9%B6%E7%BB%91%E5%AE%9A%E5%9F%9F%E5%90%8D/#more" target="_blank" rel="noopener">ä¸ªäººåšå®¢å»ºè®¾3-ç”³è¯·å¹¶ç»‘å®šåŸŸå</a></li>
<li><a href="https://www.guojingde.cn/2020/01/21/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE4-%E9%80%89%E6%8B%A9%E5%8D%9A%E5%AE%A2%E4%B8%BB%E9%A2%98/" target="_blank" rel="noopener">ä¸ªäººåšå®¢å»ºè®¾4-é€‰æ‹©åšå®¢ä¸»é¢˜</a></li>
<li><a href="https://www.guojingde.cn/2020/01/21/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE5-Markdown%E8%AF%AD%E6%B3%95/" target="_blank" rel="noopener">ä¸ªäººåšå®¢å»ºè®¾5-Markdownè¯­æ³•</a></li>
<li><a href="https://www.guojingde.cn/2020/01/21/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE6-%E4%B8%BB%E9%A2%98%E7%9A%84%E9%85%8D%E7%BD%AENext/" target="_blank" rel="noopener">ä¸ªäººåšå®¢å»ºè®¾6-ä¸»é¢˜çš„é…ç½®Next</a></li>
</ol>
<h1 id="å…¶ä»–è¾…åŠ©èµ„æ–™"><a href="#å…¶ä»–è¾…åŠ©èµ„æ–™" class="headerlink" title="å…¶ä»–è¾…åŠ©èµ„æ–™"></a>å…¶ä»–è¾…åŠ©èµ„æ–™</h1><ol>
<li><a href="https://www.guojingde.cn/2020/01/20/Git%E7%9A%84%E5%87%A0%E4%B8%AA%E5%9F%BA%E6%9C%AC%E6%8C%87%E4%BB%A4/" target="_blank" rel="noopener">Gitçš„å‡ ä¸ªåŸºæœ¬æŒ‡ä»¤</a></li>
<li><a href="https://www.guojingde.cn/2020/01/25/Hexo%20%E5%A6%82%E4%BD%95%E4%BC%98%E9%9B%85%E7%9A%84%E5%8A%A0%E5%85%A5Rmarkdown%E7%94%9F%E6%88%90%E7%9A%84html%E6%96%87%E4%BB%B6/" target="_blank" rel="noopener">Hexo å¦‚ä½•ä¼˜é›…çš„åŠ å…¥Rmarkdownç”Ÿæˆçš„html/pdfæ–‡ä»¶</a></li>
</ol>
</font>]]></content>
      <categories>
        <category>åšå®¢æ­å»º</category>
      </categories>
      <tags>
        <tag>åšå®¢æ­å»º</tag>
      </tags>
  </entry>
</search>
